[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Programming notes",
    "section": "",
    "text": "Quarto website with rough notes on a range of topics (e.g. books, python, R, causality, simulation, etc.).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Programming notes</span>"
    ]
  },
  {
    "objectID": "content/general/linux.html",
    "href": "content/general/linux.html",
    "title": "2  Linux",
    "section": "",
    "text": "2.1 Resize an image\nimagemagick\nconvert -resize 20% source.png dest.jpg",
    "crumbs": [
      "General",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Linux</span>"
    ]
  },
  {
    "objectID": "content/general/linux.html#reinstall-google-chrome",
    "href": "content/general/linux.html#reinstall-google-chrome",
    "title": "2  Linux",
    "section": "2.2 Reinstall google chrome",
    "text": "2.2 Reinstall google chrome\nInstall the .deb file (as not available in the official APT repositories).\nwget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\nInstall google chrome.\nsudo dpkg -i google-chrome-stable_current_amd64.deb\nRemove the .deb file.\nsudo rm google-chrome-stable_current_amd64.deb",
    "crumbs": [
      "General",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Linux</span>"
    ]
  },
  {
    "objectID": "content/general/linux.html#upgrade-quarto",
    "href": "content/general/linux.html#upgrade-quarto",
    "title": "2  Linux",
    "section": "2.3 Upgrade quarto",
    "text": "2.3 Upgrade quarto\nInstall the .deb file, then run e.g….\nsudo dpkg -i quarto-1.6.40-linux-amd64.deb",
    "crumbs": [
      "General",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Linux</span>"
    ]
  },
  {
    "objectID": "content/general/git.html",
    "href": "content/general/git.html",
    "title": "3  Git",
    "section": "",
    "text": "3.1 Conventional commits\nVSCode has extension Conventional Commits to help with following this criteria.\nFormat:\nIf you have a breaking change (correlates with major in semantic versioning) you need to: * Add a “!” after type/scope * In footer, have “BREAKING CHANGE: description of change”\nType (mandatory):\nScope (optional)\nDescription (mandatory) * Concise description of change in present tense * Don’t capitalise first letter, and no dot (.) at end\nBody (optional) * Motivation for change and contrast with previous behaviour, in present tense * Must have one blank line before\nFooter (optional) * Must have one blank line before * Should include issue ref (e.g. Fixes: #13). As long as you use one of GitHub’s key words, it will automatically create a link. (close, closes, closed, fix, fixes, fixed, resolve, resolves, resolved). If for a commit in a different repository, then e.g. Resolves: neudesic/projectcenter#22 * Should only include BREAKING CHANGE, external links, issue references, and other meta-information\nExamples:\nSources: * https://kapeli.com/cheat_sheets/Conventional_Commits.docset/Contents/Resources/Documents/index * https://gist.github.com/qoomon/5dfcdf8eec66a051ecd85625518cfd13 * https://www.conventionalcommits.org/en/v1.0.0-beta.2/ * https://medium.com/neudesic-innovation/conventional-commits-a-better-way-78d6785c2e08",
    "crumbs": [
      "General",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Git</span>"
    ]
  },
  {
    "objectID": "content/general/git.html#conventional-commits",
    "href": "content/general/git.html#conventional-commits",
    "title": "3  Git",
    "section": "",
    "text": "type(scope): description\n\nbody\n\nfooter\n\n\n\n\n\n\n\n\n\n\nType\nSummary\nDescription\n\n\n\n\nstyle\nStyles\nCommit doesn’t affect meaning (white-space, formatting, missing semi-colons, etc.)\n\n\ndocs\nDocumentation\nCommit affects documentation only (correlates with patch in semantic versioning)\n\n\nrefactor\nCode refactoring\nCommit rewrites/restructures code but doesn’t change expected behaviour of code\n\n\nperf\nPerformance improvements\nCommit rewrites/restructures code to improve performance (but doesn’t change expected behaviour of code)\n\n\nimprovement\nImprovement\nCommit improves current implementation without adding new feature or fixing bug (would have no effect on semantic versioning)\n\n\nfeat\nFeatures\nNew feature\n\n\nfix\nBug fixes\nBug fix\n\n\ntest\nTest\nCommit adds or amends tests\n\n\nbuild\nBuilds\nCommit affects build system or external dependencies (e.g. adding, removing, upgrading dependencies)\n\n\nops\nOperational\nCommit affects operational components (e.g. infrastructure, deployment, backup, recovery)\n\n\nchore\nChores\nMiscellanous commit (e.g. modifying .gitignore, regenerating generated code, releasing product)\n\n\nrevert\nReverts\nRevert to old commit\n\n\nci\nContinuous integrations (CI)\nCommit changes CI files (e.g. Travis)\n\n\n\n\n\n\n\n\nfix: add missing parameter to service call\n\nThe error occurred because of &lt;reasons&gt;.\nfeat!: remove ticket list endpoint\n\nrefers to JIRA-1337\n\nBREAKING CHANGES: ticket enpoints no longer supports list all entites.\nfeat(shopping cart): add the amazing button",
    "crumbs": [
      "General",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Git</span>"
    ]
  },
  {
    "objectID": "content/general/git.html#download-folder-from-a-github-repository",
    "href": "content/general/git.html#download-folder-from-a-github-repository",
    "title": "3  Git",
    "section": "3.2 Download folder from a GitHub repository",
    "text": "3.2 Download folder from a GitHub repository\nReplace .com in the URL with .dev. This will open an editor, where you can then right click the folder in the sidebar and select download.",
    "crumbs": [
      "General",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Git</span>"
    ]
  },
  {
    "objectID": "content/general/git.html#create-and-delete-repositories",
    "href": "content/general/git.html#create-and-delete-repositories",
    "title": "3  Git",
    "section": "3.3 Create and delete repositories",
    "text": "3.3 Create and delete repositories\nWhen you create the repository on GitHub, be sure to include:\n* .gitignore - tells git which files it should ignore when run git status and upload files to git (e.g. ignoring .Rhistory when use R)\n* LICENSE - use open source license like MIT\n* README.md - contains information about project, shown on front page of repository\nTo clone remote repository, start in the directory where you would like to create the folder (don’t start within folder with desired name, or will make the same named folder again within it!), then e.g.: git clone https://github.com/amyheather/programming_notes.git\nTo delete repository, enter it and type: rm -rf .git. Then leave go up a directory and do rm -r repofoldername\nTo add local repository to remote:\n* Set up local repository, making it use git, git init -b main\n* Add and commit something\n* Create empty repo on GitHub account\n* git remote add origin https://github.com/amyheather/package_test.git\n* git push -u origin main",
    "crumbs": [
      "General",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Git</span>"
    ]
  },
  {
    "objectID": "content/general/git.html#add-rename-remove-push",
    "href": "content/general/git.html#add-rename-remove-push",
    "title": "3  Git",
    "section": "3.4 Add, rename, remove, push",
    "text": "3.4 Add, rename, remove, push\nTo check status: git status\nTo add file: git add filename\nTo add all files: git add --all\nTo remove file: git rm filename\nTo rename file: git mv oldfilename newfilename\nTo add commit message (applies to all staged files): git commit -m \"Message\"\nTo push commits to remote repo: git push",
    "crumbs": [
      "General",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Git</span>"
    ]
  },
  {
    "objectID": "content/general/git.html#undo-git-command",
    "href": "content/general/git.html#undo-git-command",
    "title": "3  Git",
    "section": "3.5 Undo git command",
    "text": "3.5 Undo git command\nTo unstage specific uncommitted file: git reset filename\nTo unstage all uncommitted files: git reset\nTo unstage latest commmit: git reset HEAD^",
    "crumbs": [
      "General",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Git</span>"
    ]
  },
  {
    "objectID": "content/general/git.html#track-large-file",
    "href": "content/general/git.html#track-large-file",
    "title": "3  Git",
    "section": "3.6 Track large file",
    "text": "3.6 Track large file\nGit will give warning for file over 50MB and block file over 100MB. However, you can track (as long as not too big)… * Install git-lfs sufo apt-get install git-lfs * Initialise in repository git-lfs install * Set tracking of a particular file git lfs track 'filename.csv' * Can then git add and commit that file * Recommended that you also add the .gitattributes file produced",
    "crumbs": [
      "General",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Git</span>"
    ]
  },
  {
    "objectID": "content/general/git.html#tags",
    "href": "content/general/git.html#tags",
    "title": "3  Git",
    "section": "3.7 Tags",
    "text": "3.7 Tags\nTag an old release: git tag tagname commit e.g. git tag v0.1.0 f94e8f6\nPush tag to remote: git push origin v0.1.0\nView date of tag (i.e. date of commit that was tagged): git log -1 --format=%ai v0.1.0",
    "crumbs": [
      "General",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Git</span>"
    ]
  },
  {
    "objectID": "content/general/git.html#git-fetch-v.s.-git-pull",
    "href": "content/general/git.html#git-fetch-v.s.-git-pull",
    "title": "3  Git",
    "section": "3.8 Git fetch v.s. git pull",
    "text": "3.8 Git fetch v.s. git pull\ngit fetch downloads remote data, but doesn’t integrate it with your files - it gives you a fresh view but is totally “harmless”, won’t affect local stuff.\ngit pull updates your local files with changes from the remote server, integrating it with the working copies of your files (and will try to merge remote changes in merge conflicts) - so should not do this when you have uncommitted local changes.",
    "crumbs": [
      "General",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Git</span>"
    ]
  },
  {
    "objectID": "content/general/git.html#branches-and-versions",
    "href": "content/general/git.html#branches-and-versions",
    "title": "3  Git",
    "section": "3.9 Branches and versions",
    "text": "3.9 Branches and versions\nTo view branches (current branch marked with star): git branch\nTo create new branch:\ngit branch branchname\ngit push --set-upstream origin branchname\nTo switch branches: git checkout branchname\nTo delete remote branch: git push origin --delete branchname\nTo delete local branch: git branch -d branchname\nTo remove listing of remote branches that have actually been removed: git remote prune origin\nTo check for different between local and remote file:\ngit fetch origin branchname\ngit diff origin/branchname -- filename\nTo merge dev branch with main branch, push, then return to dev:\ngit checkout main\ngit merge dev\ngit push\ngit checkout dev\nTo move to main branch and force local files to match main (loses unstaged commits):\nNot sure this is best way! Need to investigate more\ngit checkout main\ngit fetch\ngit reset --hard origin/main",
    "crumbs": [
      "General",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Git</span>"
    ]
  },
  {
    "objectID": "content/general/git.html#git-stash",
    "href": "content/general/git.html#git-stash",
    "title": "3  Git",
    "section": "3.10 Git stash",
    "text": "3.10 Git stash\nIf you have local uncommitted changes, and realise you are not working on the right branch, you can resolve this using git stash…\ngit stash\ngit checkout branchname\ngit stash apply\nIf there are differences, a merge conflict will appear, and you can resolve using conflict editor in VS Code which shows you the two versions and changes on each line, and as you accept one or the other, it shows you the end result from that.",
    "crumbs": [
      "General",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Git</span>"
    ]
  },
  {
    "objectID": "content/general/git.html#git-fork",
    "href": "content/general/git.html#git-fork",
    "title": "3  Git",
    "section": "3.11 Git fork",
    "text": "3.11 Git fork\nIf you fork a repository and want to push changes to a branch other than main, switch to that branch in the forked repository and make the changes there.",
    "crumbs": [
      "General",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Git</span>"
    ]
  },
  {
    "objectID": "content/general/git.html#commits",
    "href": "content/general/git.html#commits",
    "title": "3  Git",
    "section": "3.12 Commits",
    "text": "3.12 Commits\nTo view commit history: git log\nTo return to a previous commit:\ngit revert --no-commit 65f8ed3..HEAD\ngit commit -m \"Revert back as removing ambulance filters results in heavily skewed means\"\ngit push",
    "crumbs": [
      "General",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Git</span>"
    ]
  },
  {
    "objectID": "content/general/git.html#storing-credentials",
    "href": "content/general/git.html#storing-credentials",
    "title": "3  Git",
    "section": "3.13 Storing credentials",
    "text": "3.13 Storing credentials\nThe credential.helper cache value tells Git to keep your password cached in memory for a particular amount of minutes. The default is 15 minutes, you can set a longer timeout with:\n# Cache for 1 hour\ngit config --global credential.helper \"cache --timeout=3600\"\n\n# Cache for 1 day\ngit config --global credential.helper \"cache --timeout=86400\"\n\n# Cache for 1 week\ngit config --global credential.helper \"cache --timeout=604800\"",
    "crumbs": [
      "General",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Git</span>"
    ]
  },
  {
    "objectID": "content/general/git.html#public-v.s.-private-repositories",
    "href": "content/general/git.html#public-v.s.-private-repositories",
    "title": "3  Git",
    "section": "3.14 Public v.s. private repositories",
    "text": "3.14 Public v.s. private repositories\nNobody can push to public repository until you add them as collaborator.",
    "crumbs": [
      "General",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Git</span>"
    ]
  },
  {
    "objectID": "content/general/latex.html",
    "href": "content/general/latex.html",
    "title": "4  Latex",
    "section": "",
    "text": "4.1 Unicode error\nTo find a unicode error (e.g. for error 202F, it’s a narrow no-break space, which is hard to spot), input the following -",
    "crumbs": [
      "General",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Latex</span>"
    ]
  },
  {
    "objectID": "content/general/latex.html#unicode-error",
    "href": "content/general/latex.html#unicode-error",
    "title": "4  Latex",
    "section": "",
    "text": "\\makeatletter\n\\def\\UTFviii@defined#1{\\ifx#1\\relax!!FIXME!!\\else\\expandafter#1\\fi}\n\\makeatother",
    "crumbs": [
      "General",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Latex</span>"
    ]
  },
  {
    "objectID": "content/general/latex.html#url",
    "href": "content/general/latex.html#url",
    "title": "4  Latex",
    "section": "4.2 URL",
    "text": "4.2 URL\nOptions include: * \\url{} * \\href{}{}",
    "crumbs": [
      "General",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Latex</span>"
    ]
  },
  {
    "objectID": "content/general/markdown.html",
    "href": "content/general/markdown.html",
    "title": "5  Markdown",
    "section": "",
    "text": "Add image with text alongside it: &lt;img align='left' src='image file path' alt='alt_name' width='900'&gt; Text alongside image\nAdmonitions are created using MyST [see documentation]…\nThis page explains how to make custom admonitions.\nBlue:\n`````zweehiujqfgi Note :class: note\nThis is the class “note”.\n\nOrange:\n\n`````{admonition} Important\n:class: important\n\nThis is the class \"important\".\n`````zweehiujqfgi Caution :class: caution\nThis is the class “caution”.\n\n`````{admonition} Warning\n:class: warning\n\nThis is the class \"warning\".\n`````zweehiujqfgi Attention :class: attention\nThis is the class “attention”.\n\nRed:\n\n`````{admonition} Error\n:class: error\n\nThis is the class \"error\".\n`````zweehiujqfgi Danger :class: danger\nThis is the class “danger”.\n\nGreen:\n\n`````{admonition} See Also\n:class: seealso\n\nThis is the class \"seealso\".\n`````zweehiujqfgi Hint :class: hint\nThis is the class “hint”.\n\n`````{admonition} Tip\n:class: tip\n\nThis is the class \"tip\".",
    "crumbs": [
      "General",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Markdown</span>"
    ]
  },
  {
    "objectID": "content/general/machine_learning.html",
    "href": "content/general/machine_learning.html",
    "title": "6  Machine learning",
    "section": "",
    "text": "6.1 Causality\nhttps://youtu.be/-7vSiWRasxY",
    "crumbs": [
      "General",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Machine learning</span>"
    ]
  },
  {
    "objectID": "content/general/machine_learning.html#causality",
    "href": "content/general/machine_learning.html#causality",
    "title": "6  Machine learning",
    "section": "",
    "text": "ML learns outcome by finding distinguishing features and how cases differ\nThere will be missing data… censored data.. bias…\nML learns target outcome only as a function of the observable data\nPeople with expert/domain knowledge would be able to identify some of the people or fields missing\nML also usually fails to account for relationships between variables\nIn order to improve AI learning, need to incorporate causal knowledge\nPearl’s ladder of causation: moving from association (what if I see) to intervention (what if I do) to counterfactuals (what if I had done)\nArgues that you can smartly use smaller datasets in causal models to get smart outcomes rather than just throwing big datasets in models and getting useless outcomes",
    "crumbs": [
      "General",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Machine learning</span>"
    ]
  },
  {
    "objectID": "content/general/open_science.html",
    "href": "content/general/open_science.html",
    "title": "7  Open Science",
    "section": "",
    "text": "7.1 Open Science Framework (OSF)",
    "crumbs": [
      "General",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Open Science</span>"
    ]
  },
  {
    "objectID": "content/general/open_science.html#open-science-framework-osf",
    "href": "content/general/open_science.html#open-science-framework-osf",
    "title": "7  Open Science",
    "section": "",
    "text": "7.1.1 Project\nCreate a project, and store all the files related to that project together (e.g. registrations, archived copies of documents like surveys, pre-print publications).\nTo add project contributors: * Go to Project –&gt; Contributors * Give permissions (Admin, Read/Write, or Read) * If select Bibliographic, this means they’re included in the overall citation for the published project\n\n\n7.1.2 Registrations\nThe purpose of a registration is to be transparent about what you plan to do (to prevent hypothesis after done), so you’re clear what was confirmatory and what was exploratory, and to prevent fishing for results. These are done, for example, before data collection, or after collection and before analysis.\nTo create e.g. pre-registration before data collections: * Go to Project –&gt; Registrations –&gt; New registration * Select OSF Preregistration template, and complete. When done, click register * You cannot modify it after registering (defeats the purpose), but you can make a new registration and link it to your old - click on generate new registration, and then in the description, write something along the lines of ’This is a revised pre-registration relating to the previous pre-registration published (DATE) under the same project anme. See the link: (LINK). Due to (INSERT) reasons, the following changes were made.",
    "crumbs": [
      "General",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Open Science</span>"
    ]
  },
  {
    "objectID": "content/general/zenodo.html",
    "href": "content/general/zenodo.html",
    "title": "8  Zenodo",
    "section": "",
    "text": "8.1 Pushing old releases\nThis method was from https://github.com/zenodo/zenodo/issues/1463#issuecomment-2349177518.\nI made the zenodo_archiver.py file (as below). I set the repository to sync on Zenodo sandbox. I got the access token from the webhook page in the repository settings. I then ran the command python3 zenodo_archiver.py pythonhealthdatascience stars-eom-rcc v1.1.0 {{token}}. You should see `&lt;Response [202]&gt;, which indicates it has been successful.\n(You can do this for actual Zenodo - just remove sandbox. from url below, and sync and use webhook for actual zenodo)",
    "crumbs": [
      "General",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Zenodo</span>"
    ]
  },
  {
    "objectID": "content/general/zenodo.html#pushing-old-releases",
    "href": "content/general/zenodo.html#pushing-old-releases",
    "title": "8  Zenodo",
    "section": "",
    "text": "# Source: https://github.com/zenodo/zenodo/issues/1463#issuecomment-2349177518\n__author__ = ('prohde', 'dr-joe-wirth')\nimport requests, sys\n\n# functions\ndef _submit(user_name:str, repo_name:str, tag:str, access_token:str) -&gt; None:\n    \"\"\"tricks Zenodo into archiving a pre-existing release\n\n    Args:\n        user_name (str): github username\n        repo_name (str): github repository name\n        tag (str): the desired tag to archive\n        access_token (str): the access token from webhook page in repo settings\n    \"\"\"\n    # constant\n    HEADERS = {\"Accept\": \"application/vnd.github.v3+json\"}\n\n    # build the repo\n    repo = \"/\".join((user_name, repo_name))\n    \n    # get the repo response and the release response for the repo\n    repo_response = requests.get(f\"https://api.github.com/repos/{repo}\", headers=HEADERS)\n    release_response = requests.get(f\"https://api.github.com/repos/{repo}/releases\", headers=HEADERS)\n    \n    # get the data for the desired release\n    desired_release = [x for x in release_response.json() if x['tag_name'] == tag].pop()\n    \n    # build the payload for the desired release\n    payload = {\"action\": \"published\", \"release\": desired_release, \"repository\": repo_response.json()}\n    \n    # submit the payload to zenodo's api\n    submit_response = requests.post(\n        f\"https://sandbox.zenodo.org/api/hooks/receivers/github/events/?access_token={access_token}\",\n        json=payload\n    )\n    \n    # print the response\n    print(submit_response)\n\n\ndef _main() -&gt; None:\n    \"\"\"main runner function\n    \"\"\"\n    # parse command line arguments\n    user  = sys.argv[1]\n    repo  = sys.argv[2]\n    tag   = sys.argv[3]\n    token = sys.argv[4]\n    \n    # submit to zenodo\n    _submit(user, repo, tag, token)\n\n\n# entrypoint\nif __name__ == \"__main__\":\n    _main()",
    "crumbs": [
      "General",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Zenodo</span>"
    ]
  },
  {
    "objectID": "content/general/other.html",
    "href": "content/general/other.html",
    "title": "9  Other",
    "section": "",
    "text": "9.1 Resources\nhttps://ml-science-book.com/",
    "crumbs": [
      "General",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Other</span>"
    ]
  },
  {
    "objectID": "content/general/other.html#draw.io",
    "href": "content/general/other.html#draw.io",
    "title": "9  Other",
    "section": "9.2 draw.io",
    "text": "9.2 draw.io\nTo add transparency, select shape then right click &gt; edit style, then add line like opacity=60;\nTo export at higher quality, File &gt; Export as &gt; Advanced &gt; Set to 300DPI (standard for publication).",
    "crumbs": [
      "General",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Other</span>"
    ]
  },
  {
    "objectID": "content/general/other.html#zotero",
    "href": "content/general/other.html#zotero",
    "title": "9  Other",
    "section": "9.3 Zotero",
    "text": "9.3 Zotero\nIf you have an institutional author (e.g. “World Health Organisation” rather than “John Smith”), you should change the field mode from two-field mode to single-field mode. You can do this by clicking the small square icon next to the name (just before the + and - buttons). [source]",
    "crumbs": [
      "General",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Other</span>"
    ]
  },
  {
    "objectID": "content/general/other.html#citation.cff",
    "href": "content/general/other.html#citation.cff",
    "title": "9  Other",
    "section": "9.4 Citation.cff",
    "text": "9.4 Citation.cff\nUse this schema-guide to help produce: https://github.com/citation-file-format/citation-file-format/blob/main/schema-guide.md",
    "crumbs": [
      "General",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Other</span>"
    ]
  },
  {
    "objectID": "content/general/other.html#changelog",
    "href": "content/general/other.html#changelog",
    "title": "9  Other",
    "section": "9.5 Changelog",
    "text": "9.5 Changelog\nKeep a Changelog is one of the first complete guides for writing a changelog.\nCommon Changelog is a style guide adapted from Keep A changelog that is more detailed.",
    "crumbs": [
      "General",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Other</span>"
    ]
  },
  {
    "objectID": "content/python/general.html",
    "href": "content/python/general.html",
    "title": "10  General Python Stuff",
    "section": "",
    "text": "10.1 Jupyter notebook merge conflicts",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>General Python Stuff</span>"
    ]
  },
  {
    "objectID": "content/python/general.html#jupyter-notebook-merge-conflicts",
    "href": "content/python/general.html#jupyter-notebook-merge-conflicts",
    "title": "10  General Python Stuff",
    "section": "",
    "text": "pip install nbdime\nnbdime config-git --enable --global\ngit merge branchname",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>General Python Stuff</span>"
    ]
  },
  {
    "objectID": "content/python/general.html#using-vs-code",
    "href": "content/python/general.html#using-vs-code",
    "title": "10  General Python Stuff",
    "section": "10.2 Using VS Code",
    "text": "10.2 Using VS Code\nTo open VS Code from terminal: code .\nTo activate conda environment in VS Code: Ctrl+Shift+P &gt; Python Interpreter, then select correct environment\nTo view/create settings.json in .vscode folder: Ctrl+Shift+P &gt; Preferences: Open Workspace Settings (JSON)",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>General Python Stuff</span>"
    ]
  },
  {
    "objectID": "content/python/general.html#type-hinting-in-vs-code-with-pylance",
    "href": "content/python/general.html#type-hinting-in-vs-code-with-pylance",
    "title": "10  General Python Stuff",
    "section": "10.3 Type hinting in VS Code with Pylance",
    "text": "10.3 Type hinting in VS Code with Pylance\nExtension &gt; Pylance should be installed\nIn settings.json, set \"python.analysis.typeCheckingMode\": \"strict\" or to \"None\" or \"basic\"",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>General Python Stuff</span>"
    ]
  },
  {
    "objectID": "content/python/general.html#violin-plot",
    "href": "content/python/general.html#violin-plot",
    "title": "10  General Python Stuff",
    "section": "10.4 Violin plot",
    "text": "10.4 Violin plot\ndf = data.groupby('quarter_year')['stroke_severity'].agg(lambda x: list(x))\nfig, ax = plt.subplots()\nax.violinplot(df)\nax.set_xticks(np.arange(1, len(df.index)+1), labels=df.index)\nplt.show()",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>General Python Stuff</span>"
    ]
  },
  {
    "objectID": "content/python/general.html#jupyter-lab",
    "href": "content/python/general.html#jupyter-lab",
    "title": "10  General Python Stuff",
    "section": "10.5 Jupyter lab",
    "text": "10.5 Jupyter lab\nInstall jupyterlab package.\nWhere you want to open, in terminal, type jupyter lab",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>General Python Stuff</span>"
    ]
  },
  {
    "objectID": "content/python/general.html#comparing-two-dataframes",
    "href": "content/python/general.html#comparing-two-dataframes",
    "title": "10  General Python Stuff",
    "section": "10.6 Comparing two dataframes",
    "text": "10.6 Comparing two dataframes\nSimple check if match: df1.equals(df2) Drop duplicate columns: pd.concat([df1, df2]).drop_duplicates(keep=False)",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>General Python Stuff</span>"
    ]
  },
  {
    "objectID": "content/python/general.html#linting",
    "href": "content/python/general.html#linting",
    "title": "10  General Python Stuff",
    "section": "10.7 Linting",
    "text": "10.7 Linting\nInstall flake8 and nbqa packages.\nTo lint file: flake8 filename.py\nTo lint jupyter notebook: nbqa flake8 filename.ipynb\nTo lint within notebook: %load_ext pycodestyle_magic and %pycodestyle_on",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>General Python Stuff</span>"
    ]
  },
  {
    "objectID": "content/python/general.html#compare-dataframes",
    "href": "content/python/general.html#compare-dataframes",
    "title": "10  General Python Stuff",
    "section": "10.8 Compare dataframes",
    "text": "10.8 Compare dataframes\nTrue/False of difference (note: sometimes False but no actual difference - then try assertion below)\ndf1.equals(df2)\nShow differences between them\ndf1.compare(df2)\nGet feedback on exactly what is missing\nfrom pandas.testing import assert_frame_equal\nassert_frame_equal(df1, df2)",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>General Python Stuff</span>"
    ]
  },
  {
    "objectID": "content/python/general.html#package",
    "href": "content/python/general.html#package",
    "title": "10  General Python Stuff",
    "section": "10.9 Package",
    "text": "10.9 Package\nhttps://betterscientificsoftware.github.io/python-for-hpc/tutorials/python-pypi-packaging/ * Make setup.py - if you read from requirements for it, will need to make MANIFEST.in file which includes the line include requirements.txt * python setup.py check * python setup.py sdist bdist_wheel * pip install twine * twine upload --repository-url https://test.pypi.org/legacy/ dist/* * When want to update package: * Delete dist folder * python setup.py sdist bdist_wheel * twine upload --skip-existing --repository-url https://test.pypi.org/legacy/ dist/* * Above uses test pypi URL - for real pypi upload, use https://upload.pypi.org/legacy/ * To install the package locally (rather than from pypi) using your requirements.txt file, if for example it was in a sister directory, add to the text file ../foldername/dist/filename.whl * If you are simultaneously working on the package and other code - so if you want to be getting live changes to the package rather than having to rebuild and reinstall the package each time - use pip install -e /path/to/repo/ - this will use a symbolic link to the repository meaning any changes to the code will also be automatically reflected. For example, when its in a sister directory, pip install -e ../kailo_beewell_dashboard_package. You can also do this from your requirements.txt by adding -e ../packagefolder. If the package was already in your environment, you’ll need to delete it, and make just delete and remake the environment",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>General Python Stuff</span>"
    ]
  },
  {
    "objectID": "content/python/general.html#unit-testing",
    "href": "content/python/general.html#unit-testing",
    "title": "10  General Python Stuff",
    "section": "10.10 Unit testing",
    "text": "10.10 Unit testing\nNote: unit testing is typically for testing function outputs - so although the code below works, you wouldn’t typically write unit tests for this purpose.\nExample:\nimport numpy as np\nimport os\nimport pandas as pd\nimport unittest\n\nclass DataTests(unittest.TestCase):\n    '''Class for running unit tests'''\n\n    # Run set up once for whole class\n    @classmethod\n    def setUpClass(self):\n        '''Set up - runs prior to each test'''\n        # Paths and filenames\n        raw_path: str = './data'\n        raw_filename: str = 'SAMueL ssnap extract v2.csv'\n        clean_path: str = './output'\n        clean_filename: str = 'reformatted_data.csv'\n        # Import dataframes\n        raw_data = pd.read_csv(os.path.join(raw_path, raw_filename),\n                               low_memory=False)\n        clean_data = pd.read_csv(os.path.join(clean_path, clean_filename),\n                                 low_memory=False)\n        # Save to DataTests class\n        self.raw = raw_data\n        self.clean = clean_data\n\n    def freq(self, raw_col, raw_val, clean_col, clean_val):\n        '''\n        Test that the frequency of a value in the raw data is same as\n        the frequency of a value in the cleaned data\n        Inputs:\n        - self\n        - raw_col and clean_col = string\n        - raw_val and clean_val = string, number, or list\n        Performs assertEqual test.\n        '''\n        # If values are not lists, convert to lists\n        if type(raw_val) != list:\n            raw_val = [raw_val]\n        if type(clean_val) != list:\n            clean_val = [clean_val]\n        # Find frequencies and check if equal\n        raw_freq = (self.raw[raw_col].isin(raw_val).values).sum()\n        clean_freq = (self.clean[clean_col].isin(clean_val).values).sum()\n        self.assertEqual(raw_freq, clean_freq)\n\n    def time_neg(self, time_column):\n        '''\n        Function for testing that times are not negative when expected\n        to be positive.\n        Input: time_column = string, column with times\n        '''\n        self.assertEqual(sum(self.clean[time_column] &lt; 0), 0)\n\n    def equal_array(self, df, col, exp_array):\n        '''\n        Function to check that the only possible values in a column are\n        those provided by exp_array.\n        Inputs:\n        - df = dataframe (raw or clean)\n        - col = string (column name)\n        - exp_array = array (expected values for column)\n        '''\n        # Sorted so that array order does not matter\n        self.assertEqual(sorted(df[col].unique()), sorted(exp_array))\n\n    def test_raw_shape(self):\n        '''Test the raw dataframe shape is as expected'''\n        self.assertEqual(self.raw.shape, (360381, 83))\n        \n    def test_id(self):\n        '''Test that ID numbers are all unique'''\n        self.assertEqual(len(self.clean.id.unique()),\n                         len(self.clean.index))\n\n    def test_onset(self):\n        '''Test that onset_known is equal to precise + best estimate'''\n        self.freq('S1OnsetTimeType', ['P', 'BE'], 'onset_known', 1)\n        self.freq('S1OnsetTimeType', 'NK', 'onset_known', 0)\n        \n    def test_time_negative(self):\n        '''Test that times are not negative when expected to be positive'''\n        time_col = ['onset_to_arrival_time',\n                    'call_to_ambulance_arrival_time',\n                    'ambulance_on_scene_time',\n                    'ambulance_travel_to_hospital_time',\n                    'ambulance_wait_time_at_hospital',\n                    'scan_to_thrombolysis_time',\n                    'arrival_to_thrombectomy_time']\n        for col in time_col:\n            with self.subTest(msg=col):\n                self.time_neg(col)\n\n    def test_no_ambulance(self):\n        '''\n        Test that people who do not arrive by ambulance therefore have\n        no ambulance times\n        '''\n        amb_neg = self.clean[(self.clean['arrive_by_ambulance'] == 0) & (\n            (self.clean['call_to_ambulance_arrival_time'].notnull()) |\n            (self.clean['ambulance_on_scene_time'].notnull()) |\n            (self.clean['ambulance_travel_to_hospital_time'].notnull()) |\n            (self.clean['ambulance_wait_time_at_hospital'].notnull()))]\n        self.assertEqual(len(amb_neg.index), 0)\n\n\n\nif __name__ == '__main__':\n    unittest.main()",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>General Python Stuff</span>"
    ]
  },
  {
    "objectID": "content/python/general.html#colour-scales",
    "href": "content/python/general.html#colour-scales",
    "title": "10  General Python Stuff",
    "section": "10.11 Colour scales",
    "text": "10.11 Colour scales\n'''\nHelper functions for creating discrete colour scale sampling in the\ncontinuous scale between two provided colours.\nSource: https://bsouthga.dev/posts/color-gradients-with-python\nDirectly copied from that webpage, all credit to them.\n'''\n\ndef hex_to_RGB(hex):\n  ''' \"#FFFFFF\" -&gt; [255,255,255] '''\n  # Pass 16 to the integer function for change of base\n  return [int(hex[i:i+2], 16) for i in range(1,6,2)]\n\ndef RGB_to_hex(RGB):\n  ''' [255,255,255] -&gt; \"#FFFFFF\" '''\n  # Components need to be integers for hex to make sense\n  RGB = [int(x) for x in RGB]\n  return \"#\"+\"\".join([\"0{0:x}\".format(v) if v &lt; 16 else\n            \"{0:x}\".format(v) for v in RGB])\n\ndef color_dict(gradient):\n  ''' Takes in a list of RGB sub-lists and returns dictionary of\n    colors in RGB and hex form for use in a graphing function\n    defined later on '''\n  return {\"hex\":[RGB_to_hex(RGB) for RGB in gradient],\n      \"r\":[RGB[0] for RGB in gradient],\n      \"g\":[RGB[1] for RGB in gradient],\n      \"b\":[RGB[2] for RGB in gradient]}\n\ndef linear_gradient(start_hex, finish_hex=\"#FFFFFF\", n=10):\n  ''' returns a gradient list of (n) colors between\n    two hex colors. start_hex and finish_hex\n    should be the full six-digit color string,\n    inlcuding the number sign (\"#FFFFFF\") '''\n  # Starting and ending colors in RGB form\n  s = hex_to_RGB(start_hex)\n  f = hex_to_RGB(finish_hex)\n  # Initilize a list of the output colors with the starting color\n  RGB_list = [s]\n  # Calcuate a color at each evenly spaced value of t from 1 to n\n  for t in range(1, n):\n    # Interpolate RGB vector for color at the current value of t\n    curr_vector = [\n      int(s[j] + (float(t)/(n-1))*(f[j]-s[j]))\n      for j in range(3)\n    ]\n    # Add it to our list of output colors\n    RGB_list.append(curr_vector)\n\n  return color_dict(RGB_list)\n\n10.11.1 Save HTML to csv and then import again and convert to PDF (this is not ideal tbh)\nimport weasyprint\nimport csv\n\nwith open(\"out.csv\", \"w\", encoding=\"utf-8\") as csv_file:\n    writer = csv.writer(csv_file)\n    writer.writerow([html_content])\n\nwith open(\"out.csv\", \"r\") as csv_file:\n    csv_text = csv_file.readlines()\n\nhtml_check = ''.join(csv_text)[1:-1]\nweasyprint.HTML(string=html_check).write_pdf('report/report.pdf')",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>General Python Stuff</span>"
    ]
  },
  {
    "objectID": "content/python/environments.html",
    "href": "content/python/environments.html",
    "title": "11  Python environments",
    "section": "",
    "text": "11.1 Conda\nPoetry - you specify python version BUT you have to have it already Venv - no python version Conda - you specific python version and it gets it for you Mamba - quicker version of conda https://alpopkes.com/posts/python/packaging_tools/\nTo see conda environments: conda env list\nTo see packages in current environment: conda list\nTo activate environment: conda activate env_name\nTo create environment from yml file: conda env create --name env_name --file environment.yml\nTo edit yml file from terminal: nano environment.yml\nTo update current environment from yml file: conda env update --file environment.yml --prune\nTo delete environment: deactivate then conda remove -n env_name --all",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Python environments</span>"
    ]
  },
  {
    "objectID": "content/python/environments.html#virtualenv",
    "href": "content/python/environments.html#virtualenv",
    "title": "11  Python environments",
    "section": "11.2 Virtualenv",
    "text": "11.2 Virtualenv\n\n11.2.1 Virtualenvwrapper\nRecommend using virtualenvwrapper so all your environments are stored in the same place, otherwise its easy to forget what you named it\n\npip install virtualenvwrapper\nGo to Home\nnano .bashrc\nAdd the following:\n\nexport WORKON_HOME=$HOME/.virtualenvs\nexport VIRTUALENVWRAPPER_PYTHON=/usr/bin/python3\nexport VIRTUALENVWRAPPER_VIRTUALENV=/usr/local/bin/virtualenv\nsource /usr/local/bin/virtualenvwrapper.sh\n\nReload with source ~/.bashrc\nCreate environment with mkvirtualenv env_name. This will be located in home/.virtualenvs.\nEnvironment activated with workon env_name\nSee list of all available environments with command workon\nDelete environment - rmvirtualenv env_name\n\n\n\n11.2.2 Virtualenv\nWARNING: NEVER NAME YOUR ENVIRONMENT THE SAME AS YOUR FOLDER - as the command for deleting the environment would delete the folder…!\nSteps for setting up virtual environment: 1. If not already installed, pip install virtualenv 2. Create new environment: virtualenv env_name 3. Enter the environment: source env_name/bin/activate 4. Install requirements into environment: pip install -r requirements.txt 5. Update environment from requirements: pip install -r requirements.txt --upgrade 6. Delete environment: deactivate then rm -r env_name be careful! will delete folder of same name! 7. List packages in environment: pip list",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Python environments</span>"
    ]
  },
  {
    "objectID": "content/python/environments.html#docker",
    "href": "content/python/environments.html#docker",
    "title": "11  Python environments",
    "section": "11.3 Docker",
    "text": "11.3 Docker\nSuccessfully installed docker on Ubuntu using following instructions: https://docs.docker.com/engine/install/ubuntu/\nWill need requirements.txt file. To create this from conda environment (with pip install in that environment and environment activated): pip list --format=freeze &gt; requirements.txt (although may not have identifical packages - pip may have fewer). There is a simpler command of pip freeze &gt; requirements.txt but I found that gave odd path references.\nTo create dockerfile:\n* touch Dockerfile\n* nano Dockerfile\n# kailo_dashboards/Dockerfile\n\nFROM python:3.10-slim\n\nWORKDIR /kailo_dashboards\n\nRUN apt-get update && apt-get install -y \\\n    build-essential \\\n    curl \\\n    software-properties-common \\\n    git \\\n    && rm -rf /var/lib/apt/lists/*\n\nCOPY . .\n\nRUN pip3 install -r requirements.txt\n\nEXPOSE 8501\n\nHEALTHCHECK CMD curl --fail http://localhost:8501/_stcore/health\n\nENTRYPOINT [\"streamlit\", \"run\", \"streamlit_app.py\", \"--server.port=8501\", \"--se&gt;\nExplanation of Dockerfile:\n* FROM - sets base image, various possible - https://hub.docker.com/_/python\n* WORKDIR - sets working directory\n* RUN apt-get… to install git\n* COPY . . to get copy of all app files. If it was in a public GitHub repo, you could do RUN git clone https://github.com/amyheather/kailo_area_dashboard ., and if it was in a private repo, you could use SSH\n* Install dependencies from requirements.txt\n* EXPOSE - informs Docker that the container listens on the specified network ports at runtime. For streamlit, Your container needs to listen to Streamlit’s (default) port 8501\n* HEALTHCHECK - tells Docker how to test a container to check that it is still working. Your container needs to listen to Streamlit’s (default) port 8501\n* ENTRYPOINT - allows you to configure a container that will run as an executable. Here, it also contains the entire streamlit run command for your app, so you don’t have to call it from the command line\nTo set up docker:\n* sudo docker build -t streamlit . to build an image from the Dockerfile. -t is used to tag the file, here tagged as streamlit. Can see streamlit image under repository column if run sudo docker images",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Python environments</span>"
    ]
  },
  {
    "objectID": "content/python/seeds.html",
    "href": "content/python/seeds.html",
    "title": "12  Seeds",
    "section": "",
    "text": "12.1 Packages\nIn NumPy, the two main packages you’ll come across in relation to random seeds are numpy and random.\nIn this script, we will demonstrate using numpy.\nimport numpy as np",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Seeds</span>"
    ]
  },
  {
    "objectID": "content/python/seeds.html#generating-a-random-number",
    "href": "content/python/seeds.html#generating-a-random-number",
    "title": "12  Seeds",
    "section": "12.2 Generating a random number",
    "text": "12.2 Generating a random number\nIn order to illustrate how different practices with random number generators work, we will use the random() function, which returns a random float between 0 and 1.\nWithout a seed, you can see that we’ll get a different answer each time.\nIn Python, these values are produced by a pseudorandom number generator.\n\nprint(np.random.random())\nprint(np.random.random())\n\n0.9160257015890815\n0.014614208720352462",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Seeds</span>"
    ]
  },
  {
    "objectID": "content/python/seeds.html#what-is-a-pseudorandom-number-generator",
    "href": "content/python/seeds.html#what-is-a-pseudorandom-number-generator",
    "title": "12  Seeds",
    "section": "12.3 What is a pseudorandom number generator?",
    "text": "12.3 What is a pseudorandom number generator?\nA random number generator (RNG) is a system generating random numbers from a true source of randomness (e.g. electrostatic noise converted into random numbers). However, we don’t need true randomness, so we use a pseudorandom number generator (PRNG).\nA PRNG is also known as a deterministic random bit generator (DRBG). It generates a sequence of numbers that look close to random - but are not truly random, since it is determined by an intial value called the seed. If you do not set a seed, then it may use the current system time in seconds or milliseconds as the seed. [source 1] [source 2]",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Seeds</span>"
    ]
  },
  {
    "objectID": "content/python/seeds.html#global-random-number-generator",
    "href": "content/python/seeds.html#global-random-number-generator",
    "title": "12  Seeds",
    "section": "12.4 Global random number generator",
    "text": "12.4 Global random number generator\nWe have what can be referred to a the “global” RNG for our script. It is common practice to set the seed for the global RNG at the start of the script using random.seed() or np.random.seed().[source]\nAs you’ll see, setting this seed ensures you then get the same result.\n\nnp.random.seed(5)\nprint(np.random.random())\n\nnp.random.seed(5)\nprint(np.random.random())\n\n0.22199317108973948\n0.22199317108973948\n\n\nHowever, this is actually considered bad practice. Global variables (like a global RNG) are not recommended as they can have influences throughout your script.\nYou could change the variable for one part of the script and inadvertently impact elsewhere in the script - such as in the case of random numbers, where you could unknowingly reset the seed elsewhere in the codebase.\nThe numpy RNG policy instead recommends that you create a generator object with a seed and pass that around…[source]",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Seeds</span>"
    ]
  },
  {
    "objectID": "content/python/seeds.html#create-a-generator-object-and-pass-it-around",
    "href": "content/python/seeds.html#create-a-generator-object-and-pass-it-around",
    "title": "12  Seeds",
    "section": "12.5 Create a generator object and pass it around",
    "text": "12.5 Create a generator object and pass it around\nYou can use np.random.default_rng() to create a RNG.\nYou then use that RNG object when calling .random().\n\n# Illustration that this enables reproducibility\nrng = np.random.default_rng(seed=12345)\nprint(rng.random())\n\nrng = np.random.default_rng(seed=12345)\nprint(rng.random())\n\n# Showing that this is a generator object\nprint(rng)\n\n0.22733602246716966\n0.22733602246716966\nGenerator(PCG64)\n\n\nIt has generally been said that you should create a single PRNG and draw from it sequentially (rather than creating new PRNGs within your script). This is having multiple streams, you can be concerned about what the streams from different seeds might be less independent/more correlated that those creates from a single seed. However, the numpy developers have explained that the size and entropy seeding of their PRNGs means that this concern is not particularly relevant. [source 1] [source 2]\n\n# Using a single PRNG...\nrng = np.random.default_rng(seed=12345)\nprint(rng.random())\nprint(rng.integers(1,5))\nprint(rng.standard_normal())\n\n0.22733602246716966\n4\n-0.8706617379590857",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Seeds</span>"
    ]
  },
  {
    "objectID": "content/python/seeds.html#parallel-random-number-generation",
    "href": "content/python/seeds.html#parallel-random-number-generation",
    "title": "12  Seeds",
    "section": "12.6 Parallel random number generation",
    "text": "12.6 Parallel random number generation\nIf we have a stochastic simulation and we want to run it thirty times, we might do so via paralleling processing (such as with the joblib library). We don’t want the same results each run, but we do want to be able to reproduce our set of thirty runs. We need to be wary that: * Not setting a seed, the forked Python processes might “use the same random seed, generated for instance from system entropy, and thus produce the exact same outputs” * Setting a single seed, the RNG will be deep copied * Setting different seeds for each RNG, we cannot be sure that the RNGs are statistically independent. [source]\nNumPy offers a few different methods to create independent PRNGs: * SeedSequence spawning * Sequence of integer seeds * Independent BitGenerator * Jumping the BitGenerator state [source]\nWe’ll focus on SeedSequence spawning…",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Seeds</span>"
    ]
  },
  {
    "objectID": "content/python/seeds.html#seedsequence-spawning",
    "href": "content/python/seeds.html#seedsequence-spawning",
    "title": "12  Seeds",
    "section": "12.7 SeedSequence spawning",
    "text": "12.7 SeedSequence spawning\nYou provide a seed to SeedSequence. It then uses this seed to create N BitGenerator states that are highly probable to be independent of each other. Conversion to bit generator states “uses hashing techniques to ensure that low-quality seeds are turned into high quality initial states”. You can then use these to instantiate PRNGs.[source]\n\nss = np.random.SeedSequence(entropy=12345)\nprint(f'ss: {ss}')\n\n# Spawn 10 child SeedSequences to pass to child processes\nchild_seeds = ss.spawn(10)\nprint(f'example child_seeds: {child_seeds[0:2]}')\n\n# Create 10 PRNGs\nprngs = [np.random.default_rng(s) for s in child_seeds]\nprint(f'example PRNGs: {prngs[0:2]}')\n\nss: SeedSequence(\n    entropy=12345,\n)\nexample child_seeds: [SeedSequence(\n    entropy=12345,\n    spawn_key=(0,),\n), SeedSequence(\n    entropy=12345,\n    spawn_key=(1,),\n)]\nexample PRNGs: [Generator(PCG64) at 0x70A2B3C559A0, Generator(PCG64) at 0x70A2B3C54D60]\n\n\nFor convenience, you can spawn independent child generators directly from a parent generator (without needing to directly use SeedSequence). Those children can likewise be used to spawn yet more independent generators.[source]\n\nparent_rng = np.random.default_rng(12345)\nprint(parent_rng)\n\nstreams = parent_rng.spawn(10)\nstreams\n\nGenerator(PCG64)\n\n\n[Generator(PCG64) at 0x70A2B3C55E00,\n Generator(PCG64) at 0x70A2B3C565E0,\n Generator(PCG64) at 0x70A2B3C56340,\n Generator(PCG64) at 0x70A2B3C56500,\n Generator(PCG64) at 0x70A2B3C54BA0,\n Generator(PCG64) at 0x70A2B3C567A0,\n Generator(PCG64) at 0x70A2B3C56880,\n Generator(PCG64) at 0x70A2B3C566C0,\n Generator(PCG64) at 0x70A2B3C54C80,\n Generator(PCG64) at 0x70A2B3C56DC0]",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Seeds</span>"
    ]
  },
  {
    "objectID": "content/python/seeds.html#creating-individual-random-number-streams-for-sampling",
    "href": "content/python/seeds.html#creating-individual-random-number-streams-for-sampling",
    "title": "12  Seeds",
    "section": "12.8 Creating individual random number streams for sampling",
    "text": "12.8 Creating individual random number streams for sampling\nIn stochastic simulations, we sample from a distribution. However, it is important to set individual random number streams for sampling, as illustrated below (based on source).\nIn our example, we sample acute length of stay (LoS) from an exponential distribution and rehabilitation LoS from a uniform distribution.\n\n# Seed and parameters for our distributions\nseed = 42\nacute_mean = 32\nrehab_min = 15\nrehab_max = 80\nn_patients = 5\n\n# Instantiate PRNG\nrng = np.random.default_rng(seed)\n\n# Sample 5 \nacute_los = rng.exponential(scale=1/acute_mean, size=n_patients)\nrehab_los = rng.uniform(low=rehab_min, high=rehab_max, size=n_patients)\n\nprint(f'Acute LoS: {acute_los}')\nprint(f'Rehab LoS: {rehab_los}')\n\nAcute LoS: [0.07513152 0.07300593 0.07452378 0.00874357 0.00270117]\nRehab LoS: [78.41545286 64.47408063 66.09417984 23.32738612 44.27508596]\n\n\nThen we reset the stream using the same seed but limit to simulating just two patients.\nAs such, we might expect to get this when rerunning the code:\nwe reduce the size two (for example, reflecting less patients arriving per hour). We might expect to get:\n\nAcute LoS: [0.07513152 0.07300593]\nRehab LoS: [78.41545286 64.47408063]\n\nBut we do not! Instead we get -\n\n# Change to 2 patients\nn_patients = 2\n\n# Instantiate PRNG\nrng = np.random.default_rng(seed)\n\n# Sample 5 \nacute_los = rng.exponential(scale=1/acute_mean, size=n_patients)\nrehab_los = rng.uniform(low=rehab_min, high=rehab_max, size=n_patients)\n\nprint(f'Acute LoS: {acute_los}')\nprint(f'Rehab LoS: {rehab_los}')\n\nAcute LoS: [76.93467533 74.75806899]\nRehab LoS: [70.80886479 60.32892189]\n\n\nThis is because both sampling methods as using the same PRNG/same pseudo random number stream.\nThis problem will introduce noise between experiments, where variation in results are due to this and not actually the change we made to the simulation.\nHence, it is recommended to use individual PRNGs for each sampling distribution (as in this example).\n\n# Source: https://pythonhealthdatascience.github.io/stars-simpy-example-docs/content/02_model_code/04_model.html\n\nclass Exponential:\n    def __init__(self, mean, random_seed=None):\n        self.rng = np.random.default_rng(seed=random_seed)\n        self.mean = mean\n    def sample(self, size=None):\n        return self.rng.exponential(self.mean, size=size)\n\n\nclass Normal:\n    def __init__(self, mean, sigma, random_seed=None):\n        self.rng = np.random.default_rng(seed=random_seed)\n        self.mean = mean\n        self.sigma = sigma\n    def sample(self, size=None):\n        return self.rng.normal(self.mean, self.sigma, size=size)\n\n\n# Parameters\nseed = 42\ntriage_mean = 3\nexam_mean = 16\nexam_var = 3\nn_streams = 20\n\n# Spawning seeds to use to create generators\nseed_sequence = np.random.SeedSequence(seed)\nseeds = seed_sequence.spawn(n_streams)\n\n# Creating the seperate distributions...\ntriage_dist = Exponential(triage_mean, random_seed = seeds[0])\nexam_dist = Normal(exam_mean, np.sqrt(exam_var), random_seed = seeds[1])\n\n# Sampling five from each\nprint('Sampling five from each...')\nprint(triage_dist.sample(5))\nprint(exam_dist.sample(5))\n\n# Creating the seperate distributions\ntriage_dist = Exponential(triage_mean, random_seed = seeds[0])\nexam_dist = Normal(exam_mean, np.sqrt(exam_var), random_seed = seeds[1])\n\n# Sampling two from each\nprint('Sampling two from each...')\nprint(triage_dist.sample(2))\nprint(exam_dist.sample(2))\n\nSampling five from each...\n[2.90633292 5.48489155 1.37244157 0.83179124 9.79691459]\n[18.17284798 17.05012411 13.67874431 14.14819982 18.3269625 ]\nSampling two from each...\n[2.90633292 5.48489155]\n[18.17284798 17.05012411]\n\n\nBehaviour as expected - yay!\nAnother way of setting up this same thing (from me, just to help illustrate it in another way, but you’ll see it is the same results)…\nFrom looking at it, you’ll also understand the benefit of the way the Classes are set up above.\n\n# Create parent generator and spawn child sequences\nparent_rng = np.random.default_rng(seed)\ngenerators = parent_rng.spawn(10)\n\n# Create generators for each distribution using the children\ntriage_rng = generators[0]\nexam_rng = generators[1]\n\n# Sample five from each\nprint('Sampling five from each...')\nprint(triage_rng.exponential(triage_mean, 5))\nprint(exam_rng.normal(exam_mean, np.sqrt(exam_var), 5))\n\n# Remaking to illustrate reproducibility\n# Sample two from each\nparent_rng = np.random.default_rng(seed)\ngenerators = parent_rng.spawn(10)\ntriage_rng = generators[0]\nexam_rng = generators[1]\nprint('Sampling two from each...')\nprint(triage_rng.exponential(triage_mean, 2))\nprint(exam_rng.normal(exam_mean, np.sqrt(exam_var), 2))\n\nSampling five from each...\n[2.90633292 5.48489155 1.37244157 0.83179124 9.79691459]\n[18.17284798 17.05012411 13.67874431 14.14819982 18.3269625 ]\nSampling two from each...\n[2.90633292 5.48489155]\n[18.17284798 17.05012411]",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Seeds</span>"
    ]
  },
  {
    "objectID": "content/python/seeds.html#legacy-random-generation",
    "href": "content/python/seeds.html#legacy-random-generation",
    "title": "12  Seeds",
    "section": "12.9 Legacy Random Generation",
    "text": "12.9 Legacy Random Generation\nRandomState is no longer supported. It is the old way of creating a random number generator, allowing you to set the seed just for the generator (and not messing about with the global random generation).\n\nrng = np.random.RandomState(12345)\nrng.randint(-1, 2, size=10)\n\narray([ 1,  0,  0,  0, -1,  0,  1,  1,  0,  1])",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Seeds</span>"
    ]
  },
  {
    "objectID": "content/python/simpy.html",
    "href": "content/python/simpy.html",
    "title": "13  DES using SimPy",
    "section": "",
    "text": "13.1 Basic model structure (using object-oriented programming)\nTwo main ways to programme in python: * Functional - write functions (designed to do one thing) * Object oriented - create objects which have attributes and methods. These objects are created as classes. This is useful when have lots of logic to use, or when want to make copies of a similar thing.\nA basic structure of a SimPy model: * Class to store global model parameters (which you won’t create instances of, but instead will refer to directly when need to access something, hence might be lower case) * Class to represent entity (which will have their attributes) * Class to represent system (which will have generators, entity journeys, and store simpy environment) * Class to represent trial (i.e. batch of simulation runs with methods to run them, and to store, record and display results from the trial) [source]\nflowchart TD;\n\n    param(\"&lt;b&gt;Global model parameters&lt;/b&gt;\");\n    entity(\"&lt;b&gt;Entity&lt;/b&gt;&lt;br&gt;Attributes\");\n    system(\"&lt;b&gt;System&lt;/b&gt;&lt;br&gt;Generators&lt;br&gt;Resources&lt;br&gt;Entity journeys&lt;br&gt;Stores simpy environment\");\n    trial(\"&lt;b&gt;Trial&lt;/b&gt;&lt;br&gt;Run model&lt;br&gt;Record and display result\")\nWhere store results:\nflowchart TD;\n\n    param(\"&lt;b&gt;Global model parameters&lt;/b&gt;\");\n    entity(\"&lt;b&gt;Entity&lt;/b&gt;&lt;br&gt;Entity queue time\");\n    system(\"&lt;b&gt;System&lt;/b&gt;&lt;br&gt;Create and populate&lt;br&gt;results dataframe&lt;br&gt;with row for each entity\");\n    trial(\"&lt;b&gt;Trial&lt;/b&gt;&lt;br&gt;Run model&lt;br&gt;Record and display result\")\n:::{dropdown} View full code example\nCode source: HSMA\nThis is a basic example (e.g. doesn’t have sim-tools distributions with reproducibility)\n:::\nWhen we use env.timeout, it is specification to each entity (i.e. time passes for just that entity), and so if you kill a process, only that entity is affected. However, it is possible to have some specific types of process that interrupt each other. [source]\nTo add another activity, just write it after the first one in the pathway generator function (although outside the with statement, else will drag resource from previous activity with it, unless you want that) - see example.\nYou can model branching paths using conditional logic - see example.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>DES using SimPy</span>"
    ]
  },
  {
    "objectID": "content/python/simpy.html#basic-model-structure-using-object-oriented-programming",
    "href": "content/python/simpy.html#basic-model-structure-using-object-oriented-programming",
    "title": "13  DES using SimPy",
    "section": "",
    "text": "import simpy\nimport random\nimport pandas as pd\n\n\n# Global parameter values\n# Don't make instance of this class, just access it directly\nclass g:\n    patient_inter = 5\n    mean_n_consult_time = 6\n    number_of_nurses = 1\n    sim_duration = 120\n    number_of_runs = 5\n\n\n# Entity (Patient) with two attributes: ID and time queueing\nclass Patient:\n    def __init__(self, p_id):\n        self.id = p_id\n        self.q_time_nurse = 0\n\n\nclass Model:\n    # Constructor to set up model for run\n    def __init__(self, run_number):\n        self.env = simpy.Environment()\n        self.patient_counter = 0\n        self.nurse = simpy.Resource(self.env, capacity=g.number_of_nurses)\n        self.run_number = run_number\n        # Blank dataframe for results\n        self.results_df = pd.DataFrame()\n        self.results_df[\"Patient ID\"] = [1]\n        self.results_df[\"Q Time Nurse\"] = [0.0]\n        self.results_df[\"Time with Nurse\"] = [0.0]\n        self.results_df.set_index(\"Patient ID\", inplace=True)\n        # Blank attribute to store mean queue time of run\n        self.mean_q_time_nurse = 0\n\n    # Generator function for patient arriving\n    def generator_patient_arrivals(self):\n        # Infinite loop does this indefinitely during simulation run\n        while True:\n            self.patient_counter += 1\n            # Create a new patient\n            p = Patient(self.patient_counter)\n            # Send through process\n            self.env.process(self.attend_clinic(p))\n            # Sample time to next patient (inter-arrival time)\n            sampled_inter = random.expovariate(1.0 / g.patient_inter)\n            # Freeze this instance of this function until time has elapsed\n            yield self.env.timeout(sampled_inter)\n\n    # Generator function for patient going through clinic\n    def attend_clinic(self, patient):\n        # Wait start time\n        start_q_nurse = self.env.now\n        # Request a nurse resource\n        with self.nurse.request() as req:\n            # Queue until resource available\n            yield req\n            # Wait end time\n            end_q_nurse = self.env.now\n            patient.q_time_nurse = end_q_nurse - start_q_nurse\n            # Sample time with nurse\n            sampled_nurse_act_time = random.expovariate(1.0 /\n                                                        g.mean_n_consult_time)\n            # Store queue time and time with nurse in results\n            self.results_df.at[patient.id, \"Q Time Nurse\"] = (\n                patient.q_time_nurse)\n            self.results_df.at[patient.id, \"Time with Nurse\"] = (\n                sampled_nurse_act_time)\n            # Freeze this instance of this function (i.e. for this entity) for the time spent with nurse\n            yield self.env.timeout(sampled_nurse_act_time)\n            # Sink\n\n\n    def calculate_run_results(self):\n        # Find mean queue time\n        self.mean_q_time_nurse = self.results_df[\"Q Time Nurse\"].mean()\n\n\n    def run(self):\n        # Start up DES entity generators that create new patients\n        self.env.process(self.generator_patient_arrivals())\n        # Run the model\n        self.env.run(until=g.sim_duration)\n        # Calculate results\n        self.calculate_run_results()\n        # Print results\n        print (f\"Run Number {self.run_number}\")\n        print (self.results_df)\n\n\nclass Trial:\n    # Set up dataframe to store results from each run\n    def  __init__(self):\n        self.df_trial_results = pd.DataFrame()\n        self.df_trial_results[\"Run Number\"] = [0]\n        self.df_trial_results[\"Mean Q Time Nurse\"] = [0.0]\n        self.df_trial_results.set_index(\"Run Number\", inplace=True)\n\n    # Print trial results\n    def print_trial_results(self):\n        print (\"Trial Results\")\n        print (self.df_trial_results)\n\n    def run_trial(self):\n        for run in range(g.number_of_runs):\n            # Conduct run of model\n            my_model = Model(run)\n            my_model.run()\n            # Store results from run\n            self.df_trial_results.loc[run] = [my_model.mean_q_time_nurse]\n        # Print trial results\n        self.print_trial_results()\n\n# To run the above, create instance of trial class and run it\nmy_trial = Trial()\nmy_trial.run_trial()",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>DES using SimPy</span>"
    ]
  },
  {
    "objectID": "content/python/simpy.html#alternatives-layouts",
    "href": "content/python/simpy.html#alternatives-layouts",
    "title": "13  DES using SimPy",
    "section": "13.2 Alternatives layouts",
    "text": "13.2 Alternatives layouts\nMostly below explains modifications based on the HSMA model layout. However, there are lots of different ways you can choose to lay out a SimPy model. Some more examples are provided below…\n:::{dropdown} Basic example from Tom on MSc\nExample source.\nYou can see it’s pretty similar. Ignoring the slight differences in what is being modelled (and that is not collecting results), the main differences in how code is structured are: * No class for global parameters - defined within script * Function for patient attending clinic (service()) is within Patient rather than Model * Environment, Resource, run and process outside of the Model\nclass Patient:\n    '''\n    Encapsulates the process a patient caller undergoes when they dial 111\n    and speaks to an operator who triages their call.\n    '''\n    def __init__(self, identifier, env, operators):\n        '''\n        Constructor method\n        \n        Params:\n        -----\n        identifier: int\n            a numeric identifier for the patient.\n            \n        env: simpy.Environment\n            the simulation environment\n            \n        operators: simpy.Resource\n            the call operators\n        '''\n        self.identifier = identifier\n        self.env = env\n        self.operators = operators\n         \n    def service(self):\n        '''\n        simualtes the service process for a call operator\n        \n        1. request and wait for a call operator\n        2. phone triage (triangular)\n        3. exit system\n        '''\n        # record the time that call entered the queue\n        start_wait = env.now\n\n        # request an operator \n        with self.operators.request() as req:\n            yield req\n            \n            # record the waiting time for call to be answered\n            self.waiting_time = self.env.now - start_wait\n            print(f'operator answered call {self.identifier} at ' \\\n                      + f'{self.env.now:.3f}')\n            \n            # sample call duration.\n            call_duration = np.random.triangular(left=0.01, mode=0.12, \n                                                 right=0.16)\n            yield self.env.timeout(call_duration)\n            \n            print(f'call {self.identifier} ended {self.env.now:.3f}; ' \\\n                      + f'waiting time was {self.waiting_time:.3f}')\n\n\nclass UrgentCareCallCentre:\n    '''\n    111 call centre model\n    '''\n    def __init__(self, env, operators):\n        '''\n        Constructor method\n        \n        Params:\n        --------\n        env: simpy.Environment\n        \n        operators: simpy.Resource\n            A pool of call operators triage incoming patient calls.\n        '''\n        self.env = env\n        self.operators = operators \n            \n    def arrivals_generator(self):\n        '''\n        IAT is exponentially distributed with mean\n\n        Parameters:\n        ------\n        env: simpy.Environment\n\n        operators: simpy.Resource\n            the call operators.\n        '''\n\n        # use itertools as it provides an infinite loop with a counter variable\n        for caller_count in itertools.count(start=1):\n\n            # 100 calls per hour (units = hours). Time between calls is 1/100\n            inter_arrival_time = np.random.exponential(1/100)\n            yield env.timeout(inter_arrival_time)\n\n            print(f'call arrives at: {env.now:.3f}')\n            new_caller = Patient(caller_count, self.env, self.operators)\n            env.process(new_caller.service())\n\n\n# model parameters\nRUN_LENGTH = 0.25\nN_OPERATORS = 13\n\n# create simpy environment and operator resources\nenv = simpy.Environment()\noperators = simpy.Resource(env, capacity=N_OPERATORS)\n\n# create a model\nmodel = UrgentCareCallCentre(env, operators)\n\nenv.process(model.arrivals_generator())\nenv.run(until=RUN_LENGTH)\nprint(f'end of run. simulation clock time = {env.now}')\n:::",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>DES using SimPy</span>"
    ]
  },
  {
    "objectID": "content/python/simpy.html#producing-entity-arrivals-using-generator-functions",
    "href": "content/python/simpy.html#producing-entity-arrivals-using-generator-functions",
    "title": "13  DES using SimPy",
    "section": "13.3 Producing entity arrivals (using generator functions)",
    "text": "13.3 Producing entity arrivals (using generator functions)\n:::{dropdown} What are generator functions?\nIn a normal function we return values, which returns a value then terminates the function. In a generator function we yield values, which returns a value then pauses execution by saving states. * Local variables and their states are remembered between successive calls of the generator function. * A generator function is creating an iterator and returning an iterator object * We can use multiple yield statements in the generator function\nExample:\n# Source: https://www.geeksforgeeks.org/generators-in-python/\n\n# Generator function\ndef simple_generator(): \n    yield 1\n    yield 2\n    yield 3\n\n# Generator object\nx = simple_generator()\n\n# Iterating over the generator object using next\nprint(next(x))\nprint(next(x))\nprint(next(x))\n[source]\nWhy use them? * Easier to implement then iterators * Memory efficient - a normal function can return a sequence of items but it has to create a sequence in memory before it can give result - whilst a generator function produces one output at a time * Infinite sequence - can’t store infinite sequences in a given memory, but as generators only produce one item at a time, they can present an infinite stream of data [source]\nYou can make a python generator expression (instead of a function) which is a short-hand form of a generator function. You implement them similar to lambda functions but use round brackets. The difference here is: * List comprehension returns list of items * Generator expression returns an iterable object\nExample:\n# List comprehension\nlist_ = [i for i in range(x) if i % 2 == 0]\n\n# Generator expression\nlist_ = (i for i in range(x) if i % 2 == 0)\n[source]\n:::\nThe generator functions are used for entities arriving into a process * Set up simpy environment, customer counter, resource, run number, dataframe to store results * Create generator function to represent customer arrivals, which creates new customer, uses another generator (for calls), then moves time forward by inter-arrival time (which know from sampling from distribution) * Have generator function to represent customer calling helpline - requests resource, calculates time they had queued, samples time then spent with resource, then move forward by that amount of time (i.e. free that function in place for the activity time sample above) [source]",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>DES using SimPy</span>"
    ]
  },
  {
    "objectID": "content/python/simpy.html#resources",
    "href": "content/python/simpy.html#resources",
    "title": "13  DES using SimPy",
    "section": "13.4 Resources",
    "text": "13.4 Resources\n\n13.4.1 Requesting multiple resources simultaneously\nFor example, if you need to request a nurse and a room, before can proceed with appointment.\nTo set this up, create the resources, with attributes and columns in the results dataframe to record queue time and appointment time.\nYou’ll have conditional logic to check if have one resource (and need to wait for other), or have both resources available at once.\nWe request the resources without indentation, and so manually specifying when to stop using the resource which makes it easier when you are writing code that requests multiple resources at once.\n# There are two ways to request a resource in SimPy...\n\n# Method 1\nwith self.receptionist.request() as req:\n    yield req\n    # Rest of code here, to run whilst holding resource\n\n# Method 2\nnurse_request = self.receptionist.request()\nyield nurse_request\n# Rest of code here, to run whilst holding resource\nself.nurse.release(nurse_request)\n[source]\n\n\n13.4.2 Priority queue\nCreate priority based queueing with PriorityResource. It’s like the standard SimPy Resource class, but has functionality that allows it to select which entity to pull out of queue based on a priority value. To use: 1. Set up resource using PriorityResource rather than Resource 2. Have attribute in each entity that gives their priority (lower value = higher priority) When request PriorityResource, tell it what attribute it should use to determine priority in queue 3. If there are a few people waiting, it will just the person with the highest priority. If people have the same priority, it will choose the oldest of those.\nRemember that our results only include people who get seen by the nurse - and so when the model stops, there could still be loads of people waiting in a queue - you’ll want to account for that. [source]\n\n\n13.4.3 Resource unavailability\nIf the level of resource varies (e.g. its not 5 doctors constantly available 24/7), then you can model this in SimPy by “obstructing” a resource for a certain amount of time.\nExample: Nurse takes 15 minute break every 2 hours 1. Set frequency and duration of unavailability as parameter values in parameter class 2. Set up nurse as PriorityResource 3. Create entity generator to demand the nurse resource with a higher priority than any patient every 2 hours (i.e. use a negative value), which will freeze the nurse with them for 15 minutes (but this means the nurse will complete the current patient and won’t walk out midway through) 4. Set up the new generator running in the run method of the Model class [source]",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>DES using SimPy</span>"
    ]
  },
  {
    "objectID": "content/python/simpy.html#sampling-from-distributions",
    "href": "content/python/simpy.html#sampling-from-distributions",
    "title": "13  DES using SimPy",
    "section": "13.5 Sampling from distributions",
    "text": "13.5 Sampling from distributions\nSim-tools contains a bunch of functions you can use. This are set up to have individual seeds, to esaily enable reproducibility.\nExample:\nfrom sim_tools.distributions import Exponential\n\n# Later in code...\n\nself.patient_inter_arrival_dist = Exponential(mean = g.patient_inter, random_seed = self.run_number*2)\nThe Exponential Class from the package (you’ll see there’s actually only a few lines of code):\nclass Exponential(Distribution):\n    \"\"\"\n    Convenience class for the exponential distribution.\n    packages up distribution parameters, seed and random generator.\n    \"\"\"\n\n    def __init__(self, mean: float, random_seed: Optional[int] = None):\n        \"\"\"\n        Constructor\n\n        Params:\n        ------\n        mean: float\n            The mean of the exponential distribution\n\n        random_seed: int, optional (default=None)\n            A random seed to reproduce samples.  If set to none then a unique\n            sample is created.\n        \"\"\"\n        super().__init__(random_seed)\n        self.mean = mean\n\n    @abstractmethod\n    def sample(self, size: Optional[int] = None) -&gt; float | np.ndarray:\n        \"\"\"\n        Generate a sample from the exponential distribution\n\n        Params:\n        -------\n        size: int, optional (default=None)\n            the number of samples to return.  If size=None then a single\n            sample is returned.\n        \"\"\"\n        return self.rng.exponential(self.mean, size=size)\n[source]",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>DES using SimPy</span>"
    ]
  },
  {
    "objectID": "content/python/simpy.html#variable-arrival-rates-i.e.-time-dependent-arrivals",
    "href": "content/python/simpy.html#variable-arrival-rates-i.e.-time-dependent-arrivals",
    "title": "13  DES using SimPy",
    "section": "13.6 Variable arrival rates (i.e. time-dependent arrivals)",
    "text": "13.6 Variable arrival rates (i.e. time-dependent arrivals)\nYou could have variable arrival rates (e.g. high in afternoon, low at night). To model this you can use sim-tools which has a class that creates a non-stationary poisson process via thinning. [source]\n:::{dropdown} What is a non-stationary Poisson process with thinning?\nFirst, we define a random process (or stochastic process) as a collection of random variables usually indexed by time. For each time point, the value is a random variable. They can be classified as continuous-time or discrete-time.\nOne example of a random process is the Poisson process. This is a continuous-time random process.[source] It is typically used when we want to account the occurence of something that happens at a certain rate over some time interval but is also completely at random, with occurences independent of each other (e.g. know that area has rate of 2 earthquakes a month, although their timing is otherwise completely random).[source]\nA Poisson process - also known as stationary or time-stationary or time-homogenous Poisson process - has a fixed and constant rate over time. However, if the rate varies over time (e.g. higher rate of arrivals in afternoon than at night), then it is a non-stationary Poisson process.[source] Hence, a non-stationary Poisson process is an arrival process with an arrival rate that varies by time.[source]\nA non-stationary Poisson process can be generated either by thinning or rate inversion. The thinning method involves first generating a stationary Poisson process with a constant rate, but then rejecting potential arrivals at a given time by a given probability. [source] In other words, thinning is an acceptance-rejection sampling method[source] from a time dependent distribution where each time period follows its own exponential distribution.[source]\n:::\nTo set this up we: * Provide dataframe with mean IAT at various time points * Set up arrival distribution in Model class with NSPPThinning() [source]",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>DES using SimPy</span>"
    ]
  },
  {
    "objectID": "content/python/simpy.html#tracking-resource-utilisation",
    "href": "content/python/simpy.html#tracking-resource-utilisation",
    "title": "13  DES using SimPy",
    "section": "13.7 Tracking resource utilisation",
    "text": "13.7 Tracking resource utilisation\n\n13.7.1 Method 1. Simple average across the run\nWe can track how long each entity spends using a resource, then sum time spent, divide by time elapsed in model, and multiply by number of resources in model.\nThis may slightly overestimate utilisation though as we record time spent with the resource prior to that time elapsing - e.g. “If we have a model run time of 600 time units, and someone reaches the nurse at unit 595 but has an activity time of 30, we will record that and use it in our calculations - even though only 5 minutes of that time actually took place during our model run.” You could adapt code to account for that (e.g. check if time remaining in model is greater than activity time sampled, and record whichever of those is smaller). [source]\n\n\n13.7.2 Method 2. Interval audit process\nFor this, we create a function that takes: 1. A list of resources to monitor, and 2. A time interval at which to snapshot utilisation\nThis is integrated into the code, and then set up as a process. [source]\nOverview of how HSMA code is modified for auditing:\n\n\n\n\n\n\n\nClass\nChanges\n\n\n\n\nGlobal model parameters\n• Set audit interval (e.g. 5 min)\n\n\nPatient\n-\n\n\nModel\n• Generator function to get metrics at given intervals, storing results as attribute• Add audit process to run i.e. self.env.process()\n\n\nTrial\n• Get audit results from model attributes\n\n\n\nThat is a HSMA example. We can also look at a different example from Tom (basic code structure differs - see above). In his example, he does not modify the Model or Patient classes, but instead creates an Auditor class. It performs the same job - getting metrics at given intervals. * Audits occur at first_obs and then interval time units apart * Queues and services are lists that will store a tuple identifying what want to audit * Metrics is a dict to store audits * Scheduled_audit - env.timeout() for delay between aduits, record_queue_length() finds length of queues (for each service, append length of their queue to dictionary)\n[source]\n:::{dropdown} Tom’s Auditor class\nclass Auditor:\n    def __init__(self, env, run_length, first_obs=None, interval=None):\n        '''\n        Auditor Constructor\n        \n        Params:\n        -----\n        env: simpy.Environment\n            \n        first_obs: float, optional (default=None)\n            Time of first scheduled observation.  If none then no scheduled\n            audit will take place\n        \n        interval: float, optional (default=None)\n            Time period between scheduled observations. If none then no scheduled\n            audit will take place\n        '''\n        self.env = env\n        self.first_observation = first_obs\n        self.interval = interval\n        self.run_length = run_length\n        \n        self.queues = []\n        self.service = []\n        \n        # dict to hold states\n        self.metrics = {}\n        \n        # scheduled the periodic audits\n        if not first_obs is None:\n            env.process(self.scheduled_observation())\n            env.process(self.process_end_of_run())\n            \n    def add_resource_to_audit(self, resource, name, audit_type='qs'):\n        if 'q' in audit_type:\n            self.queues.append((name, resource))\n            self.metrics[f'queue_length_{name}'] = []\n        \n        if 's' in audit_type:\n            self.service.append((name, resource))\n            self.metrics[f'system_{name}'] = []           \n            \n    def scheduled_observation(self):\n        '''\n        simpy process to control the frequency of \n        auditor observations of the model.  \n        \n        The first observation takes place at self.first_obs\n        and subsequent observations are spaced self.interval\n        apart in time.\n        '''\n        # delay first observation\n        yield self.env.timeout(self.first_observation)\n        self.record_queue_length()\n        self.record_calls_in_progress()\n        \n        while True:\n            yield self.env.timeout(self.interval)\n            self.record_queue_length()\n            self.record_calls_in_progress()\n    \n    def record_queue_length(self):\n        for name, res in self.queues:\n            self.metrics[f'queue_length_{name}'].append(len(res.queue)) \n        \n        \n    def record_calls_in_progress(self):\n        for name, res in self.service:\n            self.metrics[f'system_{name}'].append(res.count + len(res.queue)) \n               \n        \n    def process_end_of_run(self):\n        '''\n        Create an end of run summary\n        \n        Returns:\n        ---------\n            pd.DataFrame\n        '''\n        \n        yield self.env.timeout(self.run_length - 1)\n        \n        run_results = {}\n\n        for name, res in self.queues:\n            queue_length = np.array(self.metrics[f'queue_length_{name}'])\n            run_results[f'mean_queue_{name}'] = queue_length.mean()\n            \n        for name, res in self.service:\n            total_in_system = np.array(self.metrics[f'system_{name}'])\n            run_results[f'mean_system_{name}'] = total_in_system.mean()\n        \n#         #mean number in system and in queue (specific to operations)        \n#         queue_length = np.array(self.metrics['queue_length_ops'])\n#         total_in_system = queue_length + np.array(self.metrics['service_ops'])\n\n#         run_results['mean_queue'] = queue_length.mean()\n#         run_results['mean_system'] = total_in_system.mean()\n\n        self.summary_frame = pd.Series(run_results).to_frame()\n        self.summary_frame.columns = ['estimate']\n:::",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>DES using SimPy</span>"
    ]
  },
  {
    "objectID": "content/python/simpy.html#testing-a-large-number-of-scenarios",
    "href": "content/python/simpy.html#testing-a-large-number-of-scenarios",
    "title": "13  DES using SimPy",
    "section": "13.8 Testing a large number of scenarios",
    "text": "13.8 Testing a large number of scenarios\nWe run scenarios with different levels of resources, etc. To test a large number, recommend the method here. It involves you having: * Dictionary of scenarios * Use itertools to create all possible permutations of scenarios * Add scenario name to g class, then enumerate through the scenarios and run each one, storing the results as you go [source]",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>DES using SimPy</span>"
    ]
  },
  {
    "objectID": "content/python/simpy.html#determining-how-many-replications-to-run.",
    "href": "content/python/simpy.html#determining-how-many-replications-to-run.",
    "title": "13  DES using SimPy",
    "section": "13.9 Determining how many replications to run.",
    "text": "13.9 Determining how many replications to run.\n‘You will now use the confidence interval method to select the number replications to run in order to get a good estimate the models mean performance. The narrower the confidence interval the more precise our estimate of the mean. In general, the more replications you run the narrower the confidence interval. The method requires you to set a predefined width of the confidence interval’ - e.g. 5% or 10% either side of mean. Use the following function to implement in Python…[source]\n:::{dropdown} Confidence interval method to determine replication number\n[source]\n‘The method is less useful for values very close to zero. As the utilisation measures are between 0 and 1 it is recommended that you multiple the values by 100.’\nFunction:\ndef confidence_interval_method(replications, alpha=0.05, desired_precision=0.05, \n                               min_rep=5, decimal_place=2):\n    '''\n    The confidence interval method for selecting the number of replications\n    to run in a simulation.\n    \n    Finds the smallest number of replications where the width of the confidence\n    interval is less than the desired_precision.  \n    \n    Returns both the number of replications and the full results dataframe.\n    \n    Parameters:\n    ----------\n    replications: arraylike\n        Array (e.g. np.ndarray or list) of replications of a performance metric\n        \n    alpha: float, optional (default=0.05)\n        procedure constructs a 100(1-alpha) confidence interval for the \n        cumulative mean.\n        \n    desired_precision: float, optional (default=0.05)\n        Desired mean deviation from confidence interval.\n        \n    min_rep: int, optional (default=5)\n        set to a integer &gt; 0 and ignore all of the replications prior to it \n        when selecting the number of replications to run to achieve the desired\n        precision.  Useful when the number of replications returned does not\n        provide a stable precision below target.\n        \n    decimal_places: int, optional (default=2)\n        sets the number of decimal places of the returned dataframe containing\n        the results\n    \n    Returns:\n    --------\n        tuple: int, pd.DataFrame\n    \n    '''\n    n = len(replications)\n    cumulative_mean = [replications[0]]\n    running_var = [0.0]\n    for i in range(1, n):\n        cumulative_mean.append(cumulative_mean[i-1] + \\\n                       (replications[i] - cumulative_mean[i-1] ) / (i+1))\n        \n        # running biased variance\n        running_var.append(running_var[i-1] + (replications[i] \n                                               - cumulative_mean[i-1]) \\\n                            * (replications[i] - cumulative_mean[i]))\n        \n    # unbiased std dev = running_var / (n - 1)\n    with np.errstate(divide='ignore', invalid='ignore'):\n        running_std = np.sqrt(running_var / np.arange(n))\n    \n    # half width of interval\n    dof = len(replications) - 1\n    t_value = t.ppf(1 - (alpha / 2),  dof)    \n    with np.errstate(divide='ignore', invalid='ignore'):\n        std_error = running_std / np.sqrt(np.arange(1, n+1))\n        \n    half_width = t_value * std_error\n        \n    # upper and lower confidence interval\n    upper = cumulative_mean + half_width\n    lower = cumulative_mean - half_width\n    \n    # Mean deviation\n    with np.errstate(divide='ignore', invalid='ignore'):\n        deviation = (half_width / cumulative_mean) * 100\n    \n    # commbine results into a single dataframe\n    results = pd.DataFrame([replications, cumulative_mean, \n                            running_std, lower, upper, deviation]).T\n    results.columns = ['Mean', 'Cumulative Mean', 'Standard Deviation', \n                       'Lower Interval', 'Upper Interval', '% deviation']\n    results.index = np.arange(1, n+1)\n    results.index.name = 'replications'\n    \n    # get the smallest no. of reps where deviation is less than precision target\n    try:\n        n_reps = results.iloc[min_rep:].loc[results['% deviation'] \n                             &lt;= desired_precision*100].iloc[0].name\n    except:\n        # no replications with desired precision\n        message = 'WARNING: the replications do not reach desired precision'\n        warnings.warn(message)\n        n_reps = -1 \n\n    \n    return n_reps, results.round(2)\nUsing function:\n# run the method on the operator_wait replications\nn_reps, conf_ints = \\\n    confidence_interval_method(replications['operator_wait'].to_numpy(),\n                               desired_precision=0.05)\n\n# print out the min number of replications to achieve precision\nprint(f'\\nminimum number of reps for 5% precision: {n_reps}\\n')\n\n# peek at table of results\nconf_ints.head()\n\ndef plot_confidence_interval_method(n_reps, conf_ints, metric_name, \n                                    figsize=(12,4)):\n    '''\n    Plot the confidence intervals and cumulative mean\n    \n    Parameters:\n    ----------\n    n_reps: int\n        minimum number of reps selected\n        \n    conf_ints: pandas.DataFrame\n       results of the `confidence_interval_method` function\n       \n    metric_name: str\n        Name of the performance measure\n        \n    figsize: tuple, optional (default=(12,4))\n        The size of the plot\n        \n    Returns:\n    -------\n        matplotlib.pyplot.axis\n    '''\n    # plot cumulative mean + lower/upper intervals\n    ax = conf_ints[['Cumulative Mean', 'Lower Interval', \n                         'Upper Interval']].plot(figsize=figsize)\n    # add the \n    ax.axvline(x=n_reps, ls='--', color='red')\n    \n    ax.set_ylabel(f'cumulative mean: {metric_name}')\n    \n    return ax\n\n# plot the confidence intervals\nax = plot_confidence_interval_method(n_reps, conf_ints, \n                                     metric_name='operator_wait')\n\n# quick check if the % deviation remains below 5% for the next 10 reps?\nlookahead = 15\nconf_ints.iloc[n_reps-1:n_reps+lookahead]\n# run the method on the operator_wait replications\nn_reps, conf_ints = \\\n    confidence_interval_method(replications['operator_wait'].to_numpy(),\n                               desired_precision=0.05, min_rep=36)\n\n\n# print out the min number of replications to achieve precision\nprint(f'\\nminimum number of reps for 5% precision: {n_reps}\\n')\n\n# plot the confidence intervals\nax = plot_confidence_interval_method(n_reps, conf_ints, \n                                     metric_name='operator_wait', figsize=(9,6))\n:::",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>DES using SimPy</span>"
    ]
  },
  {
    "objectID": "content/python/simpy.html#appointment-booking-i.e.-scheduling",
    "href": "content/python/simpy.html#appointment-booking-i.e.-scheduling",
    "title": "13  DES using SimPy",
    "section": "13.10 Appointment booking (i.e. scheduling)",
    "text": "13.10 Appointment booking (i.e. scheduling)\nWe may want to model appointment booking (i.e. delay between enter system to book appointment, and attending appointment), rather than immediate attendance. See example from HSMA, which is based on Tom’s example.\nOverview of core changes to HSMA code for appointment booking (with time unit here being days rather than minutes):\n\n\n\n\n\n\n\nClass\nModifications\n\n\n\n\nGlobal parameters\nNew attributes:• Annual demand rather than IAT• Shifts dataframe with number of available appointments per day• Minimum wait (so can’t get appointment immediately)\n\n\nPatient\nNew attributes:• Booker• Arrival time• Wait time\n\n\nBooker\nAttributes:• Priority• ModelFunctions:• find_slot(): Find available slot in diary• book_slot(): Book the slot\n\n\nModel\nNew attributes:•Available slots• Bookings• Run create_slots()• Run create_bookings()• Arrival distribution• Monitoring of patient queue time and meanNew functions:• create_slots(): extrapolate shift dataframe to cover run time + longer (so can book ahead)• create_bookings(): create blank dataframe of same dimensions so can use to track number of patients bookedChanged functions:• generator_patient_arrivals(): instead of generate patient then wait until next, instead sample arrivals per day, loop through referrals and make patients, start booker for each patient, then run attend clinic for each patient• attend_clinic(): find_slot(), book_slot(), env.timeout() until appointment, then save time between enter system and appointment\n\n\nTrial\nSave new metrics (appointment wait)\n\n\n\nYou can allow patients to have a range of possible clinics that they can book into. You can do this by having a pooling matrix which is a simple table of 0s and 1s determining whether a patient can access each clinic. The package sim_utility has some premade Booker classes. To work with pooled booking, you could use the class one of their pooled booker classes e.g. LowPriorityPooledBooker from sim_utility (rather than LowPriorityBooker). [source]",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>DES using SimPy</span>"
    ]
  },
  {
    "objectID": "content/python/simpy.html#parallelisation",
    "href": "content/python/simpy.html#parallelisation",
    "title": "13  DES using SimPy",
    "section": "13.11 Parallelisation",
    "text": "13.11 Parallelisation\nBy default, Python code will run everything sequentially on a single core. We can run code in parallel to reduce run length. Use Joblib to split SimPy code to run across multiple processor cores. May not be possible when deploying on web.\n:::{dropdown} How does Joblib work?\nWe can illustrate this with a simple function:\nimport time\nimport math\nfrom joblib import Parallel, delayed\n\ndef my_function(i):\n    # Wait one second\n    time.sleep(1)\n    # Return square root of i**2\n    return math.sqrt(i**2)\nIf we run this function with a for loop, it takes ten seconds:\ni = num\nstart = time.time()\n\n# Run function in for loop\nfor i in range(num):\n    my_function(i)\n\nend = time.time()\n# Print time elapsed\nprint('{:.4f} s'.format(end-start))\n\n10.0387 s\nIf we run it using Parallel and delayed functions from joblib, it takes five seconds.[source] We use the delayed() function as it prevents the function from running. If we just had Parallel(n_jobs=2)(my_function(i) for i in range(num)), by the time it gets passed to Paralell, the my_function(i) calls have already returned, and there’s nothing left to execute in Parallel. [source]\nstart = time.time()\n\n# Run function using parallel processing\n# n_jobs is the number of parallel jobs\nParallel(n_jobs=2)(delayed(my_function(i) for i in range(num)))\n\nend = time.time()\n# Print time elapsed\nprint('{:.4f} s'.format(end-start))\n\n5.6560 s\n[source]\n:::\nAn overview of the changes we make to implement this for our SimPy model are below. These are from this HSMA example, which is based on an example from Mike.\n\n\n\n\n\n\n\nClass\nChanges\n\n\n\n\nGlobal parameters\n-\n\n\nPatient\n-\n\n\nModel\n-\n\n\nTrial\n• Changes for how save results, so save dictionary of results from run into list (rather than setting up a dummy dataframe and using .loc to write results into correct row, as will just end up with empty results list, when running parallel)• Split run_trial() into two functions, with run_trial() containing self.df_trial_results = Parallel(n_jobs=-1)(delayed(self.run_single)(run) for run in range(g.number_of_runs)) (-1 means every available core will run code)",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>DES using SimPy</span>"
    ]
  },
  {
    "objectID": "content/python/simpy.html#input-modelling",
    "href": "content/python/simpy.html#input-modelling",
    "title": "13  DES using SimPy",
    "section": "13.12 Input modelling",
    "text": "13.12 Input modelling\nMike write some code (archived but copied below) which fits various distributions to the data, then conductes Chi-Squared and KS-Test and ranks by the Chi-Squared statistics. It produces histograms, p-p plots, and q-q plots of the data against the theoretical distributions\n:::{dropdown} Auto fit\n[source]\nimport warnings\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\nimport pandas as pd\nimport scipy\nimport matplotlib.pyplot as plt\n\nwarnings.filterwarnings(\"ignore\")\n\ndef auto_fit(data_to_fit, hist=False, pp=False, dist_names=None):\n\n    y = data_to_fit.dropna().to_numpy()\n    size = len(y)\n    sc = StandardScaler() \n    yy = y.reshape(-1, 1)\n    sc.fit(yy)\n    y_std = sc.transform(yy)\n    y_std = y_std.flatten()\n    del yy\n\n    if dist_names is None:\n        dist_names = ['beta',\n                      'expon',\n                      'gamma',\n                      'lognorm',\n                      'norm',\n                      'pearson3',\n                      'weibull_min', \n                      'weibull_max']\n\n    # Set up empty lists to store results\n    chi_square = []\n    p_values = []\n\n    # Set up 50 bins for chi-square test\n    # Observed data will be approximately evenly distrubuted aross all bins\n    percentile_bins = np.linspace(0,100,50)\n    percentile_cutoffs = np.percentile(y_std, percentile_bins)\n    \n    #\"fix\": sometimes np.percentile produces cut-off that are not sorted!?  \n    #Why?  The test data was synthetic LoS in an ED and they are rounded numbers...\n    observed_frequency, bins = (np.histogram(y_std, bins=np.sort(percentile_cutoffs)))\n    cum_observed_frequency = np.cumsum(observed_frequency)\n\n    # Loop through candidate distributions\n\n    for distribution in dist_names:\n        # Set up distribution and get fitted distribution parameters\n        dist = getattr(scipy.stats, distribution)\n        param = dist.fit(y_std)\n\n        # Obtain the KS test P statistic, round it to 5 decimal places\n        p = scipy.stats.kstest(y_std, distribution, args=param)[1]\n        p = np.around(p, 5)\n        p_values.append(p)    \n\n        # Get expected counts in percentile bins\n        # This is based on a 'cumulative distrubution function' (cdf)\n        cdf_fitted = dist.cdf(percentile_cutoffs, *param[:-2], loc=param[-2], \n                              scale=param[-1])\n        expected_frequency = []\n        for bin in range(len(percentile_bins)-1):\n            expected_cdf_area = cdf_fitted[bin+1] - cdf_fitted[bin]\n            expected_frequency.append(expected_cdf_area)\n\n        # calculate chi-squared\n        expected_frequency = np.array(expected_frequency) * size\n        cum_expected_frequency = np.cumsum(expected_frequency)\n        ss = sum (((cum_expected_frequency - cum_observed_frequency) ** 2) / cum_observed_frequency)\n        chi_square.append(ss)\n\n    # Collate results and sort by goodness of fit (best at top)\n\n    results = pd.DataFrame()\n    results['Distribution'] = dist_names\n    results['chi_square'] = chi_square\n    results['p_value'] = p_values\n    results.sort_values(['chi_square'], inplace=True)\n\n    # Report results\n\n    print('\\nDistributions sorted by goodness of fit:')\n    print('----------------------------------------')\n    print(results)\n    \n    if hist:\n        fitted_histogram(y, results)\n        \n    if pp:\n        pp_qq_plots(y, y_std, dist_names)\n        \ndef fitted_histogram(y, results):    \n    size = len(y)\n    x = np.arange(len(y))\n\n\n    # Divide the observed data into 100 bins for plotting (this can be changed)\n    number_of_bins = 100\n    bin_cutoffs = np.linspace(np.percentile(y,0), np.percentile(y,99),number_of_bins)\n\n    # Create the plot\n    h = plt.hist(y, bins = bin_cutoffs, color='0.75')\n\n    # Get the top three distributions from the previous phase\n    number_distributions_to_plot = 5\n    dist_names = results['Distribution'].iloc[0:number_distributions_to_plot]\n\n    # Create an empty list to stroe fitted distribution parameters\n    parameters = []\n\n    # Loop through the distributions ot get line fit and paraemters\n\n    for dist_name in dist_names:\n        # Set up distribution and store distribution paraemters\n        dist = getattr(scipy.stats, dist_name)\n        param = dist.fit(y)\n        parameters.append(param)\n\n        # Get line for each distribution (and scale to match observed data)\n        pdf_fitted = dist.pdf(x, *param[:-2], loc=param[-2], scale=param[-1])\n        scale_pdf = np.trapz (h[0], h[1][:-1]) / np.trapz (pdf_fitted, x)\n        pdf_fitted *= scale_pdf\n\n        # Add the line to the plot\n        plt.plot(pdf_fitted, label=dist_name)\n\n        # Set the plot x axis to contain 99% of the data\n        # This can be removed, but sometimes outlier data makes the plot less clear\n        plt.xlim(0,np.percentile(y,99))\n\n    # Add legend and display plot\n\n    plt.legend()\n    plt.show()\n\n    # Store distribution paraemters in a dataframe (this could also be saved)\n    dist_parameters = pd.DataFrame()\n    dist_parameters['Distribution'] = (\n            results['Distribution'].iloc[0:number_distributions_to_plot])\n    dist_parameters['Distribution parameters'] = parameters\n\n    # Print parameter results\n    print ('\\nDistribution parameters:')\n    print ('------------------------')\n\n    for index, row in dist_parameters.iterrows():\n        print ('\\nDistribution:', row[0])\n        print ('Parameters:', row[1] )\n        \ndef pp_qq_plots(y, y_std, dist_names):\n    ## qq and pp plots\n    data = y_std.copy()\n    data.sort()\n    size = len(y)\n    x = np.arange(len(y))\n\n    x = x.reshape(-1, 1)\n    y = y.reshape(-1, 1)\n\n    # Loop through selected distributions (as previously selected)\n\n    for distribution in dist_names:\n        # Set up distribution\n        dist = getattr(scipy.stats, distribution)\n        param = dist.fit(y_std)\n\n        # Get random numbers from distribution\n        norm = dist.rvs(*param[0:-2],loc=param[-2], scale=param[-1],size = size)\n        norm.sort()\n\n        # Create figure\n        fig = plt.figure(figsize=(8,5)) \n\n        # qq plot\n        ax1 = fig.add_subplot(121) # Grid of 2x2, this is suplot 1\n        ax1.plot(norm,data,\"o\")\n        min_value = np.floor(min(min(norm),min(data)))\n        max_value = np.ceil(max(max(norm),max(data)))\n        ax1.plot([min_value,max_value],[min_value,max_value],'r--')\n        ax1.set_xlim(min_value,max_value)\n        ax1.set_xlabel('Theoretical quantiles')\n        ax1.set_ylabel('Observed quantiles')\n        title = 'qq plot for ' + distribution +' distribution'\n        ax1.set_title(title)\n\n        # pp plot\n        ax2 = fig.add_subplot(122)\n\n        # Calculate cumulative distributions\n        bins = np.percentile(norm,range(0,101))\n        data_counts, bins = np.histogram(data,bins)\n        norm_counts, bins = np.histogram(norm,bins)\n        cum_data = np.cumsum(data_counts)\n        cum_norm = np.cumsum(norm_counts)\n        cum_data = cum_data / max(cum_data)\n        cum_norm = cum_norm / max(cum_norm)\n\n        # plot\n        ax2.plot(cum_norm,cum_data,\"o\")\n        min_value = np.floor(min(min(cum_norm),min(cum_data)))\n        max_value = np.ceil(max(max(cum_norm),max(cum_data)))\n        ax2.plot([min_value,max_value],[min_value,max_value],'r--')\n        ax2.set_xlim(min_value,max_value)\n        ax2.set_xlabel('Theoretical cumulative distribution')\n        ax2.set_ylabel('Observed cumulative distribution')\n        title = 'pp plot for ' + distribution +' distribution'\n        ax2.set_title(title)\n\n        # Display plot    \n        plt.tight_layout(pad=4)\n        plt.show()\nWe can then run the code as follows: [source]\nfrom input_modelling.fitting import auto_fit\n\nrng = np.random.default_rng(42)\nsamples = rng.exponential(scale=32, size=10_000)\nsamples = pd.DataFrame(samples)\nsamples.head()\n\nauto_fit(samples)\n\n# Plot the distributions and use a few extra options\ndists_to_test = ['expon', 'gamma']\nauto_fit(samples, hist=True, pp=True, dist_names=dists_to_test)\n:::",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>DES using SimPy</span>"
    ]
  },
  {
    "objectID": "content/python/simpy.html#viewing-results-from-the-model",
    "href": "content/python/simpy.html#viewing-results-from-the-model",
    "title": "13  DES using SimPy",
    "section": "13.13 Viewing results from the model",
    "text": "13.13 Viewing results from the model\nI have made limited notes on output analysis, mainly focussing this on how the model is set up. However, you can see more about output analysis from each the HSMA DES book pages (which typically shows the output from all the different variants of model setup, using varying visualisations).",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>DES using SimPy</span>"
    ]
  },
  {
    "objectID": "content/python/books.html",
    "href": "content/python/books.html",
    "title": "14  Books",
    "section": "",
    "text": "System\nValid input files\nCompatibility with Python & R\nBlog posts (tags, dates, filters) ?\nNice examples\nAdditional details\n\n\n\n\nJupyter BookDocumentation\n- Markdown .md- Jupyter notebook .ipynb- MyST markdown notebook .md- reStructured Text .rst (not recommended)[source]\nPython tool. Would probably only use with R if you were working with R in .ipynb. Possible to convert .Rmd to ipynb or MyST\nNot found example\nSAMueL-1SAMueL-2\nEasy integration with BinderHub and Google Colab [source]Uses Sphinx to build book [source]\n\n\nSphinxDocumentation\n- reStructured Text .rst (default)- Markdown .md- Jupyter notebook .ipynb (with nbsphinx and MyST-NB)\nDesigned for Python, didn’t easily run into R implementation, but examples here in various languages\nYes\npyOpenSciLittle book of R for biomedical statisticsChris Holdgraf’s blog\nThumbnail gallery, Binder, nbviewer [source]\n\n\nQuartoDocumentation\n- Cross-language Quarto markdown .qmd (which combines markdown and executable code)-Jupyter notebook .ipynb- Markdown .md- R markdown .Rmd[source]\nExplicitly supports dynamic content from Python, R, Julia and Observable [source]Comparison with Rmd\nWell supported nativelyTutorial 1 Tutorial 2Tutorial 3\nddanieltan’s blogQuarto’s blog - github, siteR for Data SciencePython for Data AnalysisHSMA DES Book - github, site\nHuge range of supported output formats [source]\n\n\nJekyll\n- Markdown .md- HTML\nWritten in Ruby. Creates simple static sites.\n\nExeter RSE Workshop - github, siteRuby’s website\n\n\n\nMkdocsDocumentation\n- Markdown .mdSeems possible for others but more designed for markdown?\nDesigned for Python\nYes with Material for Mkdocs plugin\nMaterial for MkDocs - github, siteCookiecutter Data Science - github, site\nMaterial for MkDocs provides additional features\n\n\nBookdownDocumentation\n- R Markdown .Rmd\nDesigned for R\n\nR Markdown Definitive GuideR Markdown Cookbook\n\n\n\nBlogdownDocumentation\n- R Markdown .Rmd\nDesigned for R\n\nList of blogs\nBuilt on Hugo [source]\n\n\nHugodownDocumentation\n- R Markdown .Rmd\nDesigned for R\n\nList of blogs\nBuilt on Hugo\n\n\nDistill for R MarkdownDocumentation\n- R Markdown .Rmd\nDesigned for R\nYes natively (eg. set up project as blog or website)\nPiping Hot DataTidy modelsBefore I sleep\n\n\n\n\nReflections on RMarkdown, Distill, Bookdown and Blogdown.\nPaid: https://www.gitbook.com/pricing\nOther random noted down options not explored: * Sandpaper, pegboard and varnish - example: https://carpentries-lab.github.io/good-enough-practices/index.html * Sweave/LaTeX RStudio/LaTeX Pandoc SageMath Colab Notebooks Nbconvert Pelican Org mode DocOnce Scribus Madoko Texinfo",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Books</span>"
    ]
  },
  {
    "objectID": "content/python/quarto.html",
    "href": "content/python/quarto.html",
    "title": "15  Quarto",
    "section": "",
    "text": "15.1 Linting quarto\nGetting set up… 1. Install Quarto - https://quarto.org/docs/get-started/ - I downloaded the Linux deb file, then ran sudo dpkg -i packagename.deb 2. Create environment with necessary requirements (this may not be a necessary step, but this is what I decided to do) - for me, I needed:\nRender full .qmd: Ctrl+Shift+K\nRender file: quarto render hello.qmd --to html\nRender to alt. output: quarto render hello.qmd --to docx\nRun individual cells by selecting the Run Cell button.\nYou’ll see there’s a few Quarto projects that pop up - * Basic * Book * Blog * Manuscript * Website\nA book is actually a special type of website - the most important difference is that it uses chapter numbers and supports cross-references between different chapters. [source]\nFor a blog, the posts are in seperate pages as the filename has to be index.qmd.\nTo change preview type when using VSCode to preview, go to Settings &gt; Quarto &gt; Render: Preview Type and set to external\nCommands to check/make/preview book:\nLinting files - pre-requisites pylint / flake8 (sudo apt install pylint etc.)\nWith file lint_qmd.sh:\nCopy of file:",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "content/python/quarto.html#linting-quarto",
    "href": "content/python/quarto.html#linting-quarto",
    "title": "15  Quarto",
    "section": "",
    "text": "bash ./lint_qmd.sh ./ pylint\nbash ./lint_qmd.sh ./ flake8 | less -R\n\n#!/bin/bash\n\n# Directory to search for .qmd files\nDIRECTORY=${1:-.}\n\n# Linter to use (default to flake8)\nLINTER=${2:-flake8}\n\n# Validate the linter choice\nif [[ \"$LINTER\" != \"flake8\" && \"$LINTER\" != \"pylint\" ]]; then\n    echo -e \"\\033[1mError: Unsupported linter. Please choose either 'flake8' or 'pylint'.\\033[0m\"\n    echo\n    exit 1\nfi\n\n# Temporary file to store extracted Python code\nTEMP_FILE=$(mktemp)\n\n# Loop through all .qmd files in the directory\nfind \"$DIRECTORY\" -name \"*.qmd\" | while read -r FILE; do\n    echo\n    echo \"--------------------------------------------------------------------\"\n    echo \"Linting Python code in: $FILE\"\n    echo \"WARNING: Line numbers may be wrong, so section with error is shown\"\n    echo \"--------------------------------------------------------------------\"\n    echo\n\n    # Extract Python code blocks and save to TEMP_FILE\n    awk '\n        /```{python}/ {code=1; next}\n        /```/ {code=0; next}\n        code {print $0}\n    ' \"$FILE\" &gt; \"$TEMP_FILE\"\n\n    # Check if the temp file is not empty\n    if [ -s \"$TEMP_FILE\" ]; then\n        # Run the linter on the extracted code and capture the output\n        echo \"Linting extracted code...\"\n        echo\n        \n        # Store seen errors to avoid duplicates\n        declare -A seen_errors\n\n        $LINTER \"$TEMP_FILE\" 2&gt;&1 | while read -r LINE; do\n            # Ignore the file path part in the linter's output\n            # The format is typically something like: /tmp/tmp.bKSqCNR0PZ:28:1:\n            if [[ \"$LINE\" =~ [^:]+:([0-9]+):([0-9]+)-?([0-9]+)*: ]]; then\n                # Extract the line and column number, and the error message\n                LINTER_LINE=${BASH_REMATCH[1]}  # Get the line number\n                MESSAGE=${LINE#*:}  # Get the error message after the file and line info\n                MESSAGE=$(echo \"$MESSAGE\" | sed 's/^[0-9]*: //') # Strip off any leading line number if present\n\n                # Check if this error has already been processed\n                if [[ -z \"${seen_errors[$LINTER_LINE]}\" ]]; then\n                    # Mark this error as seen\n                    seen_errors[$LINTER_LINE]=1\n\n                    # Print the linter message in bold\n                    echo -e \"\\033[1m$MESSAGE\\033[0m\"\n                    echo\n\n                    # Find the corresponding line in the original .qmd file\n                    ORIGINAL_LINE=$(sed -n \"${LINTER_LINE}p\" \"$TEMP_FILE\")\n\n                    # Print the context: 2 lines above, current line, and 2 lines below\n                    sed -n \"$((LINTER_LINE - 2)),$((LINTER_LINE + 2))p\" \"$TEMP_FILE\" | while read -r CONTEXT_LINE; do\n                        echo \"  $CONTEXT_LINE\"\n                    done\n\n                    echo\n                    echo\n                fi\n            else\n                # Print any other output from the linter without modification\n                echo \"$LINE\"\n            fi\n        done\n    else\n        echo \"No Python code found in $FILE\"\n    fi\ndone\n\n# Remove the temporary file\nrm -f \"$TEMP_FILE\"",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "content/python/jupyter_book.html",
    "href": "content/python/jupyter_book.html",
    "title": "16  Jupyter book",
    "section": "",
    "text": "16.1 GitHub pages\nInstall jupyter-book package.\nTo create sample book stored in current location: jupyter-book create book_name/\nTo build/rebuild book: jupyter-book build book_name\nIf add new page and book table of contents doesn’t update upon rebuild, try: jupyter-book build --all book_name\nTo publish book on GitHub Pages:\n* Install ghp-import\n* Navigate to book’s root directory (contains _build/html) and run: ghp-import -n -p -f _build/html (if a directory above, can do bookname/_build/html instead)\n* View book at https://&lt;user&gt;.github.io/&lt;myonlinebook&gt;/\nTo update book, make changes in main branch, rebuild book, then use ghp-import -n -p -f _build/html as before to push newly built HTML to gh-pages branch. Will take a few minutes for page to update.\nTo set this up to just run from the command book: * cd to go to top of directory (i.e. above documents) * nano .bashrc * At bottom of file, add alias book=\"jupyter-book build ./ && ghp-import -n -p -f _build/html\", then save the file * Run source .bashrc to refresh * Then, when you want to recreate the jupyter book and push to GH pages, go to where the book is (i.e. where build and config is) and run book * Also set up config file if desired to execute pages off if don’t want to re-run all the notebooks each time",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Jupyter book</span>"
    ]
  },
  {
    "objectID": "content/python/sphinx.html",
    "href": "content/python/sphinx.html",
    "title": "17  Sphinx",
    "section": "",
    "text": "17.1 Example Makefile\nTo convert from Jupyter book to sphinx, run: jupyter-book config sphinx path/to/book. This will generate a conf.py file from your _config.yml and _toc.yml files. You can then run `sphinx-build path/to/book path/to/book/_build/html -b html.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Sphinx</span>"
    ]
  },
  {
    "objectID": "content/python/sphinx.html#example-makefile",
    "href": "content/python/sphinx.html#example-makefile",
    "title": "17  Sphinx",
    "section": "",
    "text": "# Minimal makefile for Sphinx documentation\n#\n\n# You can set these variables from the command line, and also\n# from the environment for the first two.\nSPHINXOPTS    +=\nSPHINXBUILD   ?= sphinx-build\nSOURCEDIR     = .\nBUILDDIR      = _build\n\n# Put it first so that \"make\" without argument is like \"make help\".\nhelp:\n    @$(SPHINXBUILD) -M help \"$(SOURCEDIR)\" \"$(BUILDDIR)\" $(SPHINXOPTS) $(O)\n\n.PHONY: help Makefile\n\n# Catch-all target: route all unknown targets to Sphinx using the new\n# \"make mode\" option.  $(O) is meant as a shortcut for $(SPHINXOPTS).\n%: Makefile\n    @$(SPHINXBUILD) -M $@ \"$(SOURCEDIR)\" \"$(BUILDDIR)\" $(SPHINXOPTS) $(O)",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Sphinx</span>"
    ]
  },
  {
    "objectID": "content/python/sphinx.html#example-requirements.txt",
    "href": "content/python/sphinx.html#example-requirements.txt",
    "title": "17  Sphinx",
    "section": "17.2 Example requirements.txt",
    "text": "17.2 Example requirements.txt\njupyter-book\nmatplotlib\nnumpy\npydata-sphinx-theme\nsphinxcontrib-mermaid",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Sphinx</span>"
    ]
  },
  {
    "objectID": "content/python/sphinx.html#example-index.md",
    "href": "content/python/sphinx.html#example-index.md",
    "title": "17  Sphinx",
    "section": "17.3 Example index.md",
    "text": "17.3 Example index.md\nStart with\n“```{toctree} :hidden: True”\nThen\nBooks &lt;books/making_books&gt;\nPython &lt;python/general&gt;\nR &lt;r/general&gt;\nprogramming_notes/git\nsimulation/simulation\nCausality &lt;causal_concepts/1_predict_vs_causal&gt;\nOther &lt;programming_notes/latex&gt;",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Sphinx</span>"
    ]
  },
  {
    "objectID": "content/python/sphinx.html#example-conf.py",
    "href": "content/python/sphinx.html#example-conf.py",
    "title": "17  Sphinx",
    "section": "17.4 Example conf.py",
    "text": "17.4 Example conf.py\n# Configuration file for the Sphinx documentation builder.\n\n# -- Project information -----------------------------------------------------\n\nproject = 'Amy Notes'\ncopyright = '2024, Amy Heather'\nauthor = 'Amy Heather'\n\n# -- General configuration ---------------------------------------------------\n\nextensions = [\n    'sphinxcontrib.mermaid',  # To render mermaid diagrams\n    'myst_nb',\n    'sphinx_copybutton', # Adds a copy button next to code blocks\n    'sphinx_togglebutton', # Allows you to make admonitions toggle-able\n    'sphinx_design'  # Allows grides, cards, dropdowns, tabs, badges, etc.\n]\n\nmyst_enable_extensions = [\n    'colon_fence'  # To use sphinx-design alongside myst_parser\n]\n\n# File types for documentation\nsource_suffix = ['.md']\n\n# Files to ignore\nexclude_patterns = [\n    '**.ipynb_checkpoints',\n    '_build',\n    'README.md'\n]\n\n# Notebook execution\nnb_execution_allow_errors = False\nnb_execution_cache_path = ''\nnb_execution_excludepatterns = []\nnb_execution_in_temp = False\nnb_execution_mode = 'off'\nnb_execution_timeout = 30\n\n# -- Options for HTML output -------------------------------------------------\n\nhtml_theme = 'pydata_sphinx_theme'\n\nhtml_theme_options = {\n    # Set logo\n    'logo': {\n        'text': 'Programming notes'\n    },\n    # Add icons to the bar across the top\n    'icon_links': [\n        {\n            'name': 'GitHub',\n            'url': 'https://github.com/amyheather/programming_notes/',\n            'icon': 'fab fa-github-square'\n        }\n    ]\n}\n\n# Custom CSS style sheet\nhtml_static_path = ['_static']\nhtml_css_files = [\n    'css/custom.css',\n]",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Sphinx</span>"
    ]
  },
  {
    "objectID": "content/python/sphinx.html#example-custom.css",
    "href": "content/python/sphinx.html#example-custom.css",
    "title": "17  Sphinx",
    "section": "17.5 Example custom.css",
    "text": "17.5 Example custom.css\n.navbar {\n    background-color: #F0F8FF !important;\n}",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Sphinx</span>"
    ]
  },
  {
    "objectID": "content/r/general.html",
    "href": "content/r/general.html",
    "title": "18  General",
    "section": "",
    "text": "18.1 Comparing two dataframes",
    "crumbs": [
      "R",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>General</span>"
    ]
  },
  {
    "objectID": "content/r/general.html#comparing-two-dataframes",
    "href": "content/r/general.html#comparing-two-dataframes",
    "title": "18  General",
    "section": "",
    "text": "waldo::compare(df1, df2)",
    "crumbs": [
      "R",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>General</span>"
    ]
  },
  {
    "objectID": "content/r/general.html#function-setting-default-inputs",
    "href": "content/r/general.html#function-setting-default-inputs",
    "title": "18  General",
    "section": "18.2 Function setting default inputs",
    "text": "18.2 Function setting default inputs\nIf default input is different to the input name, you won’t have an issue. If it’s the same you’ll have an error.\n# Fine:\nx &lt;- function(T = 1){}\n# Problem:\nx &lt;- function(T = T){}\nIn that case, you can resolve this by doing…\nx &lt;- function(T = parent.frame()$T)",
    "crumbs": [
      "R",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>General</span>"
    ]
  },
  {
    "objectID": "content/r/general.html#function-returning-multiple-objects",
    "href": "content/r/general.html#function-returning-multiple-objects",
    "title": "18  General",
    "section": "18.3 Function returning multiple objects",
    "text": "18.3 Function returning multiple objects\nIn python, you can return multiple objects as follows:\ndef fun():\n    str = \"example\"\n    x = 20\n    return str, x; # Return tuple, we could also write (str, x)\nstr_1, x_1 = fun() # Assign returned tuple\nprint(str_1)\nprint(x_1)\nThis is not possible in R. Instead, a good work around is:\nfun &lt;- function(suffix) {\n    str &lt;- \"example\"\n    x &lt;- 20\n    return_names &lt;- c(\"str\", \"x\")\n    return_objects &lt;- mget(return_names)\n    return_new_names &lt;- paste0(return_names, \"_\", suffix)\n    return(list(return_new_names, return_objects))\n}\noutput &lt;- fun(\"1\")\nfor (i in seq_along(output[[1]])){\n    assign(output[[1]][i], output[[2]][i])\n}",
    "crumbs": [
      "R",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>General</span>"
    ]
  },
  {
    "objectID": "content/r/general.html#linting",
    "href": "content/r/general.html#linting",
    "title": "18  General",
    "section": "18.4 Linting",
    "text": "18.4 Linting\ninstall.packages(\"lintr\")\nlibrary(lintr)\nlint(\"filename.R\")\nlintr::lint_dir(\"foldername\")\nlintr::lint_package()",
    "crumbs": [
      "R",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>General</span>"
    ]
  },
  {
    "objectID": "content/r/packages.html",
    "href": "content/r/packages.html",
    "title": "19  Packages",
    "section": "",
    "text": "19.1 Packages: set up\nNotes from working through: https://r-pkgs.org/\nRemember: Git commit after each change\nStart in a fresh R session in the directory where you want to create the folder (e.g. Documents).\nEnvironment:\n* library(devtools) (if not already installed, will need to run install.packages(“devtools”)\n* create_package(\"~/Documents/packagename\"), which will create a directory (if doesn’t already exist) and populate with .gitignore, .Rbuildignore, DESCRIPTION, NAMESPACE, R folder, and packagename.Rproj. RStudio will open a new window with Rproj activated/open. As it’s a fresh R session, then call library(devtools) again.\n* renv::init() to set up with renv\n* install.packages(c(\"devtools\", \"roxygen2\", \"testthat\", \"knitr\")) then renv::snapshot()\n* library(devtools), or add to .Rprofile to always library() it:\nGitHub:\n* Make it a GitHub repository - usethis::use_git()\n* Rename branch git branch -m master main\n* Link with GitHub - usethis::use_github(). May get error about personal access token if not set. To see if it is set, run gh_token_help(). To store, use gitcreds::gitcreds_set()\nDescription, license and readme:\n* Edit DESCRIPTION (title, author, description)\n* use_mit_license() to create the license files\n* use_readme_rmd() to create basic README file, add some lines to .Rbuildignore, and creates a Git pre-commit hook to help you keep README.Rmd and README.md in sync. To update README runbuild_readme() to create README.md from README/Rmd. The advantage of having a README.Rmd file is that you can include code chunks, embed plots, etc. You can also use GitHub Actions to re-render README.Rmd every time you push, example here: https://github.com/r-lib/actions/tree/v1/examples. I have not tried that yet though.",
    "crumbs": [
      "R",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Packages</span>"
    ]
  },
  {
    "objectID": "content/r/packages.html#packages-set-up",
    "href": "content/r/packages.html#packages-set-up",
    "title": "19  Packages",
    "section": "",
    "text": "if (interactive()) {\n  suppressMessages(require(devtools))\n}",
    "crumbs": [
      "R",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Packages</span>"
    ]
  },
  {
    "objectID": "content/r/packages.html#packages-adding-functions-and-testing",
    "href": "content/r/packages.html#packages-adding-functions-and-testing",
    "title": "19  Packages",
    "section": "19.2 Packages: adding functions and testing",
    "text": "19.2 Packages: adding functions and testing\nuse_r(\"functionname\") to create or open script functionname.R in R/ directory. Add the function there (only that, not libraries etc.).\nDocstring:\n* Open function R file, click cursor within function, then do Code &gt; Insert Roxygen skelecton. * Edit the Rxoygen description. It will include @export. This tells Roxygen2 to add this function to the NAMESPACE file, so that it will be accessible to users. For your first R package, you’ll probably want to include @export for each of your functions.\n* Run document() to update documentation. It will convert the roxygen comment into man/functionname.Rd and write in NAMESPACE\n* The @example directive is used when you want to use an external file that contains the examples. If you’re including the example(s) directly in your roxygen documentation then use @examples. If you don’t want the examples to run (e.g. if providing with a fake file that doesn’t exist, and hence would fail check() as it tests examples work), then use @examplesIf interactive()\n* Can now preview helpfile by running ?functionname\n* Usage is derived from function specification. Can set manually with @usage and remove entirely with @usage NULL\nTesting:\n* use_testthat(). This initializes the unit testing machinery for your package. It adds Suggests: testthat to DESCRIPTION, creates the directory tests/testthat/, and adds the script tests/testthat.R.\n* use_test(\"functionname\") to create matching testfile.\n* library(testthat), load_all(), test() to run the test. Tests will also run whenever check() package\n* rename_files(\"strsplit1\", \"str_split_one\") to rename the R file and will also rename testthat file (which we also edit with new function name), to fit with convention of filename matching function name",
    "crumbs": [
      "R",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Packages</span>"
    ]
  },
  {
    "objectID": "content/r/packages.html#packages-using-other-libraries",
    "href": "content/r/packages.html#packages-using-other-libraries",
    "title": "19  Packages",
    "section": "19.3 Packages: using other libraries",
    "text": "19.3 Packages: using other libraries\nBased on: https://kbroman.org/pkg_primer/pages/depends.html\n* Add libraries that are essential for package to run and that want R to install when install your package to Imports: section of DESCRIPTION FILE. Instead of manual change, can also run use_package(“readxl”) which adds it to Imports section of DESCRIPTION.\n* Add required imports to each function - ideally with specific functions - or just whole package - example:\n#' @import tidyverse\n#' @importFrom readxl read_excel\n\nRun check() to see if issue is resolved",
    "crumbs": [
      "R",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Packages</span>"
    ]
  },
  {
    "objectID": "content/r/packages.html#packages-loading-and-checking",
    "href": "content/r/packages.html#packages-loading-and-checking",
    "title": "19  Packages",
    "section": "19.4 Packages: loading and checking",
    "text": "19.4 Packages: loading and checking\n\nload_all() to simulate process of building, installing and attaching the package. You can then use functions from that package.\n\ncheck() to check package is in working order. Read output - it’s easier to deal with these problems earlier\n\ninstall() then library(packagename) to install and use package in current environment",
    "crumbs": [
      "R",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Packages</span>"
    ]
  },
  {
    "objectID": "content/r/packages.html#packages---vignettes",
    "href": "content/r/packages.html#packages---vignettes",
    "title": "19  Packages",
    "section": "19.5 Packages - Vignettes:",
    "text": "19.5 Packages - Vignettes:\n\nusethis::use_vignette(\"my-vignette\") - creates directory, modifies DESCRIPTION, drafts vignette Rmd, adds patterns to .gitignore\n\ninstall(), library(package_name) then knit\nIf you rename vignette, change VignetteIndexEntry to match title (doesn’t need to match filename). When run, will create new html under docs/articles called newfilename.html. Delete old filename. Build site. Push.",
    "crumbs": [
      "R",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Packages</span>"
    ]
  },
  {
    "objectID": "content/r/packages.html#packages---create-github-pages-website",
    "href": "content/r/packages.html#packages---create-github-pages-website",
    "title": "19  Packages",
    "section": "19.6 Packages - Create GitHub pages website:",
    "text": "19.6 Packages - Create GitHub pages website:\nInitial set-up:\n* First step is to ensure set up with git creds. When I first tried this, I had lots of issues with it not working, and realised this was because I hadn’t set it up with the Personal Access Token (PAT). Do this using gitcreds::gitcreds_set(). Check settings using usethis::gh_token_help().\n* Next step is to go to repository Settings &gt; Actions &gt; General &gt; Workflow permissions and check “read and write permissions”\nCreating site:\n* OPTION 1. usethis::use_pkgdown(), then pkgdown::build_site(), then remove docs from .gitignore so you can commit them to GitHub, then push to main\n* OPTION 2: usethis::use_pkgdown_github_pages()\nTo update website, rebuild locally using pkgdown::build_site() then push to GitHub and push to main branch. GitHub pages will always based on files in docs/ folder for source code of website in specified branch (default: main).",
    "crumbs": [
      "R",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Packages</span>"
    ]
  },
  {
    "objectID": "content/r/environments.html",
    "href": "content/r/environments.html",
    "title": "20  Dependency management in R",
    "section": "",
    "text": "20.1 How to set-up renv for reproducible research\nCan use gnome-box to run a new system.\nCRAN won’t let you use old versions of packages.\nPeople like R because they can just use one R environment and everything works in that latest environment. General recommendation is just to use the latest stuff\n“A big reason R doesn’t have as rich an ecosystem for package installation tools (as compared to other languages) is because CRAN’s design alleviates many of the challenges traditionally faced in package installation. As an example, CRAN checks new package updates to ensure they work with their upstream reverse dependencies. If updates fail to pass these”revdep checks”, the package author must shoulder the burden of getting those reverse dependencies in line. Overall, this ensures that users going to install.packages get a set of packages that work together. Other languages push more of this work onto the person (and client) installing the package. However, as the R package ecosystem has grown, and people have developed more mission-critical workflows that require reproducibility, we have seen an uptick in the need for package management (as opposed to installation) tools.” [source]\nIssues with renv:\nPossible solutions:\nBinary packages, pre-compiled, etc. etc.\nCRAN Task View Initiative suggests checkpoint, containerit, dateback, groundhog, liftr, miniCRAN, packrat, rang, renv, Require, switchr.\nCreate DESCRIPTION file e.g.\nThen start new empty environment with renv::init(bare=TRUE).\nWhen initialising, you should be prompted to only install from the DESCRIPTION - select yes to this. Otherwise, run the command yourself: renv::settings$snapshot.type(\"explicit\").\nYou can then install the packages from DESCRIPTION by running renv::install(), and then create the lock file by running renv::snapshot.\nIf you make any changes to the packages and versions, simply modified the DESCRIPTION file and then run renv::install() followed by renv::snapshot.\nIf you run into issues where it cannot find a specific package/version, this may be due to the formatting of the version number. For example, for the package tiff:\nThe error was due to how those versions are formatted on CRAN, as you can see on the tiff archive.",
    "crumbs": [
      "R",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Dependency management in R</span>"
    ]
  },
  {
    "objectID": "content/r/environments.html#how-to-set-up-renv-for-reproducible-research",
    "href": "content/r/environments.html#how-to-set-up-renv-for-reproducible-research",
    "title": "20  Dependency management in R",
    "section": "",
    "text": "Title: quarto_huang_2019\nDepends: \n    R (&gt;= 3.7)\nImports:\n    knitr (==1.47),\n    rmarkdown (==2.27),\n    remotes (==2.5.0),\n    tiff (==0.1-12)\n\n\n\n\n\n\ntiff - installs latest version (0.1.12)\ntiff (==0.1.11) - cannot find package\ntiff (==0.1-11) - installs older version (0.1.11)",
    "crumbs": [
      "R",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Dependency management in R</span>"
    ]
  },
  {
    "objectID": "content/r/environments.html#basic-renv-commands",
    "href": "content/r/environments.html#basic-renv-commands",
    "title": "20  Dependency management in R",
    "section": "20.2 Basic renv commands",
    "text": "20.2 Basic renv commands\nTo start new project environment, creating .RProfile: renv::init()\nTo save state of project library to lockfile renv.lock: renv::snapshot()\nTo return to environment in lockfile: renv::restore()",
    "crumbs": [
      "R",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Dependency management in R</span>"
    ]
  },
  {
    "objectID": "content/r/environments.html#binder",
    "href": "content/r/environments.html#binder",
    "title": "20  Dependency management in R",
    "section": "20.3 Binder",
    "text": "20.3 Binder\nCreated using instructions from here and here.\n\nCreate runtime.txt file with R version\n\nCreate install.R file with package installations\n\nNavigate to https://mybinder.org/, paste in GitHub repository, set to “URL to open (optional)” and type in “rstudio”, then launch",
    "crumbs": [
      "R",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Dependency management in R</span>"
    ]
  },
  {
    "objectID": "content/r/environments.html#a-few-other-options",
    "href": "content/r/environments.html#a-few-other-options",
    "title": "20  Dependency management in R",
    "section": "20.4 A few other options…",
    "text": "20.4 A few other options…\n\nPosit Public Package Manager- can use Snapshot (earliest is Oct 2017, and 5 most recent versions of R), for Linux can install binary packages (which is much quicker, as usually R installs from source rather than binary unlike for Windows and Mac which makes it really slow) - source 1, source 2\nGroundhog - can go back to R 3.2 and April 2015 (and apparently can patch to go earlier) - source 1\nminiCRAN - source 1\n\n\nrequires license for non-academic (e.g. NHS) use - but Podman can drop in as replacement. To do development inside a container isn’t natively supported by RStudio but can use RStudioServer via Rocker. By default, it runs in ephemeral mode - any code created or saved is lost when close - but you can use volume argument to mount local folders source 1",
    "crumbs": [
      "R",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Dependency management in R</span>"
    ]
  },
  {
    "objectID": "content/simulation/simulation.html",
    "href": "content/simulation/simulation.html",
    "title": "21  Simulation",
    "section": "",
    "text": "21.1 Simulation\nYou can think of there as being three models for healthcare systems: 1. Analytical queueing models (i.e. mathematical models) - easy and fast inference for simple models, model require minimal data, maths can become complex… may be too simplified, or intractable (hard to control or direct) 2. Computer simulation - very flexible, doesn’t have limits of analytical models, easier for healthcare clients to understand theoretically, can be easy to include too much detail, and complex models require lots of data and have complex output 3. Real experimentation on healthcare systems - high risk, may be unethical, expensive, will be lots of other interventions happening at once [TomL7]\nThis section focusses on simulation…\n“A simulation imitates the operation of real world processes or systems with the use of models. The model represents the key behaviours and characteristics of the selected process or system while the simulation represents how the model evolves under different conditions over time.” [source]",
    "crumbs": [
      "Simulation",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Simulation</span>"
    ]
  },
  {
    "objectID": "content/simulation/simulation.html#simulation",
    "href": "content/simulation/simulation.html#simulation",
    "title": "21  Simulation",
    "section": "",
    "text": "21.1.1 Characteristics of a simulation\n\n21.1.1.1 Stochastic or deterministic\n\nStochastic - use random variables as inputs. Also known as probablistic models. Incorporate randomness/information about uncertainty. [source]\nDeterministic - model behaviour is completely predictable from inputs\n\n\n\n21.1.1.2 Static or dynamic\n\nStatic - represents one point in time [source] (e.g. calculating stress on a bridge) [source]\nDynamic - represents changes in system over time\n\n\n\n21.1.1.3 Discrete or continuous\n\nDiscrete - variables change at discrete points in time (e.g. customer arrivals)\nContinuous - variables continuously change over time (e.g. speed of car) [source]",
    "crumbs": [
      "Simulation",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Simulation</span>"
    ]
  },
  {
    "objectID": "content/simulation/simulation.html#types-of-simulation-models",
    "href": "content/simulation/simulation.html#types-of-simulation-models",
    "title": "21  Simulation",
    "section": "21.2 Types of simulation models",
    "text": "21.2 Types of simulation models\nMonte Carlo - ‘use random sampling techniques to model uncertainty and vaiability in a system’ - often ‘to estimate the probability of different outcomes in situations with many uncertain variables’ [source] * Stochastic [source] * Static [source]\nAgent-based modelling\nDiscrete event simulation - models ‘behaviour of a system as a sequence of events in time’ [source] * Stochastic * Dynamic * Discrete\nSystem dynamic modelling",
    "crumbs": [
      "Simulation",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Simulation</span>"
    ]
  },
  {
    "objectID": "content/simulation/simulation.html#advice-on-modelling-care-pathways",
    "href": "content/simulation/simulation.html#advice-on-modelling-care-pathways",
    "title": "21  Simulation",
    "section": "21.3 Advice on modelling care pathways",
    "text": "21.3 Advice on modelling care pathways\nAdvice: 1. Clarify modelling objectives and what the clinical team wants to achieve 2. Identify the patient populations to model 3. Use the initial meeting to sketch out an overview of the pathway (e.g. how does patient arrive to patient, what resources, what order do they see, etc. etc.) 4. Spend some time observing how the operations of the pathway are managed (and they likely operate differently to how you have been told!) 5. Meet with the data controller for the pathway [TomL7]\nTypical data requirements: * Demand - e.g. referrals to an outpatient clinic, ambulance callouts, pharmacy orders. Useful to have a continuous (time series) sample over a sustained period * Process times - e.g. medical assessments, operation durations, workforce travel times. A sample of data is usually sufficient, but may not be available (in that case, may be able to set up short term data collection (limitations) or make some assumptions about the distribution we will sample from) * Resource - e.g. number of staff on shirt, inventory (blood, medical supplies, PPE, beds). These may be specialist or shared Patient routing * Trajectories of patients - Often complication… pathways may straddle multiple serivices or have different routes for classes of patients. e.g. 80% of patients who have been to ICU may need step-down high dependency care whilst 20% don’t * Queueing rules e.g. prioritisation of more severe cases * Disease progression. In some disease areas, patients may be undergoing treatment while their disease state is progressin, e.g. diabetic retinopathy… care pathway where check at right time to identify it early and successfully treat [TomL7]",
    "crumbs": [
      "Simulation",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Simulation</span>"
    ]
  },
  {
    "objectID": "content/simulation/discrete_event.html",
    "href": "content/simulation/discrete_event.html",
    "title": "22  Discrete event simulation (DES)",
    "section": "",
    "text": "22.1 What is DES?\nA DES models the behaviour of a system as a sequence of events in time. [source] It is used to model queueing problems (e.g. people waiting for service/s). [source] It is: * Stochastic (probablistic) - use random variables as inputs. Also known as probablistic models. Incorporate randomness/information about uncertainty. [source] * Dynamic - represents changes in system over time * Discrete - variables change at discrete points in time (e.g. customer arrivals) [source]\nThe output of a stochastic model is a distribution so stochastic healthcare systems have variable performance. Model is a simplification of the system that attempts to mimic that variation. If we want to compare two or more systems that have stochastic behaviour and output a random variable, then we must do so carefully as there is a risk of making an inference error.[TomL7]\nIt uses next event time handling, meaning it keeps track of when events are due and hopes from event to event. This is more efficient, as ignores time inbetween when nothing is happening.[TomL7]",
    "crumbs": [
      "Simulation",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Discrete event simulation (DES)</span>"
    ]
  },
  {
    "objectID": "content/simulation/discrete_event.html#components-of-a-des",
    "href": "content/simulation/discrete_event.html#components-of-a-des",
    "title": "22  Discrete event simulation (DES)",
    "section": "22.2 Components of a DES",
    "text": "22.2 Components of a DES\nYou can have a single run of a model, or you can have a batch of runs with the same parameter values which is a trial.\nYou model has entities (e.g. patients, ambulances, hospitals). The frequency at which they are generated can be determined by the inter-arrival time.\nEntities wait in queues for activities. The queue will have a queueing policy which determines the order that entities are released (commonly first in first out, or priority-based). How long the activity then takes is the activity time. We also know the resources required for an activity to happen (specifically, the type and number of resources).\nSinks are how entities leave the model - which happens when they are outside of the model scope.\nEntities can have attributes that determine their journey (e.g. which path, queue priority, activity length). [source]\nDiagram created used https://app.diagrams.net/:\n\n\n\nDES components diagram\n\n\nYou can have branching paths (so different entities flow to different activities and/or sinks), which might be due to attributes, or probability, or time.\nYou will monitor outputs that answer your modelling questions - often this is a description of time that entities are in the system, queue length and duration, resource utilisation, probability of exceeding a threshold. [source]",
    "crumbs": [
      "Simulation",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Discrete event simulation (DES)</span>"
    ]
  },
  {
    "objectID": "content/simulation/discrete_event.html#des-inputs",
    "href": "content/simulation/discrete_event.html#des-inputs",
    "title": "22  Discrete event simulation (DES)",
    "section": "22.3 DES Inputs",
    "text": "22.3 DES Inputs\nYou use input modelling to determine the appropriate inputs for your model. This has four stages: 1. Data collection. 2. Identifying the input data distribution. (e.g. review data histogram, think about context/intended use of data, consider whether there is correlation/dependency which may mean you need ‘more complex distributions that can represent autocorrelative processes, time-series data, or multivariate inputs’, whether the process changes over time, and the range of the data) 3. Estimating parameters for the selected distribution. (e.g. normal distribution needs mean and variance/standard deviation, bernoulli distribution needs probability of success/failure in a binary experiment). Commonly use maximum likelihood estimation (MLE) or least-squares estimation 4. Estimating the goodness of fit. Estimates ‘closeness between real data and samples that the selected probability distribution produces’ (e.g. Kolmogorov-Smirnov test) [source]",
    "crumbs": [
      "Simulation",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Discrete event simulation (DES)</span>"
    ]
  },
  {
    "objectID": "content/simulation/discrete_event.html#des-outputs",
    "href": "content/simulation/discrete_event.html#des-outputs",
    "title": "22  Discrete event simulation (DES)",
    "section": "22.4 DES Outputs",
    "text": "22.4 DES Outputs\nA simulation like DES can be either: * Terminating - starts empty and finishes empty (e.g. day surgery) * Non-terminating - no natural endpoint (e.g. emergency department) [TomL10]\nThere are four possible output types from these models: * Transient output - distribution of output is constantly changing * Steady state - output is consistent - it varies but within a fixed distribution (steady-state distribution) * Steady-state cycle - outputs shifts from one steady-state to another in a regular pattern (e.g. call volume differs by time of day, but will be in a steady-state if comparing the same time of day across multiple days) * Shifting steady-state - output shifts from one steady-state to another without a regular/predictable pattern\nTerminating models typically have transient output. Non-terminating models typically have a steady-state output (possibly with a cycle or shifts). It is possible however for a terminating model to have a steady state output (particularly if have long run length before terminates), and for a non-terminating model to not reach steady state. [source]",
    "crumbs": [
      "Simulation",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Discrete event simulation (DES)</span>"
    ]
  },
  {
    "objectID": "content/simulation/discrete_event.html#initialisation-bias",
    "href": "content/simulation/discrete_event.html#initialisation-bias",
    "title": "22  Discrete event simulation (DES)",
    "section": "22.5 Initialisation bias",
    "text": "22.5 Initialisation bias\nData collection should have a realistic starting point. Our problem is that models have initialisation bias - i.e. they start in an unrealistic state. To deal with this…\n\n22.5.1 Method 1. Inspect time series\nA warm-up period is when you run the model like normal, but don’t collect the results. [source]\nTo find the length of the warm-up period, you should run the simulation for a long time, inspect a metric (e.g. waiting time every 60 minutes), and look for when it enters steady state. You would then delete that portion of the simulation (i.e. don’t save results until past the time threshold).[TomL10]\nTo identify when you have reached the steady state, you can either: * Use statistical method to determine when equilibrium is reached - example * Eyeball it * Run it for a long time [source]\n\n\n22.5.2 Method 2. Set initial conditions manually\nThis save run time as you normally run and then delete warmup. This can either be based on: * Real system * Running the model with warm-up and observing the steady state conditions\nYou could consider using a distribution of initial conditions.",
    "crumbs": [
      "Simulation",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Discrete event simulation (DES)</span>"
    ]
  },
  {
    "objectID": "content/simulation/discrete_event.html#queue-characteristics",
    "href": "content/simulation/discrete_event.html#queue-characteristics",
    "title": "22  Discrete event simulation (DES)",
    "section": "22.6 Queue characteristics",
    "text": "22.6 Queue characteristics\n‘The queue discipline indicates the order in which members of the queue are selected for service’.[source]\nDiscplines: * FIFO (first-in first-out) - a.k.a. FCFS (first-come first served) * LIFO (last-in first-out) - last entity is served first (e.g. eating a stack of pancakes)[source] * SIRO (service in random order) - i.e. randomly * Priority queue when certain entities get priority [source]\nThere are then certain behaviours we might observe…\nReneging - Entity removes themself from a queue after a certain amount of time has elapsed * E.g. Patient attribute of patience (e.g. sample a number, and thats how many minutes they are prepared to wait), when request resource tell simpy to wait until request is met or the patients patience expires. If they reneged, they won’t see nurse and we record the number that reneged\nBalking - Entity chooses not to enter a queue in the first place because (a) it is too long for their preferences, or (b) there is no capacity for them * Example for (b): Have parameter in g class with maximum queue length allowed. Have model attribute storing patients in queue, updated whenever patient leaves or joins. Before ask for nurse resource, check if queue is at max size. If so, patient will never join queue and we record that.\nJockeying - Entity switches queues in the hope of reducing queue time * Never used in healthcare system. However, you might have system where entities pick which queue to join in the first place based on queue length (e.g. choosing between MIU or ED based on live waiting time data online) [source]",
    "crumbs": [
      "Simulation",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Discrete event simulation (DES)</span>"
    ]
  },
  {
    "objectID": "content/simulation/discrete_event.html#appointment-booking",
    "href": "content/simulation/discrete_event.html#appointment-booking",
    "title": "22  Discrete event simulation (DES)",
    "section": "22.7 Appointment booking",
    "text": "22.7 Appointment booking\nAbove, we’ve assumed that arrivals flow through the system immediately (or as quickly as possible, depending on queues). That’s good for services like ED, walk-in clinic.\nFor services where clients are booked in future, there is delay between making appointment and attending. Possibile complexities include - * Setting aside slots for urgent referrals and balancing that against capacity * Patients with regular appointments at certain intervals * Triage step before appointment where decide whether need one or not * Non-attendance of the appointment meaning either exit system or need to rebook [source]",
    "crumbs": [
      "Simulation",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Discrete event simulation (DES)</span>"
    ]
  },
  {
    "objectID": "content/simulation/discrete_event.html#reproducibility",
    "href": "content/simulation/discrete_event.html#reproducibility",
    "title": "22  Discrete event simulation (DES)",
    "section": "22.8 Reproducibility",
    "text": "22.8 Reproducibility\nYou could set one seed per run. We then do 100 runs. Then we repeat that again. We want the results to be the same - and they are, yay!\nNow we do a scenario where we change the number of nurses. We expect this to: * Change the queues for nurses and doctors - yes 🙂 * Number of arrivals remain unchanged - no! 🙁\nThis is because all the methods are using one seed. The order that the random numbers are generated in matters. As the order of events changes (e.g. as have more nurses, they can see more patients quicker, changing the order that subsequent events happen).\nHence, a robust way to do this is to set seeds for each type of event that we are generating random numbers for. This means that each event has a separate random number stream for each part of process (e.g. for the inter-arrival times, for the consult times, for our probabilities when branching). [source]",
    "crumbs": [
      "Simulation",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Discrete event simulation (DES)</span>"
    ]
  },
  {
    "objectID": "content/simulation/discrete_event.html#evaluating-model-performance",
    "href": "content/simulation/discrete_event.html#evaluating-model-performance",
    "title": "22  Discrete event simulation (DES)",
    "section": "22.9 Evaluating model performance",
    "text": "22.9 Evaluating model performance\n\n22.9.1 Metrics\nArrivals. (e.g. total arrivals per day)\nResource utilisation. We want to track resource utilisation overall as well as at specific time points. We often won’t want utilisation to be close to 100%, although what we do want depends on the type and size of service (e.g. emergency service want lower utilisation so can safely cope with spike in demand). [source]\nPercentage of entities meeting a target (e.g. 4 hour arrival to admission). Consider whether anything in historical data patterns is because of trying to meet targets (e.g. 17% of admissions to a&e between 3h50 and 4h). “If the target was removed, would this result in a change in behaviour? How might the predictions of our model be affected by this?”\nThroughput - % of people entering system who have left by time model stops running - low throughput suggests a bottleneck - “can be a useful measure to track as a quick way of assessing whether different scenarios are leading to severe bottlenecks, but it is not that useful as a standalone measure.” [source]\n\n\n22.9.2 Scenario analysis\nScenario analysis involves compare the results from multiple different scenarios (e.g. different levels of resource, arrivals, etc).\nReproducibility: When doing this, use different random seeds for each replication or move fixed LARGE number of steps down random number stream on each replication. However, you might use the same pseudo random numbers in two scenarios, else you see change from scenario and random numbers (which means more noise and needs more replications), whereas consistency means less needed. When doing this, we create a correlation between the two scenarios.[TomL7]\nYou may have limited number of pre-defined scenarios to compare (comparing alternatives), or have no pre-defined scenarios and instead be varying experiemental factors until you reach a target or output metric level (search experimentation).[source]\n\n\n22.9.3 Replications\nHave replications (multiple runs) when estimating the performance of a given scenario.\nTo decide how many do to, find the point where the confidence interval is consistently 10% deviation from the mean (worth checking by adding a few more replications, and also, you might never reach that point).[TomL10]\nImage from Tom Lecture 10:\n\n\n\nChoosing number of scenarios",
    "crumbs": [
      "Simulation",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Discrete event simulation (DES)</span>"
    ]
  },
  {
    "objectID": "content/simulation/discrete_event.html#model-verification-and-validation",
    "href": "content/simulation/discrete_event.html#model-verification-and-validation",
    "title": "22  Discrete event simulation (DES)",
    "section": "22.10 Model verification and validation",
    "text": "22.10 Model verification and validation\nVerification = ensure model design has been transformed into computer model with sufficient accuracy\nValidation = ensure model is sufficiently accurate for purpose * Conceptual model validation - whether content, assumptions and simplifications are sufficiently accurate (i.e. does model have necessary details to meet study objective) * Data validation - whether data used at any stage are sufficiently accurate * White-box validation - careful check of each part of model and whether it represents real world with sufficient accuracy * Black-box validation - thinking overall whether model represents real world with sufficient accuracy * Experimentation validation - ensuring procedures allow accurate results i.e. requirements for removing initialisation bias, run length, replications, sensitivity analysis, and searching of solution space to ensure that learning is maximised and appropriate solutions identified * Solution validation - compared the simulation model to the implemented solution (i.e. after modelling done and made changes in real life) [source]",
    "crumbs": [
      "Simulation",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Discrete event simulation (DES)</span>"
    ]
  },
  {
    "objectID": "content/simulation/discrete_event.html#packages-and-software-for-discrete-event-simulation",
    "href": "content/simulation/discrete_event.html#packages-and-software-for-discrete-event-simulation",
    "title": "22  Discrete event simulation (DES)",
    "section": "22.11 Packages and software for discrete event simulation",
    "text": "22.11 Packages and software for discrete event simulation\nPython packages: * SimPy * CIW - docs, example, conversion from SimPy * Salabim - docs, provides methods for 2D and 3D animation\nR packages: * simmerR - docs, conversion from SimPy, tutorial, can plot flow diagram of steps in model\nFOSS GUI software (i.e. drag and drop): * JaamSim [source]\nNon-FOSS GUI: * Simul8",
    "crumbs": [
      "Simulation",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Discrete event simulation (DES)</span>"
    ]
  },
  {
    "objectID": "content/simulation/discrete_event.html#examples",
    "href": "content/simulation/discrete_event.html#examples",
    "title": "22  Discrete event simulation (DES)",
    "section": "22.12 Examples",
    "text": "22.12 Examples\n\nStreamlit app with key DES concepts\nSimPy examples\nVisualisation examples [source]",
    "crumbs": [
      "Simulation",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Discrete event simulation (DES)</span>"
    ]
  },
  {
    "objectID": "content/causal_concepts/1_predict_vs_causal.html",
    "href": "content/causal_concepts/1_predict_vs_causal.html",
    "title": "23  Prediction v.s. causal inference",
    "section": "",
    "text": "23.1 How do you know whether you are interested in prediction or causation?\n`````dyhijpaaftgx Executive summary :class: info\nPredictive research aims to predict an outcome with the best accuracy. Explainability (e.g. SHAP) is about understanding why a model makes certain predictions. When making predictions, whether the direction of relationships (e.g. from SHAP values) is true/causal doesn’t matter, as the goal is just to make the best predictions.\nEtiological research aims to uncover causal effects. It involves finding an unbiased estimate of the effect of X on Y, by controlling for confounding factors that could bias the estimate. In causal inference, the true direction of relationships (and the counterfactual scenarios) are important. It typically starts with drawing a causal diagram.\n`````\nScientific research can be categorised into descriptive, predictive and etiological research. Descriptive research aims to summarise the characteristics of a group (or person).[Hamaker et al. 2020] However, this page focusses just on predictive and etiological research.",
    "crumbs": [
      "Causal concepts",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Prediction v.s. causal inference</span>"
    ]
  },
  {
    "objectID": "content/causal_concepts/1_predict_vs_causal.html#how-do-you-know-whether-you-are-interested-in-prediction-or-causation",
    "href": "content/causal_concepts/1_predict_vs_causal.html#how-do-you-know-whether-you-are-interested-in-prediction-or-causation",
    "title": "23  Prediction v.s. causal inference",
    "section": "",
    "text": "Predictive research\nEtiological research (or “explanatory” research)\n\n\n\n\nAim\nAims to predict an outcome with the best accuracy.[Ramspek et al. 2021]\nAims to uncover causal effects - i.e. causal inference.\n\n\nRelationships\nIt doesn’t matter whether predictors are causal or not - just focussed on best prediction.\nConcerned with the true causal relationships between variables.\n\n\nDirectionality\nWe’re interested in associations (i.e. relationships aren’t directional).[source]\nIt is important that relationships are directional, as these directions are required to support interventional reasoning.[source]\n\n\n\n\n23.1.1 Illustrative example\nA team have built an XGBoost model to predict whether customers will renew their subscription. They use SHAP values to understand how the model made its predictions. They notice a suprising finding: users who report more bugs are more likely to renew. Is this a problem? It depends on their goal\n\nIf their goal is to predict customer retention to estimate future revenue, then this relationship is helpful for prediction, and it doesn’t matter about the direction, as long as our predictions are good.\nHowever, if their goal is to inform actions to help retain customers, then it is important to understand the true relationships between features and the outcomes, and the counterfactual scenarios if features were modified. In this case, the team are interested in causation. In order for the team to understand the causal relationships, they would need to use causal inference methods (causal diagrams, appropriate techniques to account for confounding).\n\nWhy did this finding occur? If the team are interested in causation, they could draw a causal diagram (simplified version below). In it, they notice that some features are influenced by unmeasured confounding. WIth the example above, users who report more bugs are people who use the product more so encounter more bugs, but need the product more so are more likely to report. Because they can’t directly measure product need, the correlation they end up capturing in the predict model between bugs reported and renewal combines a small negative direct effect of bugs faced and a large positive confounding effect from product need. [source]\n\n\n\n\n\n  flowchart LR;\n\n    need(\"&lt;b&gt;Unmeasured&lt;/b&gt;&lt;br&gt;Product need\"):::white;\n    month(\"Monthly usage\"):::white;\n    face(\"&lt;b&gt;Unmeasured&lt;/b&gt;&lt;br&gt;Bugs faced\"):::white;\n    report(\"Bugs reported\"):::white;\n    ren(\"Did renew\"):::important;\n\n\n    month --&gt; face; face --&gt; report; face --&gt; ren;\n    need --&gt; report;\n    need --&gt; month;\n    need --&gt; ren;\n    month --&gt; ren;\n\n    classDef white fill:#FFFFFF, stroke:#FFFFFF\n    classDef black fill:#FFFFFF, stroke:#000000\n    classDef important fill:#DDF2D1, stroke: #FFFFFF;\n\n\n\n\n\n\n\n\n23.1.2 Explainability v.s. causality\nExplainability refers to being able to understand why a model makes certain predictions. The aim of explainable AI is to make ML models more transparent. It provides insights on: * How a model makes predictions * What features are most important * How sensitive a model is to changes in the input [Prosperi et al. 2020]\nThe contribution of individual covariates are often mistakenly interpreted causally, but the methods used were focused on combining covariates to optimise predictive accuracy, and not to predict the outcome distribution under hypothetical interventions. [Lin et al. 2021]\nHowever, it cannot be used to infer causal relationships, since the findings may be biased by stratification or unmeasured confounders, or mediated by other factors in the causal pathway.[Prosperi et al. 2020] If you wish to make claims about causality, you will need to build a casual model. Causal ML aims to infer causal relationships from observational data by estimating the effect of a specific variable on the outcome, while appropriately controlling for other confounding factors that could bias the estimate.[source]",
    "crumbs": [
      "Causal concepts",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Prediction v.s. causal inference</span>"
    ]
  },
  {
    "objectID": "content/causal_concepts/1_predict_vs_causal.html#doing-predictive-and-etiological-research",
    "href": "content/causal_concepts/1_predict_vs_causal.html#doing-predictive-and-etiological-research",
    "title": "23  Prediction v.s. causal inference",
    "section": "23.2 Doing predictive AND etiological research",
    "text": "23.2 Doing predictive AND etiological research\nMany problems will require a combination of prediction and causation. * “Pure forecasting task” - e.g. just want to predict whether or not it will rain, and don’t care why/what caused the rain * “Pure causation task” - e.g. performing a rain dance presumed to save dying crops, only if it actually causes rain * Combination of the two - e.g. if planning assignment of fire inspectors across a city, should (a) predict will establishment will be in violation of fire codes, and (b) estimate causal effect on establishment’s behaviour of receiving an inspection or not\nBeck et al. 2018 also argue that prediction remains relevant even if you’re only interested in understanding causal effects. Explanations that invoke causal mechanisms always make predictions - specifically, predictions about what will happen under an intervention. ‘Whether they do so explicitly or not, that is, causal claims necessarily make predictions; thus it is both fair and arguably useful to hold them accountable for the accuracy of the predictions they make.’ They therefore argue that the predictive performance of models and of explanations is important to include (e.g. R2, MAE, RMSE, AUC, accuracy, recall, F1).[Beck et al. 2018]",
    "crumbs": [
      "Causal concepts",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Prediction v.s. causal inference</span>"
    ]
  },
  {
    "objectID": "content/causal_concepts/1_predict_vs_causal.html#when-can-prediction-models-answer-causal-questions",
    "href": "content/causal_concepts/1_predict_vs_causal.html#when-can-prediction-models-answer-causal-questions",
    "title": "23  Prediction v.s. causal inference",
    "section": "23.3 When can prediction models answer causal questions?",
    "text": "23.3 When can prediction models answer causal questions?\nAs this example is from a simulation study where know true causal effects, we can plot the SHAP values from the prediction models v.s. the known true causal effects.\n\n\n\nCausal effects\n\n\nWe can also add clustering to see the redundancy structure of the data as a dendrogram - ‘when features merge together at the bottom (left) of the dendrogram it means that that the information those features contain about the outcome (renewal) is very redundant and the model could have used either feature. When features merge together at the top (right) of the dendrogram it means the information they contain about the outcome is independent from each other.’\n\n\n\nRedundancy\n\n\nWhen can predictive models answer causal questions? When the feature is independent of (a) other features in the model, and (b) unobserved confounders. Hence, it is not subject to bias from either unmeasured confounders or feature redundancy. Example: Economy * Independent from other features in dendogram (no observed confounding) * No unobserved confounding in causal digram\nWhen can they not be used? (1) When you have observed confounding. Example: Ad Spend (no direct causal effect on retention, but correlated with Last upgrade and Monthly usage which do drive retention). ‘Our predictive model identifies Ad Spend as the one of the best single predictors of retention because it captures so many of the true causal drivers through correlations. XGBoost imposes regularization, which is a fancy way of saying that it tries to choose the simplest possible model that still predicts well. If it could predict equally well using one feature rather than three, it will tend to do that to avoid overfitting.’\nHowever, there are methods to deal with observed confounding, such as double/debiased machine learning model. This involves: 1. Train model to predict feature (Ad spend) using set of confounders (features not caused by Ad spend) 2. Train model to predict outcome (Did Renew) using that set of confounders 3. Train model to predict residual variation of outcome (the variation left after subtracting our prediction) using the residual variation of the causal feature of interest\n‘The intuition is that if Ad Spend causes renewal, then the part of Ad Spend that can’t be predicted by other confounding features should be correlated with the part of renewal that can’t be predicted by other confounding features.’ There are packages like econML’s LinearDML for this.\nWhen can they not be used? (2) When you have non-confounding redundancy. ‘This occurs when the feature we want causal effects for causally drives, or is driven by, another feature included in the model, but that other feature is not a confounder of our feature of interest.’\nExample: Sales Calls directly impact retention, but also have an indirect effect on retention through Interactions. We can see this in the SHAP scatter plots above, which show how XGBoost underestimates the true causal effect of Sales Calls because most of that effect got put onto the Interactions feature.\n‘Non-confounding redundancy can be fixed in principle by removing the redundant variables from the model (see below). For example, if we removed Interactions from the model then we will capture the full effect of making a sales call on renewal probability. This removal is also important for double ML, since double ML will fail to capture indirect causal effects if you control for downstream features caused by the feature of interest. In this case double ML will only measure the “direct” effect that does not pass through the other feature. Double ML is however robust to controlling for upstream non-confounding redundancy (where the redundant feature causes the feature of interest), though this will reduce your statistical power to detect true effects. Unfortunately, we often don’t know the true causal graph so it can be hard to know when another feature is redundant with our feature of interest because of observed confounding vs. non-confounding redundancy. If it is because of confounding then we should control for that feature using a method like double ML, whereas if it is a downstream consequence then we should drop the feature from our model if we want full causal effects rather than only direct effects. Controlling for a feature we shouldn’t tends to hide or split up causal effects, while failing to control for a feature we should have controlled for tends to infer causal effects that do not exist. This generally makes controlling for a feature the safer option when you are uncertain.’\nWhen can they not be used? (3) When you have unobserved confounding. ‘The Discount and Bugs Reported features both suffer from unobserved confounding because not all important variables (e.g., Product Need and Bugs Faced) are measured in the data. Even though both features are relatively independent of all the other features in the model, there are important drivers that are unmeasured. In this case, both predictive models and causal models that require confounders to be observed, like double ML, will fail. This is why double ML estimates a large negative causal effect for the Discount feature even when controlling for all other observed features’\n‘Specialized causal tools based on the principals of instrumental variables, differences-in-differences, or regression discontinuities can sometimes exploit partial randomization even in cases where a full experiment is impossible. For example, instrumental variable techniques can be used to identify causal effects in cases where we cannot randomly assign a treatment, but we can randomly nudge some customers towards treatment, like sending an email encouraging them to explore a new product feature. Difference-in-difference approaches can be helpful when the introduction of new treatments is staggered across groups. Finally, regression discontinuity approaches are a good option when patterns of treatment exhibit sharp cut-offs (for example qualification for treatment based on a specific, measurable trait like revenue over $5,000 per month).’\n[source]",
    "crumbs": [
      "Causal concepts",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Prediction v.s. causal inference</span>"
    ]
  },
  {
    "objectID": "content/causal_concepts/1_predict_vs_causal.html#confusion-and-controversy",
    "href": "content/causal_concepts/1_predict_vs_causal.html#confusion-and-controversy",
    "title": "23  Prediction v.s. causal inference",
    "section": "23.4 Confusion and controversy",
    "text": "23.4 Confusion and controversy\n\n23.4.1 Reasons for confusion\nCausal inference can be confusing and controversial. Reasons for this are: * Causally unrelated variables can be highly correlated * Results may be reported in a way that is careful to avoid referring to any causal relationships, but it will often still naturally be read and interpreted as causal * Even if there is a causal relationship, sometimes the direction is unclear - would need to carefully examine the temporal relationships between the variables[source]\n\n\n23.4.2 Confusion in the literature\nIn practice, prediction and causation are commonly conflated. A review of observational studies found that 26% (46 / 180) observational cohort studies conflated between etiology and prediction - * In causal studies, this was mainly due to selection of covariates based on their ability to predict without taking causal structure into account. * In prediction studies, this was mainly due to causal interpretation of covariates included in a prediction model.",
    "crumbs": [
      "Causal concepts",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Prediction v.s. causal inference</span>"
    ]
  },
  {
    "objectID": "content/causal_concepts/1_predict_vs_causal.html#concepts-and-principles",
    "href": "content/causal_concepts/1_predict_vs_causal.html#concepts-and-principles",
    "title": "23  Prediction v.s. causal inference",
    "section": "23.5 Concepts and principles",
    "text": "23.5 Concepts and principles\n\n23.5.1 Ladder of causality\nJudea Pearl proposed the ‘Ladder of Causality’ to categorise different levels of causal thinking, with increasing levels of difficulty.\n\n\n\n\n\n\n\n\n\nLevel\nTypical activity\nTypical questions\nExamples\n\n\n\n\nAssociation\nSeeing\nWhat is?How would seeing X change my belief in Y?\nWhat does a symptom tell me about a disease?What does a survey tell us about the election results?\n\n\nIntervention\nDoing\nWhat if?What if I do X?\nWhat if I take aspirin, will my headachbe be cured?What if we ban cigarettes?\n\n\nCounterfactuals\nImagining, retrospection\nWhy?Was it X that caused Y? What if I had acted differently?\nWas it aspirin that stopped my headache?Would Kennedy be alive had Oswald not shot him?What if I had not been smoking the past two years?\n\n\n\n[source]\nDifference between interventions and counterfactuals in this hierarchy: * With interventions, you ask what will happen on average if you perform an action. * With counterfactuals, you ask what would have happened if you had performed a different action. * These two queries are mathematically distinct as they require different levels of information to be answered (counterfactuals need more information to be answered)’.[source]\n\n\n23.5.2 Types of inference\nFrom C.S.Peirce in late 1800s: * ‘Deduction - necessary inference following logic’ [Zhao 2022] * e.g. If dentist appointment at 10 and it’s 30 minute drive, deduce you need to leave at 9.30 [source] * ‘Induction - probable or non-necessary inference (purely) based on statistical data * e.g. Correlation between cigarette smoking and lung cancer’ [Zhao 2022] * Four of your six coworker order the same sandwich so you induce that the sandwich is probably good [source] * ‘Abduction - inference with implicit or explicit appeal to explanatory considerations * e.g. Investigation of a crime scene * Cigarette smoking causes lung cancer’ [Zhao 2022]",
    "crumbs": [
      "Causal concepts",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Prediction v.s. causal inference</span>"
    ]
  },
  {
    "objectID": "content/causal_concepts/2_intro_to_causality.html",
    "href": "content/causal_concepts/2_intro_to_causality.html",
    "title": "24  The three “languages” of causal inference",
    "section": "",
    "text": "24.1 Causal inference\n`````hesommckbsfj Executive summary :class: info\nIn causal studies, you are aiming to find an unbiased estimate of the effect of a exposure or treatment on an outcome, by carefully controlling for confounders of the relationship between those two variables.\nPotential outcomes/counterfactuals framework for causal inference states that we draw causal inference thought contrasting actual values and counterfactual values (if they had or had not received treatment/exposure). It is focused on all possible outcomes, and has specific notation you can use.\nCausal diagrams (directed acyclic graphs - DAGs) are based on expert knowledge. They depict the causal relationships between nodes using directed arrows. They are used to illustrate/identify sources of bias, to indicate where associations/independence should be expected, and to help inform study design.\nStructural Equation Modelling (SEM) is a statistical technique to model hypothesised relationships among variables, and these can be causal models if certain criteria are met. SEM involves visualising the hypothesised model, translating the DAG to a path statement, assessing fit statistics, improving model fit using modification indices, and so on. `````\nCausal inference research aims to uncover causal effects. It involves finding an unbiased estimate of the effect of X on Y, by controlling for confounding factors that could bias the estimate. This is an estimate of the causal effect of an exposure on an outcome.[Lederer et al. 2018]\nWhen it comes to talking about and defining causality, pioneers in causal inference have come up with three languages.",
    "crumbs": [
      "Causal concepts",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>The three \"languages\" of causal inference</span>"
    ]
  },
  {
    "objectID": "content/causal_concepts/2_intro_to_causality.html#causal-inference",
    "href": "content/causal_concepts/2_intro_to_causality.html#causal-inference",
    "title": "24  The three “languages” of causal inference",
    "section": "",
    "text": "Language\nPioneers\nStrengths\nLimitations\n\n\n\n\nUsing potential outcomes / counterfactuals\n1923 Neyman (statistics); 1973 Lewis (philosophy); 1974 Rubin (statistics); 1986 Robins (epidemiology); [Zhao 2022]\nGood for articulating the inference for a small number of causes and effects [source]Easy to add additional assumptions [Zhao 2022]\nNot as convenient if the system is complex [Zhao 2022]\n\n\nUsing graphs\n1921 Wright (genetics); 1988 Pearl (computer science “AI”); 1993 Spirtes, Glymour, Scheines (philosophy). [Zhao 2022]\nGood for understanding the scientific problems [source]Easy to visualise the causal assumptions [Zhao 2022]\nDifficult for statistical inference because model is non-parametric [Zhao 2022] (i.e. doesn’t make explicit assumptions about functional form of underlying population distribution… inference more challenging as no predefined functional forms)\n\n\nUsing structural equations\n1921 Wright (genetics); 1943 Haavelmo (econometrics); 1975 Duncan (social sciences); 2000 Pearl (computer science). [Zhao 2022]\nGood for fitting simultaneous models for the variables (espeically for abstract concepts)[source]Bridge between graphs and counterfacturals.Easy to operationalise[Zhao 2022]\nDanger to be confused with regression [Zhao 2022]",
    "crumbs": [
      "Causal concepts",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>The three \"languages\" of causal inference</span>"
    ]
  },
  {
    "objectID": "content/causal_concepts/2_intro_to_causality.html#potential-outcomescounterfactuals",
    "href": "content/causal_concepts/2_intro_to_causality.html#potential-outcomescounterfactuals",
    "title": "24  The three “languages” of causal inference",
    "section": "24.2 Potential outcomes/counterfactuals",
    "text": "24.2 Potential outcomes/counterfactuals\nNames for this approach: * Potential outcomes framework * Counterfactual approach * Equivalent concepts approach,[source] * Rubin or Neyman-Rubin causal model. [Igelström et al. 2022]\nThe basic idea of this approach\nUsing this framework, you describe your causal problem using counterfactuals. A counterfactual is a comparison between what is and what might have been - for example, “I know ingesting the poison killed John, because if John had not ingested the poison, I know he would have lived”. ‘The counterfactual outcomes of a specific individual can never be known, since we can never observe the same individual both exposed and unexposed under the same circumstances’. [Igelström et al. 2022] Instead, we estimate an average causal effect, looking at counterfactual outcomes between groups (rather than individuals).\nThe logic of counterfactuals if that you can draw causal inferences when the distribution of observed outcomes in treated group equals in expectation the distribution if they had not ben treated - and because no-one can both receive and not receive treatment, ‘causal inferences implicitly contrast actual values with counterfactual values’. In other words: ‘when exchangeability between those receiving and not receiving an intervention is obtained, the causal counterfactual can be estimated. When exchangeability (i.e., no confounding) is achieved, the contrast of actual outcomes among people who received the intervention in comparison with those who did not receive the intervention can provide the causal effect of the intervention’ [Glymour and Spiegelman 2017]\nExposure and outcomes\nWe need to identify an exposure and an outcome (and this is true for all causal inference research). An exposure is a ‘treatment, intervention or other variable that could have taken one of several counterfactual values’. [Igelström et al. 2022] We are often focussed on interventions (i.e. variables that can be manipulated), as they fit well in the potential outcomes framework. Although other variables like age, race and gender can have causal effects, they do not fit as cleanly in the potential outcomes framework.[source] Rubin states that “if you are not talking about an intervention, you can’t talk about causality” - i.e. we’re defining causality using counterfactuals, which is about the counterfactual effects of the intervention[source]\n‘Potential outcomes refer to all possible outcomes that an individual could experience—both those which are observed (factual) and those which are not (counterfactual). Given a binary exposure and a binary outcome, the possible combinations of actual and counterfactual outcomes give rise to four causal types’: * ‘Doomed: would have experienced the outcome regardless of exposure. * Causative: would have experienced the outcome if exposed, otherwise not. * Preventative: would have experienced the outcome if unexposed, otherwise not. * Immune: would not have experienced the outcome regardless of exposure status.’ [Igelström et al. 2022]\nMathematical notation\nThis framework ‘uses mathematical notation to describe counterfactual outcomes and can be used to describe the causal effect of an exposure on an outcome in statistical terms’. [Igelström et al. 2022] There are various notations used, but some examples are: * Potential outcome if exposed (Ya=1) or not exposed (Ya=0) * Expected value of continuous outcome (E(Y)) or probability of binary outcome(P(Y=1)) * Conditional expectation - expected value of Y given that another variable C is 1 (E(Y|C=1)) - i.e. expected values conditional on C / within levels of C / holding C constant. [Igelström et al. 2022]",
    "crumbs": [
      "Causal concepts",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>The three \"languages\" of causal inference</span>"
    ]
  },
  {
    "objectID": "content/causal_concepts/2_intro_to_causality.html#graphs",
    "href": "content/causal_concepts/2_intro_to_causality.html#graphs",
    "title": "24  The three “languages” of causal inference",
    "section": "24.3 Graphs",
    "text": "24.3 Graphs\nUsing this approach, you describe your causal problem using a causal diagram called a directed acyclic graphs (DAGs). DAGs are composed of nodes and arrows which depict the causal relationships between different variables. They are: * Directed - as arrows have a single direction (unidirectional) that represents known causal effects (based on prior knowledge) * Acyclic - as nodes cannot have a directed path from itself back to itself [HarvardX PH559x]\nExample:\n\n\n\n\n\n  flowchart LR;\n\n    asp(\"Aspirin\"):::white;\n    str(\"Stroke\"):::white;\n\n    asp --&gt; str;\n\n    classDef white fill:#FFFFFF, stroke:#FFFFFF\n    classDef black fill:#FFFFFF, stroke:#000000\n\n\n\n\n\n\nWe don’t draw causal diagrams as an exact, accurate representation of the world - instead, we draw causal DAGs to help us think about possible sources of bias when making causal inferences.\n\nThey make sure we illustrate and identify our sources of biases (assumptions)\n\nMore precise and efficient than writing pages of assumptions[HarvardX PH559x]\nAlthough they are based on assumptions, so are analytic models.[source]\nAlthough investigators often feel some discomfort in deciding what causal effects do and do not exist on the basis of prior knowledge, the advantage of this approach is that it makes these assumptions explicit (and hence transparent).[Lederer et al. 2018]\n\nThey indicate when associations or independence should be expected.[HarvardX PH559x]\nThey can help determine whether the effect of interest can be identified from available data, and help us clarify our study question[source] - and to identify problems in the study design[HarvardX PH559x]\n\nTo find out more about DAGs, see the subsequent page on DAGs.",
    "crumbs": [
      "Causal concepts",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>The three \"languages\" of causal inference</span>"
    ]
  },
  {
    "objectID": "content/causal_concepts/2_intro_to_causality.html#structural-equation-modelling-sem",
    "href": "content/causal_concepts/2_intro_to_causality.html#structural-equation-modelling-sem",
    "title": "24  The three “languages” of causal inference",
    "section": "24.4 Structural equation modelling (SEM)",
    "text": "24.4 Structural equation modelling (SEM)\n‘Structural Equation Modelling (SEM) is a statistical technique to model hypothesised relationships among variables.’ We first specify these relationships, based on subject matter expertise, either: * Graphically * By listing a set of functions - hence “structural”\nThe relationships can then be ‘described using structural equations, so called because they describe causal relationships rather than observed associations. A set of structural equations can sometimes be rewritten as a single reduced form equation.’ [Igelström et al. 2022]\nVariables can be: * Manifest (observed) or latent (unobserved) * Exogenous (have no cause themselves, but affect others) or endogenous (values are caused by other variables)\nRelationships between variables can be one of: * Correlational / bidrectional * Isolated / conditionally independent * Causal / unidirectional\nIn a graphical model, representations include: * Ovals = latent variables * Rectangles = manifest variables * Single or double headed arrows indicate nature of relationship [Madhanagopal and Amrhein 2019]\n\n\n\nExample of an SEM graphical model\n\n\nStructural Causal Models\nStructural Causal Models (SCM) were proposed by Judea Pearl. They integrate SEM and graphical models to aid understanding of causal relationships. SEMs are mainly ‘used to confirm a model rather than to explore a phenomenon’. SEMs can be interpreted as causal models if they meet the conditions: 1. Structure is valid representation of reality 2. Relationships are directed and acyclic * Directed acyclic gropus are a subset of graphical models, where relationships must be directional 3. Variables, conditioned on their parents, are independent of their ancestores * See example below - soccer is ancestor of heatstroke - this conditoin is met if soccer only causes heatstroke via dehydration - and is not met if soccer effects heat stroke directly or through another mediating variable (I think, if not included)\n\n\n\n\n\n  flowchart LR;\n\n    soccer(\"Soccer\"):::white;\n    dehy(\"Dehydration\"):::white;\n    heat(\"Heatstroke\"):::white;\n\n    soccer --&gt; dehy;\n    dehy --&gt; heat;\n\n    classDef white fill:#FFFFFF, stroke:#FFFFFF\n    classDef black fill:#FFFFFF, stroke:#000000\n\n\n\n\n\n\n\nThere are no “back doors” from cause to effect [Madhanagopal and Amrhein 2019]\n\nCovariance matrix\n‘The fundamental unit of information in an SEM is the covariance matrix of the model variables.’\n‘An ‘Under-Identified’ model is a model in which it is not possible to estimate all the model parameters because there are too few unique elements. A ‘Just-Identified’ model is a model in which the number of unique covariance elements equals the number of parameters being estimated. An ‘Over-Identified’ model is a model in which the number of unique covariance parameters is greater than the number of parameters being estimated. The difference is the degrees of freedom available for hypothesis tests. The total number of estimated parameters in the model should always be lower than fundamental unit of information in the data; i.e. the model should be over-identified.’ [Madhanagopal and Amrhein 2019]\nPath statement\nEach of the single headed arrows in the diagram ‘represents a hypothesised dependency. For each of these paths,’ we ‘estimate a path coefficient and test whether the coefficient statistically differs from zero’. [Madhanagopal and Amrhein 2019]\nFit statistics\nWe assess goodness of fit, and can explore model modifications to improve fit, by: 1. ‘Increasing the number of paths (i.e. allowing the corresponding coefficients to be estimated) 2. Reducing the number of paths (i.e. constraining the corresponding coefficients to zero)’ [Madhanagopal and Amrhein 2019]",
    "crumbs": [
      "Causal concepts",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>The three \"languages\" of causal inference</span>"
    ]
  },
  {
    "objectID": "content/causal_concepts/3_dags.html",
    "href": "content/causal_concepts/3_dags.html",
    "title": "25  Directed acyclic graphs",
    "section": "",
    "text": "25.1 Naming conventions\n`````iideqlmxnixc Executive summary :class: info\nIn a DAG, you can identify: * Confounders - common cause both treatment and outcome * Mediators - lie on causal path between variables, inclusion depends on whether you are interested in direct effect of treatment and outcome that doesn’t pass through mediator * Colliders - common effect of two other variables * Moderators - change size or direction of relationship between variables\nYou start the diagram with your research question (i.e. two nodes whose relationship you are interested) and then add all common causes for those nodes, and for any other nodes you add to the graph.\nD-seperation rules determine whether paths will be open (expect associations) or blocked (independent), which are based on whether condition or not on confounders and colliders.\nIf you have time-varying treatment (takes different values over time), then you will have other time-varying components (e.g. time-varying confounding). If there is treatment-confounder feedback (i.e. earlier treatment impacts value of later confounder), then you will need to use a special type of method to adjust for confounders, referred to as G-methods.\nSince you have designed the study to appropriate control for confounding for your relationship of interest - between a given treatment/exposure and outcome - the other variables included may have residual confounding or other biases that affect their associations, and it is important that these effect estimates are not presented (or are explained) - otherwise this is called ‘Table 2 fallacy’. `````\nThis page continues on from the introduction to directed acyclic graphs on the prior causal inference page.\nThere are naming conventions for particular components of the DAG: * A (or E) = Exposure / Treatment / Intervention / Primary IV * Y (or D) = Outcome * C = Covariates / Confounders * U = Unmeasured relevant variables\nWhen letters are not used, the exposure and outcome will often be highlighted using a “?” on the arrow, or through coloured boxes or arrows.\nExample:\nflowchart LR;\n\n    A:::green;\n    Y:::green;\n    C:::white;\n    U:::white;\n    \n    A --&gt;|?| Y;\n    C --&gt; A;\n    C --&gt; Y;\n    U --&gt; A;\n    U --&gt; C;\n\n    classDef white fill:#FFFFFF, stroke:#FFFFFF\n    classDef black fill:#FFFFFF, stroke:#000000\n    classDef green fill:#DDF2D1, stroke: #FFFFFF;\nNodes can also be described as: * Ancestor = direct cause (parent) or indirect cause (e.g. grandparent) of a variable * Descendent = direct effect (child) or indirect effect (e.g. grandchild) of a variable [HarvardX PH559x]",
    "crumbs": [
      "Causal concepts",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Directed acyclic graphs</span>"
    ]
  },
  {
    "objectID": "content/causal_concepts/3_dags.html#key-components",
    "href": "content/causal_concepts/3_dags.html#key-components",
    "title": "25  Directed acyclic graphs",
    "section": "25.2 Key components",
    "text": "25.2 Key components\n\n25.2.1 Confounders\nConfounders are variables that cause BOTH the treatment/exposure and outcomes.\nInformally, it occurs when there is an open backdoor path between the treatment/exposure and outcome, and you could say a confounder is a variable that - possibly together with other variables - can be used to block the backdoor path between the treatment and outcome.\nWe included measured and unmeasured confounders in our DAG.[HarvardX PH559x]\nWe can use conditioning to control for confounding. This involves examining the relationship between A and Y within levels of the conditioning variable, using either: (a) sample restriction, (b) stratification, (c) adjustment, or (d) matching. If we don’t do this, we will get confounding bias (where a common cause of A and Y is not blocked). When you condition on something, you draw a box around it on the DAG.\nOther terms like “adjusting” or “controlling” suggest a misleading interpretation of the model - although there does seem to be variability in terminology, with many sources using these terms. [source] Igelström et al. 2022 explain that ‘conditioning on a variable is analogous to controlling for, adjusting for or stratifying by it (although in practice, different methods of conditioning may have different effects on the results and their interpretation).’[Igelström et al. 2022]\nExample: smoking causes yellow fingers and lung cancer * If we don’t condition on it, we expect to see an association between yellow fingers and lung cancer (known as a marginal/unconditional association) * If we do condition on smoking, we expect to see no association between yellow fingers and lung cancers (i.e. they are “not associated conditional on smoking)[HarvardX PH559x]\n\n\n\n\n\n  flowchart TD;\n\n    con:::outline;\n    subgraph con[\"`**Conditional**`\"]\n      cig2(\"Smoking\"):::black;\n      lung2(\"Lung cancer\"):::white;\n      yellow2(\"Yellow fingers\"):::white;\n    end\n\n    cig2 --&gt; lung2;\n    cig2 --&gt; yellow2;\n\n    uncon:::outline;\n    subgraph uncon[\"`**Unconditional**`\"]\n      cig(\"Smoking\"):::white;\n      lung(\"Lung cancer\"):::white;\n      yellow(\"Yellow fingers\"):::white;\n    end\n\n    cig --&gt; lung;\n    cig --&gt; yellow;\n\n    classDef white fill:#FFFFFF, stroke:#FFFFFF;\n    classDef black fill:#FFFFFF, stroke:#000000;\n    classDef outline fill:#FFFFFF;\n\n\n\n\n\n\n\n\n25.2.2 Moderators\nModerators are variables that change the size or direction of the relationship between variables. These could also be referred to as effect modifiers or statistical interaction. [source] * If it impacts the size of the relationship, this is called non-qualitative effect modification * If it impacts the direction of the relationship, this is called qualitative effect modification [Hernán and Robins 2024] * These can also be referred to as effect measure modification (EMM). [Igelström et al. 2022]\nThey usually help you judge the external validity of your study by identifying the limitations of when the relationship between variables holds. [source]\nThere has been some disagreement on how these should be included/notation within DAGs. [source][Weinberg 2007]\n‘The presence and extent of EMM mathematically depends on the choice of an additive or multiplicative scale linking exposure and outcome; EMM may be present on either one of these scales or both’\n‘If both the exposure and effect modifier are causes of the outcome, then EMM will always be present on at least one scale.’\n‘Interaction denotes that the joint effect of two exposures is different from the sum of the individual effects of each exposure. Like EMM, the presence and extent of interaction depends on the choice of an additive or multiplicative scale and does not necessarily have a meaningful causal interpretation. ‘Interaction’ is sometimes used interchangeably with EMM, but it is helpful to think of these as different concepts: * Interaction focuses on the joint causal effect of two exposures (eg, the combined effect of smoking and asbestos exposure on lung cancer) * EMM focuses on the effect of one exposure whose effect differs across levels of another variable (eg, the effect of asbestos exposure on lung cancer in smokers vs non-smokers); with EMM, the causal effect of the effect modifier itself is not of interest.’ [Igelström et al. 2022]\n\n\n\n\n\n  flowchart LR;\n\n    A(\"A (treatment/exposure)\"):::green;\n    Y(\"Y (outcome)\"):::green;\n    Empty[ ]:::empty;\n    Mod(\"Moderator\"):::white;\n\n    Mod --&gt; Empty;\n    A --- Empty;\n    Empty --&gt;|?| Y;\n\n    classDef white fill:#FFFFFF, stroke:#FFFFFF;\n    classDef black fill:#FFFFFF, stroke:#000000;\n    classDef empty width:0px,height:0px;\n    classDef green fill:#DDF2D1, stroke: #FFFFFF;\n\n\n\n\n\n\nWhy do we care about modifiers/effect modification?\n\nThe average causal effect will differ between populations with different prevalence of the modifier - i.e. it depends on the distribution of individual causal effects in the population\n\n\n**Example: If average causal effect of exposure is harmful in women and beneficial in mean, a study with an equal gender split would find null causal effect, and a study with majority women would find harmful causal effect.*\nHence, ‘there is generally no such thing as “the average causal effect of treatment A on outcome Y (period)”, but “the average causal effect of treatment A on outcome Y in a population with a particular mix of causal effect modifiers.”’ The ability to extrapolate causal effects between populations is referred to as the transportability of causal inferences across populations.\nHowever, there will often be unmeasured effect modifiers, and so ‘transportability of causal effects is an unverifiable assumption that relives heavily on subject-matter experts’.[Hernán and Robins 2024]\n\n\nAdditive (but not multiplicative) effect modification can help identify groups who would most benefit from an intervention [Hernán and Robins 2024]\nIdentifying effect modification may help us to understand the biological, social, or other mechanisms leading to the outcome. [Hernán and Robins 2024]\n\n\n\n25.2.3 Mediators\nMediators are variables that lie in the causal path between the two other variables (e.g. between exposure and outcome), and they tell you how or why an effect takes place.[source] * A path that includes a mediator is often called an indirect effect or indirect causal path * In contrast, the arrow directly connecting the treatment and outcome represents the direct causal effect of the treatment on the outcome that is not due to changes in the mediator.[Lederer et al. 2018] This is also referred to as the controlled direct effect (CDE) [Igelström et al. 2022] * If you do not have a direct arrow between the treatment and outcome, and only via the mediator, this implies that this is the only way in which the treatment can cause the outcome, and that if you know the mediator is present, knowing whether or not the treatment was present should have no impact on the outcome.\n\n\n\n\n\n  flowchart LR;\n\n    treat(\"Treatment\"):::white;\n    med(\"Mediator\"):::white;\n    out(\"Outcome\"):::white;\n\n    treat --&gt; med;\n    med --&gt; out;\n\n    classDef white fill:#FFFFFF, stroke:#FFFFFF\n    classDef black fill:#FFFFFF, stroke:#000000\n\n\n\n\n\n\nYou might condition on a mediator if you are interested in the direct effect of treatment on outcome that doesn’t pass through mediator. Example: In racial disparity studies, will condition on mediators like socioeconomic, education, location (often though matching on these characteristics), to allow you to isolate the unique effect of race that is not explainable by those pathways. [source] This is referred to as mediation analysis - when you’re trying to ‘quantify how much of the total effect of A on Y is explained by a particular mediator (the indirect effect), and how much is not (the direct effect)’.\n\n\n\n\n\n  flowchart LR;\n\n    race(\"Race\"):::white;\n    outcome(\"Outcome\"):::white;\n    ses(\"Socioeconomic status\"):::black;\n    ed(\"Education\"):::black;\n    loc(\"Location\"):::black;\n\n    race --&gt;|?| outcome;\n    race --&gt; ses; ses --&gt; outcome;\n    race --&gt; ed; ed --&gt; outcome;\n    race --&gt; loc; loc --&gt; outcome;\n\n    classDef white fill:#FFFFFF, stroke:#FFFFFF\n    classDef black fill:#FFFFFF, stroke:#000000\n\n\n\n\n\n\n\n25.2.3.1 How is CDE estimated?\n\n‘Assuming no interaction between exposure and mediator, and no confounding between mediator and outcome, the indirect effect can be obtained by subtracting the CDE from the total effect’\n‘When interaction is present between exposure and mediator, the CDE will take on different values for different levels of the mediator, and the effect obtained by subtracting the CDE from the total effect no longer has a meaningful causal interpretation.’\n‘To address this problem, alternative definitions of causal direct and indirect effects have been proposed, such that their sum adds up to the total effect even in the presence of interactions, generally by allowing one or more of these effects to include the interaction effect.’ These include:\n\nControlled direct effect (CDE)\nNatural direct effect or pure direct effect\nNatural indirect effect or total indirect effect\nPure indirect effect\nTotal direct effect\n\n‘These effect estimands can be defined theoretically in counterfactual terms, but can only be estimated given additional assumptions that are difficult to verify and may lack applicability for estimating policy-relevant mediation quantities (eg, how much the effect of A on Y could be reduced by intervening on the mediator).’[Igelström et al. 2022]\n\n\n\n25.2.3.2 Over-adjustment bias\nSchisterman et al. 2009 defined over-adjustment bias as ‘control for an intermediate variable (or a descending proxy for an intermediate variable) on a causal path from exposure to outcome’ (i.e. controlling for a mediator).\n\n\n\n25.2.4 Colliders\nColliders are descendents of two other variables - i.e. common effect - with two arrows from the parents pointing to (“colliding with”) the descendent node. Colliders naturally block back-door paths. Controlling for a collider will open the back-door path, thereby introducing confounding.[Lederer et al. 2018] This is referred to as collider bias\nExample: A genetic factor and an environmental factor causing cancer. * Scenario #1: No conditioning - These are independent - i.e. genetic factor doesn’t have causal effect on environmental factor - and so we don’t expect to see an association between genetic and environment (unconditional/marginal association). * Scenario #2: Condition on cancer - If we condition on cancer - such as by just selecting people who have cancer - we will find an inverse association between genetics and environment (as if cancer wasn’t caused by one, it was by the other). This biased effect estimate is referred to as selection bias. * Scenario #3: Condition on surgery - We can induce selection bias by conditioning on the downstream consequence of a collider - e.g. if cancer is collider, and surgery is consequence of cancer, if we condition on surgery, we expect to see inverse association between genetic and environment conditional on surgery (just as we did for the collider cancer).[HarvardX PH559x]\n\n\n\n\n\n  flowchart TD;\n\n    con_sur:::outline;\n    subgraph con_sur[\"`**Condition on surgery**`\"]\n      gene3(\"Genetic&lt;br&gt;factor\"):::white;\n      env3(\"Environmental&lt;br&gt;factor\"):::white;\n      cancer3(\"Cancer\"):::white;\n      surgery3(\"Surgery\"):::black;\n    end\n\n    gene3 --&gt; cancer3;\n    env3 --&gt; cancer3;\n    cancer3 --&gt; surgery3;\n\n    con_cancer:::outline;\n    subgraph con_cancer[\"`**Condition on cancer**`\"]\n      gene2(\"Genetic&lt;br&gt;factor\"):::white;\n      env2(\"Environmental&lt;br&gt;factor\"):::white;\n      cancer2(\"Cancer\"):::black;\n      surgery2(\"Surgery\"):::white;\n    end\n\n    gene2 --&gt; cancer2;\n    env2 --&gt; cancer2;\n    cancer2 --&gt; surgery2;\n\n    none:::outline;\n    subgraph none[\"`**No conditioning**`\"]\n      gene(\"Genetic&lt;br&gt;factor\"):::white;\n      env(\"Environmental&lt;br&gt;factor\"):::white;\n      cancer(\"Cancer\"):::white;\n      surgery(\"Surgery\"):::white;\n    end\n\n    gene --&gt; cancer;\n    env --&gt; cancer;\n    cancer --&gt; surgery;\n\n    classDef white fill:#FFFFFF, stroke:#FFFFFF\n    classDef black fill:#FFFFFF, stroke:#000000\n    classDef outline fill:#FFFFFF;\n\n\n\n\n\n\nCollider bias may also be present when neither the exposure nor the outcome is a direct cause of the collider variable. An example is M-bias. In this example… * Focus: beta-blocker use and risk of ARDS * Might be tempted to adjust for crackles as you might think its a confounder… 1) heart failure leads to both chronic β-blocker therapy and crackles, and 2) pneumonia causes both ARDS and crackles * However, crackles is actually a collider on the back-door path of chronic β-blocker therapy ← heart failure → crackles ← pneumonia → ARDS. Adjusting for the presence of crackles opens this back-door path, introducing confounding. Ignoring the presence of crackles would be the right thing to do.[Lederer et al. 2018]\n\n\n\n\n\n  flowchart TD;\n\n    beta(\"Beta blocker use\"):::white;\n    ards(\"Acute respiratory distress syndrome (ARDS)\"):::white;\n    hf(\"Heart failure\"):::white;\n    pneu(\"Pneumonia\"):::white;\n    crackles(\"Crackles\"):::black;\n\n    hf --&gt; beta;\n    hf --&gt; crackles;\n    pneu --&gt; crackles;\n    pneu --&gt; ards;\n    beta --&gt;|?| ards;\n  \n    classDef white fill:#FFFFFF, stroke:#FFFFFF\n    classDef black fill:#FFFFFF, stroke:#000000\n\n\n\n\n\n\nAnother type of collider bias - specifically, a type of selection bias - is Berkson’s bias. This is when the selection of cases into the study depends on hospitalisation, and the treatment is another disease, or a cause of another disease, which also results in hospitalisation.[source]\n\n\n25.2.5 Selection nodes\nSelection nodes - by definition - are always conditioned on. This is because they reflect a restriction for inclusion such as: * Loss to follow-up - e.g. if some people lost to follow-up (C1) and some remain to end (C0), our analysis is restricted to C0. This means that only individuals with certain values of C are included in the analysis, as we’re essentially conditioning on it. * Inclusion/exclusion criteria for the study - e.g. if only include men, then gender –&gt; study enrollment\n\n\n25.2.6 Measurement error (mis-measured variables)\nMeasurement error is the degree to which we mismeasure a variable. If believe a variable is mismeasured, we have a node with a “” that points from variable, with another representing measurement error.[HarvardX PH559x] There are two types:  Non-differential error - if error is not in exposure or outcome - this will bias the estimate of effect towards the null (so for small effects or studies with little power, it can make a true effect disappear) * Differential error - if there is error in exposure and outcome - then, errors themselvse can be associated, opening a back-door path between exposure and outcome[source] - i.e. it is when measurement error varies in size depending on another variable. [Igelström et al. 2022]\nExample: Recall bias. Does taking multivitamins in childhood help protect against bladder cancer later in life? * Bias in outcome depends only on how well diagnosis of bladder cancer represents actually having it * Bias in exposure depends on both (a) memory of vitamin uptake, and (b) bladder cancer, since they might have spent more time reflecting on what could have caused the illness * If there is no effect of vitamins on bladder cancer, this dependency will make it seem as if vitamins are a risk for bladder cancer. If it is, in fact, protective, recall bias can reduce or even reverse the association.\n\n\n\n\n\n  flowchart TD;\n\n    me_diag(\"&lt;b&gt;Measurement error&lt;/b&gt;&lt;br&gt;in diagnosis\"):::white;\n    diag(\"Diagnosis of bladder cancer *\"):::white;\n    cancer(\"Bladder cancer\"):::white;\n    me_vit(\"&lt;b&gt;Measurement error&lt;/b&gt;&lt;br&gt;in vitamin uptake\"):::white;\n    mem_vit(\"Memory of&lt;br&gt;vitamin uptake *\"):::white;\n    vit(\"Childhood vitamin intake\"):::white;\n\n    me_diag --&gt; diag;\n    cancer --&gt; diag;\n    cancer --&gt; me_vit;\n    me_vit --&gt; mem_vit;\n    vit --&gt; mem_vit;\n\n    classDef white fill:#FFFFFF, stroke:#FFFFFF\n    classDef black fill:#FFFFFF, stroke:#000000\n    classDef outline fill:#FFFFFF\n    classDef green fill:#DDF2D1, stroke: #FFFFFF;",
    "crumbs": [
      "Causal concepts",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Directed acyclic graphs</span>"
    ]
  },
  {
    "objectID": "content/causal_concepts/3_dags.html#how-do-you-know-what-to-include-in-your-dag",
    "href": "content/causal_concepts/3_dags.html#how-do-you-know-what-to-include-in-your-dag",
    "title": "25  Directed acyclic graphs",
    "section": "25.3 How do you know what to include in your DAG?",
    "text": "25.3 How do you know what to include in your DAG?\n\n25.3.1 DAG completeness\nA DAG is said to represent a complete causal structure between a treatment and outcome if: * Treatment and outcome are presented * For any two nodes on the graph, all common causes of those two nodes are represented * All selection variables are represented (i.e. selection node) [Rogers et al. 2022]\nWhat do we mean by including common causes? Illustrating with an example… * In RCT where people were randomised to receive Aspirin, we don’t need to include other variables that can cause stroke (e.g. coronary heart disease (CHD)), as they didn’t cause why people got aspirin. * In an observational study, there will be other variables that would explain why people received aspirin (e.g. CHD), which we would need to include for it to be a causal DAG (i.e. aspirin AND stroke BOTH caused by CHD). [HarvardX PH559x]\nYou don’t need variables that cause Y but not A (although might include if for example you want to compare to other studies that did adjust for that variable).\n\n\n\n\n\n  flowchart TD;\n\n    ob:::outline;\n    subgraph ob[\"`**Observational**`\"]\n      asp2(\"Aspirin\"):::green;\n      str2(\"Stroke\"):::green;\n      chd2(\"Coronary heart disease (CHD)\"):::white;\n    end\n\n    asp2 --&gt; str2;\n    chd2 --&gt; asp2;\n    chd2 --&gt; str2;\n\n    rct:::outline;\n    subgraph rct[\"`**RCT**`\"]\n      asp(\"Aspirin\"):::green;\n      str(\"Stroke\"):::green;\n    end\n\n    asp --&gt; str;\n\n    classDef white fill:#FFFFFF, stroke:#FFFFFF\n    classDef black fill:#FFFFFF, stroke:#000000\n    classDef outline fill:#FFFFFF\n    classDef green fill:#DDF2D1, stroke: #FFFFFF;\n\n\n\n\n\n\n\n\n25.3.2 How do you know when to include mediators?\n\nIf we are interested in the total effect of A on Y, we don’t need to specify the mechanisms through which A may affect Y (i.e. don’t need any m ediators between A and Y)\nHowever, if we are interested in the direct effect of A on Y that doesn’t pass through the mediator, then we should include it. [HarvardX PH559x]\n\n\n\n25.3.3 When should you draw arrows?\nThe DAG is drawn based on expert knowledge, including arrows when you believe that something causes something else. If expert knowledge is insufficient for us to rule out a direct effect of E on D, then we should draw an arrow.\nArrows on causal graphs are not deterministic - i.e. doesn’t mean every person with exposure will see outcome - as some will never, and some without outcome will develop it. [HarvardX PH559x]\n\n\n25.3.4 Can you use variable selection methods?\nNo - it is important that these are based on expert knowledge. For causal inference, it is NOT recommended to choose included variables/relationships based on: * P value-based and model-based variable selection methods (including forward, backward, and stepwise selection) - since they ignore the causal structure underlying the hypothesis and treat confounders and colliders similarly * Use methods that rely on model fit or related constructs (e.g. R2, Akaike information criterion, and Bayesian information criterion) - since these rely heavily on the available data, in which causal relationships may or may not have been captured and may or may not be evident, and specification of model and arbitrary variables included will drive observed associations with outcome * Use selection of variables that, when included in a model, change the magnitude of the effect estimate of the exposure of interest, to identify confounders * Identify multiple ‘independent predictors’ through purposeful or automated variable selection\nIf the authors have hypotheses about each variable, then a separate model for each variable should be generated - or a prediction model could be developed, if prediction, rather than causal inference, is the goal of the analysis[Lederer et al. 2018]",
    "crumbs": [
      "Causal concepts",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Directed acyclic graphs</span>"
    ]
  },
  {
    "objectID": "content/causal_concepts/3_dags.html#how-do-you-create-a-dag",
    "href": "content/causal_concepts/3_dags.html#how-do-you-create-a-dag",
    "title": "25  Directed acyclic graphs",
    "section": "25.4 How do you create a DAG?",
    "text": "25.4 How do you create a DAG?\nFirst, start with your research question. This should be A (treatment/exposure) and Y (outcome), identificated by letters, “?” or colours.\nThen add the key components as detailed above… * Measured confounders (L) * Unmeasured confounders (U) * Selection nodes * Moderators * Mediators * Mismeasured variables\nEverytime you add a new node to the DAG, you need to conside whether it has common causes with any other variables (its not just about common causes of A and Y). For example, if you believe measurement of Y is affected by whether person is on treatment, draw arrow from A to measurement error for Y.\nThere can often be more than one appropriate DAG, and alternate DAGs can make excellent sensitivity analyses.[source] [HarvardX PH559x]\n\n\n\n\n\n  flowchart LR;\n\n    A(\"A (treatment/exposure)\"):::green;\n    Y(\"Y (outcome)\"):::green;\n    Empty[ ]:::empty;\n    Mod(\"Moderator\"):::white;\n    M(\"Mediator\"):::white;\n    L(\"L (confounder)\"):::white;\n    U(\"U (unmeasured confounder)\"):::white;\n    C(\"C (selection node)\"):::black;\n    Y*(\"Y* (mismeasured outcome)\"):::white;\n    UY(\"U&lt;sub&gt;Y&lt;/sub&gt; (measurement error for Y)\"):::white;\n\n    Mod --&gt; Empty;\n    A --- Empty; Empty --&gt;|?| Y;\n    A --&gt; M; M --&gt; Y;\n    L --&gt; A; L --&gt; Y;\n    U --&gt; L; U --&gt; Y;\n    A --&gt; C;\n    L --&gt; C;\n    Y --&gt; Y*;\n    UY --&gt; Y*;\n    A --&gt; UY;\n\n    classDef white fill:#FFFFFF, stroke:#FFFFFF;\n    classDef black fill:#FFFFFF, stroke:#000000;\n    classDef empty width:0px,height:0px;\n    classDef green fill:#DDF2D1, stroke: #FFFFFF;",
    "crumbs": [
      "Causal concepts",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Directed acyclic graphs</span>"
    ]
  },
  {
    "objectID": "content/causal_concepts/3_dags.html#paths",
    "href": "content/causal_concepts/3_dags.html#paths",
    "title": "25  Directed acyclic graphs",
    "section": "25.5 Paths",
    "text": "25.5 Paths\nA path is any route through graph - it doesn’t have to follow the direction of the arrows. Paths can be either be: * Open paths - represent statistical associations between two variables * E.g. If don’t condition on confounder, will be an open path, and see association with confounder * Blocked (or “closed” paths) - represent the absence of associations * E.g. An unconditioned collider should have no association [Williams et al. 2018]\n\n\n\n\n\n  flowchart TD;\n\n    block:::outline;\n    subgraph block[\"`**Path blocked at collider**`\"]\n      a(\"Treatment\"):::green;\n      y(\"Outcome\"):::green;\n      x(\"Collider\"):::white;\n    end\n\n    a --&gt; y;\n    a --&gt; x;\n    y --&gt; x;\n\n    open:::outline;\n    subgraph open[\"`**Path open at confounder**`\"]\n      a2(\"Treatment\"):::green;\n      y2(\"Outcome\"):::green;\n      x2(\"Confounder\"):::white;\n    end\n\n    a2 --&gt; y2;\n    x2 --&gt; a2;\n    x2 --&gt; y2;\n\n    classDef white fill:#FFFFFF, stroke:#FFFFFF\n    classDef black fill:#FFFFFF, stroke:#000000\n    classDef outline fill:#FFFFFF\n    classDef green fill:#DDF2D1, stroke: #FFFFFF;\n\n\n\n\n\n\nThe target (causal) paths are the directed paths from the exposure to the outcome which transmit the target effect. Biasing paths are non-directed open paths between the exposure and the outcome, which transmit bias for estimating the effect of the exposure on the outcome.[source]",
    "crumbs": [
      "Causal concepts",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Directed acyclic graphs</span>"
    ]
  },
  {
    "objectID": "content/causal_concepts/3_dags.html#d-seperation-rules",
    "href": "content/causal_concepts/3_dags.html#d-seperation-rules",
    "title": "25  Directed acyclic graphs",
    "section": "25.6 D-seperation rules",
    "text": "25.6 D-seperation rules\n\n25.6.1 Rules\nD-seperation rules are used to determine whether paths are open or blocked - i.e. whether variables will be associated or independent.\nIf all paths are blocked between two variables on the DAG, then they are d-seperated (i.e. not associated / statistical independence).[source] Otherwise, they are d-connected.[source]\nWhen identified here, these are structural sources of association. Another cause of association - beyond it being a causal relationship - is by chance. However, increasing our sample size, chance associations should disappear (whilst structural remain and become sharper). [HarvardX PH559x]\nRules: 1. If there are no variables being conditioned on * A path is only blocked if it contains a collider * A path is open if it does not contain a collider\n\nPath is blocked if it contains a non-collider that is conditioned on\nPath is open if collider is conditioned on\nPath is open if descendent of collider is conditioned on.[source]\n\n\n\n25.6.2 Examples of each rule\nRule 1: L to Y open\n\n\n\n\n\n  flowchart LR;\n\n    l(\"L\"):::white;\n    a(\"A\"):::white;\n    y(\"Y\"):::white;\n\n    l --&gt; a;\n    a --&gt; y;\n\n    classDef white fill:#FFFFFF, stroke:#FFFFFF\n    classDef black fill:#FFFFFF, stroke:#000000\n    classDef outline fill:#FFFFFF\n    classDef green fill:#DDF2D1, stroke: #FFFFFF;\n\n\n\n\n\n\nRule 1: L to A blocked\n\n\n\n\n\n  flowchart LR;\n\n    l(\"L\"):::white;\n    a(\"A\"):::white;\n    y(\"Y\"):::white;\n\n    l --&gt; y;\n    a --&gt; y;\n\n    classDef white fill:#FFFFFF, stroke:#FFFFFF\n    classDef black fill:#FFFFFF, stroke:#000000\n    classDef outline fill:#FFFFFF\n    classDef green fill:#DDF2D1, stroke: #FFFFFF;\n\n\n\n\n\n\nRule 2: A to Y blocked.\n\n\n\n\n\n  flowchart LR;\n\n    a(\"A\"):::white;\n    b(\"B\"):::black;\n    y(\"Y\"):::white;\n    \n    a --&gt; b; b--&gt; y;\n\n    classDef white fill:#FFFFFF, stroke:#FFFFFF\n    classDef black fill:#FFFFFF, stroke:#000000\n\n\n\n\n\n\nRule 3: A to Y open.\n\n\n\n\n\n  flowchart LR;\n\n    a(\"A\"):::white;\n    y(\"Y\"):::white;\n    l(\"L\"):::black;\n    \n    a --&gt; l; y--&gt; l;\n\n    classDef white fill:#FFFFFF, stroke:#FFFFFF\n    classDef black fill:#FFFFFF, stroke:#000000\n\n\n\n\n\n\nRule 4: A to Y open.\n\n\n\n\n\n  flowchart LR;\n\n    a(\"A\"):::white;\n    y(\"Y\"):::white;\n    l(\"L\"):::white\n    d(\"D\"):::black;\n    \n    a --&gt; l; y--&gt; l; l --&gt; d;\n\n    classDef white fill:#FFFFFF, stroke:#FFFFFF\n    classDef black fill:#FFFFFF, stroke:#000000",
    "crumbs": [
      "Causal concepts",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Directed acyclic graphs</span>"
    ]
  },
  {
    "objectID": "content/causal_concepts/3_dags.html#faithfulness",
    "href": "content/causal_concepts/3_dags.html#faithfulness",
    "title": "25  Directed acyclic graphs",
    "section": "25.7 Faithfulness",
    "text": "25.7 Faithfulness\nFaithfulness is the result of opposite effects of exactly equal magnitude - e.g. if aspirin caused stroke for half of poppulation and prevented it in the other half, then causal dag is correct (as aspirin affects stroke) but no association is observed (as they cancel each other out). In that case, we say the joint distribution of the data is not faithful to the causal DAG. These perfect cancellations are rare and we don’t expect them to happen in practice. [HarvardX PH559x]",
    "crumbs": [
      "Causal concepts",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Directed acyclic graphs</span>"
    ]
  },
  {
    "objectID": "content/causal_concepts/3_dags.html#time-varying-treatments-and-confounders",
    "href": "content/causal_concepts/3_dags.html#time-varying-treatments-and-confounders",
    "title": "25  Directed acyclic graphs",
    "section": "25.8 Time-varying treatments and confounders",
    "text": "25.8 Time-varying treatments and confounders\n\n25.8.1 What is a time-varying treatment?\nA time-varying treatment is treatment that can take different values over time - such as: * Whether or not receive medicine at each timepoint * Dose of medicine at each time point\nThis is as opposed to fixed treatments that do not vary over time (e.g. whether took vitamin D at time of conception).\nWe can represent this using two arbritary time points, K and K+1. Actual study includes many more weeks but for most purposes, two time points are enough to represent the main features of the causal structure when there is time varying treatment.\nYou’ll notice that a consequence of the time-varying treatment is time-varying confounder and outcome. A confounder is time-varying when it can take different values at different timepoint, and confound at different timepoints.\nExample: EPO used to treat anemia, dose is based on haemoglobin level at time of appointment (which itself depends on disease severity, but we’ve just represented that as a single timepoint). To show that we have time-varying components, we refer to: * L and A at timepoint K (e.g. week 0) * Y, L and A at timepoint K+1 (e.g. week 1) * Y at timepoint K+2 (e.g. week 2)\n\n\n\n\n\n  flowchart LR;\n\n    ak(\"A&lt;sub&gt;K&lt;/sub&gt;: EPO dose\"):::white;\n    ak1(\"A&lt;sub&gt;K+1&lt;/sub&gt;: EPO dose\"):::white;\n    yk1(\"Y&lt;sub&gt;K+1&lt;/sub&gt;: Death\"):::white;\n    yk2(\"Y&lt;sub&gt;K+2&lt;/sub&gt;: Death\"):::white;\n    lk(\"L&lt;sub&gt;K&lt;/sub&gt;: Haemoglobin\"):::white;\n    lk1(\"L&lt;sub&gt;K+1&lt;/sub&gt;: Haemoglobin\"):::white;\n    u(\"U: Disease severity\"):::white;\n\n    ak --&gt; yk1;\n    u --&gt; yk1;\n    u --&gt; lk;\n    lk --&gt; ak;\n    lk --&gt; ak1;\n    ak --&gt; ak1;\n    u --&gt; lk1;\n    lk1 --&gt; ak1;\n    ak1 --&gt; yk2;\n    u --&gt; yk2;\n\n    classDef white fill:#FFFFFF, stroke:#FFFFFF\n    classDef black fill:#FFFFFF, stroke:#000000\n\n\n\n\n\n\n\n\n25.8.2 Treatment-confounder feedback\nOur DAG above was incomplete - in this scenario, treatment at timepoint K will impact the confounder (levels of haemoglobin) at timepoint K+1. This is referred to as treatment-confounder feedback - when the later confounder is impacted by prior treatment.[HarvardX PH559x] This is also referred to as intermediate confounding - when a confoudner is affected by prior exposure/treatment status.[Igelström et al. 2022]\nWhen there is treatment-confounder feedback, then conventional adjustment methods, since: In other words: 1. Confounding on an intermediate confounder blocks part of the effect of prior exposure/treatment. 2. Conditioning on an intermediate confounder can introduce collider bias, opening additional back-door paths between exposure/treatment and outcome.[Igelström et al. 2022] * e.g. We’ll get a biased estimate if we condition on the Ls, as conditioning on LK+1 will open a path that was previously blocked: AK to LK+1 to U to YK. Hence, we have introduced selection bias.[HarvardX PH559x]\nThis means we will be unable to yield an unbiased estimate and, if the time-varying confounder also affects the outcome (e.g. LK+1 –&gt; YK+2), it will be impossible to estimate the total effect of the treatment.[HarvardX PH559x]\nWe need other methods to handle these settings: G-methods. [HarvardX PH559x]\n\n\n\n\n\n  flowchart LR;\n\n    ak(\"A&lt;sub&gt;K&lt;/sub&gt;: EPO dose\"):::white;\n    ak1(\"A&lt;sub&gt;K+1&lt;/sub&gt;: EPO dose\"):::white;\n    yk1(\"Y&lt;sub&gt;K+1&lt;/sub&gt;: Death\"):::white;\n    yk2(\"Y&lt;sub&gt;K+2&lt;/sub&gt;: Death\"):::white;\n    lk(\"L&lt;sub&gt;K&lt;/sub&gt;: Haemoglobin\"):::black;\n    lk1(\"L&lt;sub&gt;K+1&lt;/sub&gt;: Haemoglobin\"):::black;\n    u(\"U: Disease severity\"):::white;\n\n    ak --&gt; yk1;\n    u --&gt; yk1;\n    u --&gt; lk;\n    lk --&gt; ak;\n    lk --&gt; ak1;\n    ak --&gt; ak1;\n    ak --&gt; lk1;\n    u --&gt; lk1;\n    lk1 --&gt; ak1;\n    ak1 --&gt; yk2;\n    u --&gt; yk2;\n\n    classDef white fill:#FFFFFF, stroke:#FFFFFF\n    classDef black fill:#FFFFFF, stroke:#000000",
    "crumbs": [
      "Causal concepts",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Directed acyclic graphs</span>"
    ]
  },
  {
    "objectID": "content/causal_concepts/3_dags.html#presenting-results-from-causal-inference-studies",
    "href": "content/causal_concepts/3_dags.html#presenting-results-from-causal-inference-studies",
    "title": "25  Directed acyclic graphs",
    "section": "25.9 Presenting results from causal inference studies",
    "text": "25.9 Presenting results from causal inference studies\n‘Causal models are typically designed to test an association between a single exposure and an outcome. The additional independent variables in a model (often called “covariates”) serve to control for confounding. The observed associations between these covariates and the outcome have not been subject to the same approach to control of confounding as the exposure’ (i.e. they themselves have not been corrected for confounding - and they shouldn’t and didn’t have to be). ‘Therefore, residual confounding and other biases often heavily influence these associations.’\n‘This situation is known as “Table 2 fallacy,” a term arising from the practice of presenting effect estimates for all independent variables in “Table 2”.’ It is strongly recommended that these effect estimates are not presented.[Lederer et al. 2018]\nHartig 2019 discusses how this may not be practical in some fields (they give example of ecology) where you rarely have a clear target variable/hypothesis, but suggest instead that’s important to explicitly state/seperate reasonablly controlled varaibles from possibly confounded variables.[source]",
    "crumbs": [
      "Causal concepts",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Directed acyclic graphs</span>"
    ]
  },
  {
    "objectID": "content/causal_concepts/3_dags.html#minimal-set-of-covariates",
    "href": "content/causal_concepts/3_dags.html#minimal-set-of-covariates",
    "title": "25  Directed acyclic graphs",
    "section": "25.10 Minimal set of covariates",
    "text": "25.10 Minimal set of covariates\nWe want to identify a minimal set of covariates that: 1. Blocks all backdoor paths. 2. Doesn’t inadvertenly open closed pathways by conditioning on colliders or descendents. [source]\nWith the example below, the minimal set of variables you’d need to condition for would be L0 and L1 - wouldn’t need to for U as doing for L0 and L1 blocks the backdoor paths.[HarvardX PH559x]\n\n\n\n\n\n  flowchart LR;\n\n    ak(\"A&lt;sub&gt;K&lt;/sub&gt;: EPO dose\"):::white;\n    ak1(\"A&lt;sub&gt;K+1&lt;/sub&gt;: EPO dose\"):::white;\n    yk1(\"Y&lt;sub&gt;K+1&lt;/sub&gt;: Death\"):::white;\n    yk2(\"Y&lt;sub&gt;K+2&lt;/sub&gt;: Death\"):::white;\n    lk(\"L&lt;sub&gt;K&lt;/sub&gt;: Haemoglobin\"):::white;\n    lk1(\"L&lt;sub&gt;K+1&lt;/sub&gt;: Haemoglobin\"):::white;\n    u(\"U: Disease severity\"):::white;\n\n    ak --&gt; yk1;\n    u --&gt; yk1;\n    u --&gt; lk;\n    lk --&gt; ak;\n    lk --&gt; ak1;\n    ak --&gt; ak1;\n    u --&gt; lk1;\n    lk1 --&gt; ak1;\n    ak1 --&gt; yk2;\n    u --&gt; yk2;\n\n    classDef white fill:#FFFFFF, stroke:#FFFFFF\n    classDef black fill:#FFFFFF, stroke:#000000",
    "crumbs": [
      "Causal concepts",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Directed acyclic graphs</span>"
    ]
  },
  {
    "objectID": "content/causal_concepts/3_dags.html#example-cases",
    "href": "content/causal_concepts/3_dags.html#example-cases",
    "title": "25  Directed acyclic graphs",
    "section": "25.11 Example cases",
    "text": "25.11 Example cases\nThese examples help demonstrate the utility of causal diagrams.\n\n25.11.1 Example: Oestrogen and endometrial cancer\nIn 1970s, women began to receive oestrogren after menopause. Some studies in 1975/6 found that women receiving oestrogen had higher risk of diagnosis with endometrical cancer than women not receiving them. Why? Possibilities include… 1. Oestrogens cause cancer 2. Oestrogens can cause uterine bleeding, so women receive a uterine exam, during which the cancer (which is often silent, asymptomatic, and otherwise not diagnosed) is noticed and diagnosed - this phenomenon is called ascertainment bias\nHow do we decide which explanation is right?\n\nYale researchers restricted the data analysis to women with uterine bleeding (regardless of whether they were on oestrogens), since they should all have the same likelihood of uterine exams and existing cancer being diagnosed. If there still an association, oestrogen causative.\nBoston researchers argued we would find association even in women who bleed and even if they don’t cause cancer, and so that this approach would still have ascertainment bias.\n\nExplanation one.\n\n\n\n\n\n  flowchart LR;\n\n    a(\"A: Oestrogens\"):::white;\n    u(\"U: Cancer (unmeasured)\"):::white;\n    y(\"Y: Cancer (diagnosed)\"):::white;\n\n    a --&gt; u;\n    u --&gt; y;\n\n    classDef white fill:#FFFFFF, stroke:#FFFFFF\n    classDef black fill:#FFFFFF, stroke:#000000\n\n\n\n\n\n\nExplanation two.\n\n\n\n\n\n  flowchart LR;\n\n    a(\"A: Oestrogens\"):::white;\n    u(\"U: Cancer (unmeasured)\"):::white;\n    y(\"Y: Cancer (diagnosed)\"):::white;\n    c(\"C: Uterine bleeding\"):::white;\n\n    a --&gt; c;\n    u --&gt; c;\n    c --&gt; y;\n    u --&gt; y;\n\n    classDef white fill:#FFFFFF, stroke:#FFFFFF\n    classDef black fill:#FFFFFF, stroke:#000000\n\n\n\n\n\n\nRestrict analysis to women who bleed. If association still found, must be path of A –&gt; U –&gt; Y. Boston argued could still exist. * Conditioning on C blocks path A-C-Y * However, we still have path of A-C-U-Y, and C is collider on that path, so when condition on C, it becomes open (and conditioning on C is what we do when we restrict analysis to bleeders)\nSo Boston were right.\n\n\n\n\n\n  flowchart LR;\n\n    a(\"A: Oestrogens\"):::white;\n    u(\"U: Cancer (unmeasured)\"):::white;\n    y(\"Y: Cancer (diagnosed)\"):::white;\n    c(\"C: Uterine bleeding\"):::black;\n\n    a --&gt; c;\n    u --&gt; c;\n    c --&gt; y;\n    u --&gt; y;\n\n    classDef white fill:#FFFFFF, stroke:#FFFFFF\n    classDef black fill:#FFFFFF, stroke:#000000\n\n\n\n\n\n\nWhat can you do then? You can design study where C-Y doesn’t exist as you require all women to be screened for cancer frequently regardless of whether they bleed. If no association between A and Y, then we know there is no causal effect of A on U. If you still found association, then A must cause U.\n\n\n\n\n\n  flowchart LR;\n\n    a(\"A: Oestrogens\"):::white;\n    u(\"U: Cancer (unmeasured)\"):::white;\n    y(\"Y: Cancer (diagnosed)\"):::white;\n    c(\"C: Uterine bleeding\"):::white;\n\n    a --&gt; c;\n    u --&gt; c;\n    u --&gt; y;\n\n    classDef white fill:#FFFFFF, stroke:#FFFFFF\n    classDef black fill:#FFFFFF, stroke:#000000\n\n\n\n\n\n\n[HarvardX PH559x]\n\n\n25.11.2 Example: HIV and ART\nRandomised controlled trials of new antiretroviral therapy (ART) for HIV found it was effective and reduced morality by more than half.\nObservational of clinical data to look at real world effect of ART did not detect much benefit for new combination therapies - no increased survival among those taking ART. What was wrong with the studies?\nThey were adjusting for lots of confounders - e.g. CD4 count - and yet could not eliminate the bias. Some people say there must be lots of unmeasured confounding. However, the more time-varying confounders were adjusted for, the more biased the effect estimate seemed to be. The problem was treatment-confounder feedback - the value of CD4 count was impacted by earlier treatment - in this case, the bias was in the opposite direction.\nThere is a way to identify whether the bias is due to incomplete adjustment for confounding or for incorrect adjustment for time-varying confounders - and that is to use G-methods to adjust for the time-varying confounders. When they used G-methods, the effect estimates were much closer to the ARTs.\n\n\n\n\n\n  flowchart LR;\n\n    ak(\"A&lt;sub&gt;K&lt;/sub&gt;&lt;br&gt;ART\"):::white;\n    \n    lk(\"L&lt;sub&gt;K&lt;/sub&gt;&lt;br&gt;CD4 count\"):::black;\n    u(\"U&lt;br&gt;Immuno-suppression status\"):::white;\n    yk1(\"Y&lt;sub&gt;K+1&lt;/sub&gt;&lt;br&gt;Mortality\"):::white;\n    lk1(\"L&lt;sub&gt;K+1&lt;/sub&gt;&lt;br&gt;CD4 count\"):::black;\n    ak1(\"A&lt;sub&gt;K+1&lt;/sub&gt;&lt;br&gt;ART\"):::white;\n    yk2(\"Y&lt;sub&gt;K+2&lt;/sub&gt;&lt;br&gt;Mortality\"):::white;\n    \n    lk --&gt; ak;\n    u --&gt; lk; \n    u --&gt; yk1;\n    u --&gt; lk1;\n    lk1 --&gt; ak1;\n    u --&gt; yk2;\n    ak --&gt; lk1;\n    \n    classDef white fill:#FFFFFF, stroke:#FFFFFF\n    classDef black fill:#FFFFFF, stroke:#000000\n\n\n\n\n\n\n[HarvardX PH559x]",
    "crumbs": [
      "Causal concepts",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Directed acyclic graphs</span>"
    ]
  },
  {
    "objectID": "content/causal_concepts/4_assumptions.html",
    "href": "content/causal_concepts/4_assumptions.html",
    "title": "26  Assumptions",
    "section": "",
    "text": "26.1 Exchangeability assumption\n`````xbfumycfwrrm Executive summary :class: info\nCausal assumptions: * Exchangeability = groups are equivalent (as randomised / no confounders) * Stable Unit Treatment Value Assumption (SUTVA), which combines * Non-interference = treatment of one group cannot influence outcome of another (eg. violated for vaccinations) * Consistency = no hidden versions of treatment (eg. no undefined dose variation) * Positivity = no factors deterministic of treatment (eg. violated if treatment never prescribed with particular contraindication) * Ignorability = among people with same characteristics, can think of treatment as being randomly assigned\n`````\n‘Causal effects are impossible to measure directly, since they involve comparing unobserved counterfactual outcomes that would have happened under different circumstances. A causal effect is identifiable if it can be estimated using observable data, given certain assumptions about the data and the underlying causal relationships. Such identifying assumptions typically cannot be fully tested statistically but have to be justified based on theory and/or existing evidence about the real-world processes under study’.[Igelström et al. 2022]\nThese assumptions include: * Exchangeability assumption * Stable Unit Treatment Value Assumption (SUTVA), which combines * Non-interference assumption * Consistency assumption * Positivity assumption * Ignorability assumption\n‘The exchangeability (or “no confounding”) assumption requires that individuals who were exposed and unexposed have the same potential outcomes on average.’[Igelström et al. 2022] In other words, whatever treatment group each person was randomised too, we still would’ve seen the same outcomes in whichever were treated v.s. not, the groups are exchangeable, as no confounders are present.[HarvardX PH559x]\nThis allows the observed outcomes in an unexposed group to be used as a proxy for the counterfactual (unobservable) outcomes in an exposed group. RCTs strive to achieve exchangeability by randomly assigning the exposure, while observational studies often rely on achieving conditional exchangeability (or ‘no unmeasured confounding’), which means that exchangeability holds after conditioning on some set of variables’. [Igelström et al. 2022]",
    "crumbs": [
      "Causal concepts",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Assumptions</span>"
    ]
  },
  {
    "objectID": "content/causal_concepts/4_assumptions.html#stable-unit-treatment-value-assumption-sutva",
    "href": "content/causal_concepts/4_assumptions.html#stable-unit-treatment-value-assumption-sutva",
    "title": "26  Assumptions",
    "section": "26.2 Stable Unit Treatment Value Assumption (SUTVA)",
    "text": "26.2 Stable Unit Treatment Value Assumption (SUTVA)\nSUTVA is composed of two assumptions.\n(1) No interference between units (or “non-interference assumption”). This assumption requires that an individuals potential outcomes do ‘not depend on the exposure status of anyone else. This assumption can be violated by ‘spillover effects’ of some exposures (eg, vaccination), where an individual’s outcomes are affected by the exposure status of those around them. [Igelström et al. 2022]\n(2) There is only one version of treatment (or “consistency assumption”). This is also referred to as “no multiple versions of treatment” or “no hidden treatments”. This means that you do not have a scenario where each treatment condition has more than one version, and thus each unit may have more than one potential outcome per treatment condition. Hence, to satisfy, you need to: * Define each version as treatment, or * Restrict treatments to a subset of versions, or * Randomise versions and take average across versions, or * Redefine causal effect, acknowledgeing that estimated effect is conditional on an unknown distribution of versions. [Kimmell et al. 2021]\nIn practice, it can be impossible to achieve perfect consistency, and so the focus is instead on whether these differences are small enough for the averaged estimate to be meaningful.[Igelström et al. 2022]",
    "crumbs": [
      "Causal concepts",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Assumptions</span>"
    ]
  },
  {
    "objectID": "content/causal_concepts/4_assumptions.html#positivity-assumption",
    "href": "content/causal_concepts/4_assumptions.html#positivity-assumption",
    "title": "26  Assumptions",
    "section": "26.3 Positivity assumption",
    "text": "26.3 Positivity assumption\nPositivity means that, for every set of values for X, treatment assignment was not deterministic. If, for some values of X, treatment was deterministic, then we would have no observed values of Y for one of the treatment groups for those values of X. [Coursera]\n‘When conditioning on other variables, positivity needs to hold for each combination of covariates. This means that for every combination of covariates, it is possible to be either exposed or unexposed. The combination of covariates where this assumption holds can be called the ‘region of common support’.’\nViolations can be either: * Structural positivity violation - if some combinations are impossible (eg, if a treatment is never prescribed when a particular contraindication is present) * Random positivity violation - if combination is possible but is missing from the study sample by chance. The term ‘positivity’ may refer to both of these or only to structural positivity; the latter is usually more relevant in theoretical causal inference literature.’ [Igelström et al. 2022]",
    "crumbs": [
      "Causal concepts",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Assumptions</span>"
    ]
  },
  {
    "objectID": "content/causal_concepts/4_assumptions.html#ignorability-assumption",
    "href": "content/causal_concepts/4_assumptions.html#ignorability-assumption",
    "title": "26  Assumptions",
    "section": "26.4 Ignorability assumption",
    "text": "26.4 Ignorability assumption\nIgnorability means that, given pre-treatment covariates X, treatment assignment is independent from the potential outcomes - i.e. among people with the same values of X, we can think of treatment A being randomly assigned. This is sometimes referred to as the ‘no unmeasured confounders’ assumption.\nExample: Older people more likely to have treatment, and more likely to have outcome (hip fracture), so treatment assignment is not marginally independent from outcome - BUT within levels of age, treatment might be randomly assigned. [Coursera]",
    "crumbs": [
      "Causal concepts",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Assumptions</span>"
    ]
  },
  {
    "objectID": "content/causal_concepts/5_estimands.html",
    "href": "content/causal_concepts/5_estimands.html",
    "title": "27  Causal estimands",
    "section": "",
    "text": "27.1 Average causal effect\n`````najgmycvzwpm Executive summary :class: info\nPossible causal effect measures: * Causal mean difference * Causal mean ratio * Causal risk difference * Causal risk ratio\nPossible causal effects - i.e. causal estimand - choice of which can be guided by thinking of target trial you are trying to emulate: * Average treatment effect (ATE) * Average treatment effect in the treated (ATT) * Average treatment effect in the untreated (ATU/ATUT) * Intention-to-treat effect (ITT) * Complier average causal effect (CACE) or local average treatment effect\n`````\nIn causal inference studies, you are estimating the average causal effect of the treatment/exposure when comparing between groups of individuals. This is because it is generally impossible to estimate the causal effect for an individual, as you can’t go back in time and not give them an outcome.[source]\nThe terminology used varies, with Lederer et al. 2018 suggesting that we refer to finding causal associations and effect estimates - but not actual causal effects, or saying that the “exposure has an ‘effect’ or ‘impact’ on outcome”, or that the “exposure ‘protects against’ or ‘promotes’ outcome”. They suggest these are avoided without substantial evidence of a true causal effect.[Lederer et al. 2018]",
    "crumbs": [
      "Causal concepts",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Causal estimands</span>"
    ]
  },
  {
    "objectID": "content/causal_concepts/5_estimands.html#causal-effect-estimands",
    "href": "content/causal_concepts/5_estimands.html#causal-effect-estimands",
    "title": "27  Causal estimands",
    "section": "27.2 Causal effect estimands",
    "text": "27.2 Causal effect estimands\n\n27.2.1 Choosing a causal estimand\nBefore you estimate the causal effect, you have to choose which effect you are trying to estimate (i.e. the causal estimand).[Igelström et al. 2022] Things to consider…\nTarget population - different methods ‘allow you to estimate effects that can generalise to different target populations’ (e.g. difference between the various causal treatment effects below)\nWhether effect is marginal or conditional * Marginal - * Relevant to whole population * Involves comparing potential outcome under treatment to potential outcome under control (as in randomised trials) * Useful for finding overall effect * Conditional * Specific to certain population * Involves comparing potential outcomes within strata * Useful for finding treatment effect in particular subset of population\nThe outcome type (continuous, binary, or time-to-event).\nWhether the effect measure is non-collapsible or collapsible.[Greifer 2023] * Non-collapsible * This is when the conditional effect measure differs from the marginal effect measure even in the absence of confounding. This is true for certain non-linear effect measures like the odds ratio. * It means that conditional measures are more difficult to compare between studies (since different studies typically adjust for different sets of covariates), and that marginal effects may be less transportable between populations[Vansteelandt and Keiding 2011] * In these cases, it is very important to distinguish between marginal and conditional effects, as different methods target different types of effect * Collapsible * Same methods can be used to estimate marginal and conditional effects [Greifer 2023]\nIt can be challenging to identify the appropriate causal estimand. However, specifying a target trial (i.e. hypothetical RCT you are trying to emulate), can help with figuring this out. [Igelström et al. 2022]\n\n\n27.2.2 Effect measures\n\n\n\nEffect measure\nOutcome type\nCollapsiblility\nExample\n\n\n\n\nMean difference\nContinuous\nCollapsible\n‘An average increase in systolic blood pressure by 10 mmHg’\n\n\nRisk difference (RD)\nBinary\nCollapsible\n‘An average increase in the risk of stroke by 5%’\n\n\nRisk ratio (RR)\nBinary\nNon-collapsible\n‘An average increase in the risk of stroke by a factor of 1.5’\n\n\nOdds ratio (OR)\nBinary\nNon-collapsible\n-\n\n\nHazard ratio (HR)\nTime-to-event (i.e. survival)\nNon-collapsible [Greifer 2023]\n- [Igelström et al. 2022]\n\n\n\n\n\n27.2.3 Treatment effects\n\n\n\nCausal treatment effect\nDefinition\n\n\n\n\nAverage treatment effect (ATE)\nDifference between average outcome, when EVERYONE is exposed v.s. when NO-ONE is exposed\n\n\nAverage treatment effect in the treated (ATT)\nATE calculated only in sub-population of individuals who were actually exposed\n\n\nAverage treatment effect in the untreated (ATU/ATUT)\nATE calculated only in sub-population of individuals who were actually unexposed\n\n\nIntention-to-treat effect (ITT)\nAverage effect of being assigned to (but not necessarilly receiving) the exposure\n\n\nComplier average causal effect (CACE) or local average treatment effect\nATE calculated only among ‘compliers’ [Igelström et al. 2022]",
    "crumbs": [
      "Causal concepts",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Causal estimands</span>"
    ]
  },
  {
    "objectID": "content/causal_concepts/5_estimands.html#more-on-intention-to-treat",
    "href": "content/causal_concepts/5_estimands.html#more-on-intention-to-treat",
    "title": "27  Causal estimands",
    "section": "27.3 More on: Intention-to-treat",
    "text": "27.3 More on: Intention-to-treat\nIntention-to-treat analysis is the preferred analysis strategy for RCTs. This means that the analysis includes all participants, all retained in the group to which they were allocated.\nHowever, this can be hard to achieve due to: 1. Missing outcomes * A “complete case” (or “available case”) analysis only includes participants with no missing outcomes, and whilst only a few missing outcomes won’t cause a problem, in half of trials more than 10% of randomised patients may have missing outcomes. Hence, exclusion will reduce the sample size, and may introduce bias if loss to follow-up is related to a patient’s response to treatment. * Participants with missing outcomes can be included if their outcomes are imputed - but this requires strong assumptions. Common example is to use “last observation carried forward”, but this may introduce bias and makes no allowance for uncertainty imputation. 2. Non-adherence to protocol * Examples include participants who didn’t meet inclusion criteria (e.g. wrong diagnosis, too young), did not take all of the intended treatment, received a different treatment, or received no treatment * Intention-to-treat analysis ignores protocol deviations, including participants in their assigned groups regardless. Modified intention-to-treat (or ‘per protocol analysis’) is an analysis that excludes participants who didn’t adequately adhere (e.g. minimum amount of intervention) - but this would need to be labelled as a non-randomised, observational comparison, and be aware that the exclusion of patients compromises randomisation.[CONSORT]\nThese two problems can introduce non-random selection effects - i.e. randomisation isn’t the only cause for treatment - hence introducing confounding (bias), and meaning that exchangeability would no longer holder - and hence why intention-to-treat is recommended (i.e. ignore protocol deviations).\nThe Complier-Average Causal Effect (CACE) estimate is the comparison of the average outcome of the compliers in the treatment arm compared with the average outcome of the comparable group of would-be compliers in the control arm. It is the intention-to-treat effect in the sub-group of participants who would always have complied with their treatment allocation, and is not subject to confounding.[source]",
    "crumbs": [
      "Causal concepts",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Causal estimands</span>"
    ]
  },
  {
    "objectID": "content/causal_concepts/6_propensity_scores.html",
    "href": "content/causal_concepts/6_propensity_scores.html",
    "title": "28  Propensity scores",
    "section": "",
    "text": "`````kihqytozofta Executive summary :class: info\nPropensity scores are the probability of being in the treatment/exposure group, given your baseline characteristics.\n`````\nA propensity score is the ‘probability of treatment assignment conditional on observed baseline characteristics’. It was defined by was Rosenbaum and Rubin (1983). It is a ‘balancing score: conditional on the propensity score, the distribution of measured baseline covariates is similar between treated and untreated subjects’. [Austin 2011]\nPropensity scores are often estimated using a logistic regression model with: * Outcome = Treatment (e.g. insulin therapy) * Predictors = Observed baseline characteristics (e.g. blood pressure, BMI, lipid profile) * Propensity score = Predicted probability of treatment from the fitted model [Valojerdi et al. 2018]\nImage from Shaw Talebi on Towards Data Science:\n\n\n\nPropensity score\n\n\nUse of a propensity score enables incorporation of ‘a larger number of background covariates because it uses the covariates to estimate a single number’. [Valojerdi et al. 2018]\nFour different propensity scores methods are used for removing the effects of confounding: * Stratification on the propensity score * Propensity score matching * Inverse probability of treatment weighting (IPTW) using the propensity score * Covariate adjustment using the propensity score [Austin 2011]\nAssumptions of propensity score analysis/methods: * All covariates related to outcome and treatment (exposure) are measured and included * SUTVA - treatment effect for one individual is not affected by the treatment status of another * The assumptions of logistic regression [Valojerdi et al. 2018]",
    "crumbs": [
      "Causal concepts",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Propensity scores</span>"
    ]
  },
  {
    "objectID": "content/causal_concepts/7_target_trial.html",
    "href": "content/causal_concepts/7_target_trial.html",
    "title": "29  Target trial emulation",
    "section": "",
    "text": "29.1 Introduction\n`````jlpfsftivmkz Executive summary :class: info\nTarget trial emulation involves outlining your imaginary ideal RCT, and using that to inform study design, to help prevent avoidable biases like: * Immortal time bias - when you ignore variation in timing of treatment initiation * Lead time bias - when you detect a disease earlier than you otherwise would have in practice * Selection bias - when people in study systematically differ from population of interest\nIt also helps identify relevant causal questions and confounders.\nTo design your target trial protocol you should outline the: * Eligibility criteria * Treatment strategies * Treatment assignment * Outcomes * Causal estimand * Start and end of follow-up * Statistical analysis\nWhen emulating the target trial, you should ensure that the following are aligned at time zero: eligibility criteria met, treatment strategies assigned, follow-up started.\nThere are several study designs that attempt to minimise bias in observational research. These include…\nActive comparator (AC), new user (NU) (ACNU) design * AC: Compares between two active treatment strategies, as no treatment can mean no indication for treatment (e.g. mild) or contraindications (e.g. severe co-existing conditions) - avoiding bias from contraindications * NU: Follow from treatment initation, don’t include current users\nClone-censor-weight design * Clone (each patient starts on every treatment arm) and then censor (when no longer on arm) to reduce confounding at baseline and immortal-time bias * This introduces informative censoring, which then adjust for by weighting (using inverse-probability weights)\nSequential trial design * Series of artifical trials that divide at time points (e.g. study visits), and then within each trial, people are censored when their treatment deviates from what it was at the start of that artifical trial. Inverse probability weighting used to adjust for artifical censoring. * Can then estimate effect of sustained treatment v.s. no treatment, starting at each time point `````\n‘Target trial emulation is a framework for designing and analysing observational studies that aim to estimate the causal effect of interventions. For each causal question on an intervention, one can imagine the randomized trial (the “target trial”) that could have been conducted to answer that question. This target trial should be explicitly specified in a target trial protocol’. This then informs design of the observational study. [Fu 2023]\nTarget trial emulation is recommended as the standard approach for causal observational studies that investigate interventions. Reasons for this include…",
    "crumbs": [
      "Causal concepts",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Target trial emulation</span>"
    ]
  },
  {
    "objectID": "content/causal_concepts/7_target_trial.html#introduction",
    "href": "content/causal_concepts/7_target_trial.html#introduction",
    "title": "29  Target trial emulation",
    "section": "",
    "text": "29.1.1 Bias\nIt improves the quality of these observational studies by preventing avoidable biases. ’ Although many practitioners worry about confounding in observational studies, the effect of these “self-inflicted” biases is often much more severe.’ [Fu 2023]\nExamples: * Immortal time bias (ITB) - ‘occurs when there is variation in timing of treatment initiation from cohort entry and time-to-treatment is misclassified or ignored’, meaning that a cohort is followed during times in which outcomes cannot occur. [Agarwal et al. 2018]\nExample from [Egom 2014](http://dx.doi.org/10.1056/NEJMc1408400#SA1):\n\n\n\nImmortal Time Bias example\n\n\n\nLead time bias - ‘when a disease is detected by a screening or surveillance test at an earlier time point than it would have been if it had been diagnosed by its clinical appearance’ [Rollinson and Sabel 2007]\nExample by Mcstrother - Own work, CC BY 3.0:\n\n\n\n\nLead time bias example\n\n\n\nSelection bias - ‘occurs when individuals or groups in a study differ systematically from the population of interest leading to a systematic error in an association or outcome’ [Catalogue of Bias Collaboration]\n\nA recent review of observational studies found that: * 57% of observational studies suffer from immortal time bias * 44% suffer from depletion of susceptibles/prevalent user selection bias.’ [Fu 2023]\n\n\n29.1.2 Relevant causal questions\nTarget trial emulation ‘forces investigators to ask causal questions about interventions, leading to findings that are directly useful in decision-making’.\n\nExample: ’Many observational studies have investigated the causal effect of BMI on outcomes. BMI is not an intervention; patients cannot be randomized to have a certain BMI—a certain BMI can only be achieved through a particular intervention, such as diet, physical exercise, bariatric surgery, or medications (e.g., semaglutide or tirzepatide). These observational studies thus lose the vital information on how a patient attained a different BMI level. Each of the interventions may lower BMI by the same amount but may have completely different causal effects on the outcome. Therefore, the association between BMI and outcomes becomes an amalgamation of each of these interventions, which makes the association difficult to interpret.’\n\n‘The fact that the causal effect of biomarkers cannot be directly studied does not necessarily mean that the target trial emulation is restrictive—the investigator just needs to reformulate the question in terms of an intervention, just as has been performed to research biomarker targets in real randomized trials.’ [Fu 2023]\n\n\n29.1.3 Identify confounders\n\nExample: ’Suppose that an investigator is interested in estimating the causal effect of living donor kidney transplantation versus deceased donor kidney transplantation on graft and recipient survival. Which confounders should the investigator adjust for: donor characteristics, recipient characteristics, or both?\nWhen donor and recipient characteristics are imbalanced, the investigator may be inclined to adjust for both in the observational analysis. Fortunately, thinking about the target trial provides the solution. In a randomized trial, the investigator randomizes recipients to a kidney transplant from a living donor or a deceased donor. Consequently, the recipients in both groups have similar characteristics. However, living donors will not have characteristics similar to deceased donors in this randomized trial. The potential lower quality of kidneys from deceased donors is part of the treatment.\nThe observational analysis should therefore only adjust for recipient characteristics to emulate this randomization and not for donor characteristics.’ [Fu 2023]\n\n\n\n29.1.4 Guides required data and analysis\n‘The required data and statistical analysis logically flow from the specifications in the research question.’ [Fu 2023]",
    "crumbs": [
      "Causal concepts",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Target trial emulation</span>"
    ]
  },
  {
    "objectID": "content/causal_concepts/7_target_trial.html#how-to-design-your-target-trial-protocol",
    "href": "content/causal_concepts/7_target_trial.html#how-to-design-your-target-trial-protocol",
    "title": "29  Target trial emulation",
    "section": "29.2 How to design your target trial protocol",
    "text": "29.2 How to design your target trial protocol\nYou should include the following protocol elements. This table is adapted from [Fu 2023]. The example is ‘for an observational study aiming to estimate the causal effect of renin-angiotensin system inhibitors versus calcium channel blockers on outcomes in patients with advanced CKD’.\n\n\n\n\n\n\n\n\n\n\nProtocol element\nDescription\nTarget Trial\nSubsequent observational study\nComments\n\n\n\n\nEligibility criteria\nWho will be included in this study?\n- Age 18+- Under nephrologist care- CKD G4 (i.e., eGFR &lt;30 ml/min per 1.73 m2)- No history of kidney transplantation- No use of RASi or CCB in previous 180 d between January 2007 and December 2016\nSame as target trial\nObservational study could be tempted to include all individuals on treatment or with outcome in follow-up - but this would be incorrect, as eligibility criteria determine who is enrolled in a trial, and information from follow-up could never determine eligibilityWill often need to compromise in observational study if can’t get all data required to determine eligibility\n\n\nTreatment strategies\nWhich precise treatment strategies or interventions will eligible individuals receive?\n1. Initiate RASi (ACEi or ARB) only2. Initiate CCB only\nSame as target trial\nIn practice, more likely “initiate RASi only and always use during follow-up” - should capture that nauance. Important to be specific as guides follow-up and analysis, including whether need to adjust for time-varying confounding\n\n\nTreatment assignment\nHow will eligible individuals be assigned to the treatment strategies?\nRandomization, no blinding\nEligible individuals are assigned at baseline to the treatment strategy that their data are consistent with. To emulate randomization, we adjust for the following baseline confounders: age, sex, eGFR, systolic and diastolic blood pressure, medical history (heart failure, arrhythmia, peripheral vascular disease, cerebrovascular disease, ischemic heart disease, diabetes mellitus, hyperkalemia, AKI), medication use (β-blocker, thiazide diuretic, potassium-sparing diuretic, statin), and health care use (the total number of hospitalizations in previous year)\nIndividuals will be randomly assigned to one of the treatment strategies in the target trialAppropriate emulation of randomization requires sufficient adjustment for all baseline confounders, which need to be measured before treatment assignment. The difficulty is to obtain enough data on confounders to remove residual confounding.\n\n\nOutcomes\nWhat outcomes will be measured during follow-up?\n1. Kidney replacement therapy (dialysis or kidney transplantation) 2. All-cause mortality 3. Major adverse cardiovascular events (composite of cardiovascular death, nonfatal myocardial infarction, nonfatal stroke)\nSame as target trial. Kidney replacement therapy is registered in the Swedish renal registry; all-cause/cardiovascular mortality is identified from the Swedish death registry; hospitalizations for myocardial infarction or stroke are identified through ICD-10 codes in the national patient registry\nOutcome data may often be missingIn observational data, outcomes usuallly not assessed blindly and systematically, so this can lead to bias (e.g. if you get more measurements when you’re sicker - which you can addresss by comparing number of measurements - but not by restricting to patients with certain number of measurements, since randomised trial wouldn’t have known that at baseline, and this would lead to selection bias)\n\n\nCausal estimand\nWhich causal estimand will be estimated with the observational data?\nIntention-to-treat effect (analyse in randomised groups regardless of whether complete or switch treatment)Per protocol effect (only analyses people who strictly adhered to protocol - and excludes people who didn’t complete or switched treatment)\nPer protocol effect (effect of receiving treatment strategy as specified in protocol)\nRandomised trials are commonly effect of being randomised (intention-to-treat effect) and effect of receiving treatment as per protocol (per protocol effect). Observational studies are not randomised so you can only estimate per protocol effects, never intention-to-treat (despire investigators often using that term)\n\n\nStart and end of follow-up\nWhen does follow-up start and when does it end?\nStarts at randomization and ends at occurrence of end point, administrative censoring or 5 yr of follow-up\nStarts at medication initiation (filled prescription) and ends at occurrence of end point, administrative censoring or 5 yr of follow-up\nTarget trial starts at randomization and finishes at reaching an end point, administrative censoring, or 5 years of follow-upObservational starts when (1) patient eligible and (2) patient data congruent with start of treatment. There isn’t clear time for when “do not initiate” treatment begins - solution is to analyse question in sequential trials, which uses idea that patients in “do not initiate treatment” group can be allocated to strategy at any point in time when are eligible. When interested in patients who “always use during follow-up”, would need to censor (stop follow-up) when discontinue assigned treatment\n\n\nStatistical analysis\nWhich statistical analyses will be used to estimate the causal estimand?\nIntention-to-treat analysis, non-naïve per protocol analysis\nPer protocol analysis: Hazard ratios are estimated using Cox regression while adjusting for baseline confounders with inverse probability of treatment weighting. Weighted cumulative incidence curves are estimated using the Aalen–Johansen estimatora\nIncludes methods to adjust for confounding, how missing data are dealt with, and which methods were used to obtain effect estimates\n\n\n\n[Fu 2023]",
    "crumbs": [
      "Causal concepts",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Target trial emulation</span>"
    ]
  },
  {
    "objectID": "content/causal_concepts/7_target_trial.html#how-to-use-this-to-inform-your-observational-study",
    "href": "content/causal_concepts/7_target_trial.html#how-to-use-this-to-inform-your-observational-study",
    "title": "29  Target trial emulation",
    "section": "29.3 How to use this to inform your observational study",
    "text": "29.3 How to use this to inform your observational study\n’To ensure that the target trial protocol properly emulates the design of a randomized trial, it is a key to align the following three components at time zero (often also referred to as baseline) in the observational study: 1. Eligibility criteria are met, that is, all included patients meet the specified inclusion and exclusion criteria. 2. Treatment strategies are assigned. 3. Follow-up is started, that is, we start counting outcomes.\nNote that these three components are naturally aligned in randomized trials at the moment of randomization.’\nObservational study design options: * Active comparator, new user (ACNU) design * Clone censor weight design * Sequential trial design [Fu 2023]\n\n29.3.1 Active comparator, new user (ACNU) design\nUsed to compare effect of initiating two treatments [Fu 2023]\nDesigned to ‘emulate the intervention part of a RCT’, comparing two cohorts of drug users.\n\nActive comparator (AC) design: restrict ‘study to individuals with an indication for treatment and without contraindications’ [Lund et al. 2015]\n\nCompares with active comparator (another drug/treatment) rather than non-active comparator (no active treatment… non-users). Patients in that group - who have disease but are not on active treatment - can include those with no indication for treatment (e.g. mild disease), or people for whom all treatment is contraindicated (e.g. if have severe co-existing conditions) - and as such, are often not included in RCTs\nAvoids bias from contraindications: In practice, ‘physicians carefully choose who should or should not be treated with the drug of interest , causing imbalance between treatment groups in the baseline level of risk for the outcome of interest in observational studies—that is, confounding by indication’\n‘The active-comparator design has three main advantages: increased similarity in measured patient characteristics between treatment groups; reduced potential for unmeasured confounding; and possibly improving the clinical relevance of the research question’ [Yoshida et al. 2015]\n\nNew user (NU) design: align ‘individuals at a uniform point in time to start follow-up (i.e., treatment initiation) and ensuring the correct temporality between covariate and exposure assessment’ [Lund et al. 2015]\n\nAlso known as incident-user design or initator design\nThis ‘includes a cohort of patients initiating treatment with a drug of interest who are followed up from treatment initiation, similar to RCTs. By contrast, the prevalent-user design includes both current and new users of a drug of interest within the study period, and follow-up thus starts at a different time point in the course of each individual’s treatment’\n‘The new-user design has three main advantages: time-varying hazards and drug effects associated with treatment duration can be assessed; appropriate adjustment for confounding is ensured by capturing pretreatment variables; and potential for immortal time bias is reduced when this design is combined with the active-comparator design’ [Yoshida et al. 2015]\n\n\nIllustration of ACNU and confounding by indication from Sendor and Stürmer 2022: \n\n\n29.3.2 Clone-censor-weight design\nUseful for grace periods, treatment duration, or when treatment is started based on a biomarker level [Fu 2023]\nThe approach involves three steps: 1. ‘“Clone” each patient once for each treatment regimen of interest.’ [source] * ‘Cloning patients allows us to assign patients to both arms for the duration for which treatment allocation is unknown. At baseline, in our illustration, we assumed that all patients were equally likely to be offered surgery or not. As such, all patients entered both arms of the trial, independently of their subsequent surgery status. Thus, we created two clones of each patient with one clone allocated to each study arm, hence doubling the size of our dataset. The study arms are therefore identical with respect to demographics and clinical characteristics at the time of diagnosis. This removes confounding bias, at baseline only.’ [Maring et al. 2020] 2. ‘“Censor” each clone when their person-time is no longer consistent with the corresponding treatment regimen.’ [source] 3. ‘“Weight” the remaining person-time by the inverse probability of being censored.’ [source] * ‘If the decision to perform surgery was completely random or made based on patients’ characteristics that were not associated with the outcome, the artificial censoring done… would be ignorable, and would not bias the results. However, in most observational studies, treatment decision is based on characteristics also associated with the outcome, i.e. the confounders. In our example, the decision to perform surgery is associated with age, performance status and comorbidity index, which are also associated with survival. In such situations, the artificial censoring introduces selection bias… The proposed approach to address this problem is to use inverse-probability-of-censoring weighting (IPCW)’ [Maring et al. 2020]\nExplanation of how this addresses bias: ‘When the start of follow-up and the time of treatment initiation do not coincide i.e. when the exposure (or treatment) status is not defined at the inclusion within the study, immortal-time bias is a concern if the study groups are defined based on the observed treatment allocation. Indeed, treatment receipt at a given time t is conditional on having survived up to time t, and consequently, treatment receipt is more likely to be observed among patients with longer survival. We illustrated how cloning patients at the start of follow-up, carefully defining the survival time and vital status for each clone, and choosing the length of the grace period, as proposed by Hernàn et al.,9,10 can address both confounding and immortal-time biases. However, by cloning and censoring the patients to account for confounding at baseline and immortal-time bias, we introduce an informative censoring, which does not exist in the original dataset. This artificial censoring can be adjusted for by inverse-probability weights in the statistical analysis, which is the main complexity of this approach’ [Maring et al. 2020]\nIllustration of design from Maring et al. 2020:\n\n\n\nClone-censor-weight design\n\n\n\n\n29.3.3 Sequential trial design\nAppropriate when one group starts with treatment and other does not [Fu 2023]\nThis design was first described by Hernan et al. in 2008 and Gran et al. in 2010. It ‘was originally proposed as a simple way of making efficient use of longitudinal observational data, as it enables use of a larger sample size than if an artificial trial were formed from a single time origin’. However, it is also an approach that can be used as an alternative to inverse probability weighted estimation of marginal structural models (MSM-IPTW)\nIn this design: * Artifical “trials” are created from a sequence of new time origins (e.g. ‘study visits at which new information recorded for each individual who remains under observation’). * ‘At each time origin individuals are divided into those who have just initiated the treatment under investigation and those who have not yet initiated the treatment.’ * ‘Within each trial, individuals are artificially censored at the time at which their treatment status deviates from what it was at the time origin, if such deviation occurs.’ * ‘Inverse probability of censoring weighting is used to account for dependence of this artificial censoring on time-dependent characteristics’ * ’ The overall effects of sustained treatment versus no treatment, starting from each time origin, can then be estimated using, for example, weighted pooled logistic or Cox regression’ [Keogh 2023]",
    "crumbs": [
      "Causal concepts",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Target trial emulation</span>"
    ]
  },
  {
    "objectID": "content/causal_methods/summary.html",
    "href": "content/causal_methods/summary.html",
    "title": "30  Overview",
    "section": "",
    "text": "30.1 Identifying the focus of the research\nThis page describes how to design a causal inference study, and provides a brief overview of the possible methods you could use within your study.\nThe gold standard method for inferring causality is randomisation - e.g. randomising patients to receive a treatment or not. This is because it removes confounding - it removes the common cause of the treatment and outcome, since the only cause of treatment was randomisation.[HarvardX PH559x] When we are using observational data, there are a variety of possible methods for causal effect estimation.\nThe first step will be to clearly specify your research question. You can do so using the counterfactuals approach, directed acyclic graphs, or structural equation models. When you do this, you need to identify a single exposure and outcome, for which you want to estimate a causal effect. The exposure is a variable that can take one of several counterfactual values. It will often be a treatment or intervention.[Igelström et al. 2022]\nYou cannot be focussed on the relationship of several different variables with the outcome! This is because we design our study around identifying the true causal relationship between those two variables, but any other variables included in modelling or so on will still be vulnerable to residual confounding or biases, as we haven’t designed the study around them. The reporting of the relationships of those other variables with the outcome is known as Table 2 fallacy.[Lederer et al. 2018]",
    "crumbs": [
      "Causal methods",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "content/causal_methods/summary.html#methods-for-causal-effect-estimation-using-observational-data",
    "href": "content/causal_methods/summary.html#methods-for-causal-effect-estimation-using-observational-data",
    "title": "30  Overview",
    "section": "30.2 Methods for causal effect estimation using observational data",
    "text": "30.2 Methods for causal effect estimation using observational data\nAll of these methods are attempting to achieve one thing: the careful control of confounders in such a way that you have more confidene that the association between two variables is likely to be true.\nMethods for causal inference include:\n:::::{grid} 1 1 2 2 :class-container: text-center :gutter: 3\n\n:::{card} Multivariable regression\nInclusion of confounders as covariates in the model.\nAlso known as covariate adjustment.\n\n::::\n\n:::{card} Stratification\nSeperate participants based on their values of the confounder.\nAlso known as principal stratification.\n\n::::\n\n:::{card} Matching\nCreate same distribution of confounders in treated and untreated groups.\n\n::::\n\n:::{card} Inverse probability of treatment weighting (IPTW)\nWeight each individual by the inverse of their probability of receiving their actual treatment, resulting in equal distribution of confounders in the treated and untreated groups.\nAlso known as inverse probability weighting (IPW) and propensity score weighting.\n\n::::\n\n:::{card} Marginal structure models (MSM)\nWeight observations using IPTW, then use weights when estimate association between treatment and outcome\nAlso known as MSM with weights estimated using IPTW, and IPTW with time-varying covariates\n\n::::\n\n:::{card} G-computation\nPredict outcomes in counterfactual populations that assume all had treatment or not, and compare.\nAlso known as parametric G-formula, G-standardisation, standardisation or outcome regression.\n\n::::\n\n:::{card} G-estimation\nWork back in time, predicting counterfactual outcome (with parameters estimated using G-estimation) at each time point, given no exposure from that time point onwards, controlling for treatment and confounders prior to that point\nMostly refers to G-estimation of structural nested models (SNM).\n\n::::\n\n:::{card} Instrumental variables (IV)\nIV cause variation in treatment, but are unrelated to outcome and therefore unrelated to unmeasured confounders. Randomisation is an example of an IV. You can then use two-stage least squares to estimate causal effect (regress exposure on instrumental variable to get estimate of exposure independent of confounders, then regress outcome on that estimate).\n\n::::\n\n:::{card} Regression discontinuity design (RDD)\nCan use RDD when exposure status is determined by a continuous variable exceeding an arbitrary threshold. We look at individuals who fall just above and below the threshold. We expect the relationship between that variable and the outcome to be continuous (i.e. they’re a similar group with similar values of variable, who would’ve otherwise expect to have similar outcomes if no exposure/treatment was triggered). Therefore, any discontinuity/jump in that relationship indicates a causal relationship with the exposure. “If you see a turtle on a fencepost, you know he didn’t get there by himself”.\n\n::::\n\n:::{card} Interrupted time series (ITS)\nCompare change over time in a continuous population-level outcome before and after an exposure is introduced. Assuming the trend would have been unchanged without the exposure, a change in trend at point of introduction can be attributed to the exposure\n\n::::\n\n:::{card} Difference in differences (DiD)\nCompare change over time in a continuous population-level outcome between a group that becomes exposed/treated, and a group that does not\n\n::::\n:::::",
    "crumbs": [
      "Causal methods",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "content/causal_methods/summary.html#comparing-methods",
    "href": "content/causal_methods/summary.html#comparing-methods",
    "title": "30  Overview",
    "section": "30.3 Comparing methods",
    "text": "30.3 Comparing methods\nWhen choosing methods, there are a few different things to consider.\n\n30.3.1 Treatment-confounder feedback and unobserved confounding\nMethods are often grouped into the following three categories - with choice of method depending on whether you have treatment-confounder feedback or unobserved confounding.[Igelström et al. 2022]\n\n\n\nGroup\nMethods                          \nComments\n\n\n\n\nConventional methods - most common methods which focus on conditioning on some set of common causes of the exposure and outcome[Hernán and Robins 2024]\n• Multivariable regression• Stratification• Matching• Inverse probability of treatment weighting [Igelström et al. 2022]\n• Generally only work in simpler settings - can’t handle time-varying treatments (such as is the case for complex longitudinal data).[Hernán and Robins 2024] - except for inverse probability of treatment weighting (see marginal structural models)• Propensity score matching is recommended over stratification or multivariable regression as it eliminates a greater proportion of systemic differences in baseline characteristics between treated and untreated. [Wijn et al. 2022]\n\n\nG-methods (“generalised”) address intermediate confounding i.e. treatment-confounder feedback\n• G-computation• Marginal structure models• G-estimation [Igelström et al. 2022]\n• Developed by James Robins[source]• Deal with intermediate confounding by ’taking the observed distribution of intermediate confounders (in the population as well as over time) into account, instead of holding them constant; in other words, they estimate marginal effects.[Igelström et al. 2022]\n\n\nMethods that address unobserved confounding\n• Instrumental variables• Regression discontinuity• Interrupted time series• Difference in differences [Igelström et al. 2022]\n• These methods work ‘by exploiting some assignment mechanism (akin to randomisation in an RCT) that determines exposure status but is thought to be unrelated to any unobserved confounders.’ [Igelström et al. 2022]•‘Although G-computation and G-estimation methods have advantages over marginal structural models (e.g. more flexible modeling of time-varying effect modification and robustness to situations that would threaten the positivity assumption of IPTW-based procedures) they remain under-used; partly for limited implementation in standard statistical packages’ [Williamson and Ravani 2017]\n\n\n\nIt is important that you do not adjust for time-varying confounders using conventional methods. ‘Adjusting for time-dependent confounders using conventional methods, such as time-dependent Cox regression, often fails in these circumstances, as adjusting for time-dependent confounders affected by past exposure (i.e. in the role of mediator) may inappropriately block the effect of the past exposure on the outcome (i.e. overadjustment bias). For example, we wish to determine the effect of blood pressure measured over time (as our time-varying exposure) on the risk of end-stage kidney disease (ESKD) (outcome of interest), adjusted for eGFR measured over time (time-dependent confounder). As eGFR acts as both a mediator in the pathway between previous blood pressure measurement and ESKD risk, as well as a true time-dependent confounder in the association between blood pressure and ESKD, simply adding eGFR to the model will both correct for the confounding effect of eGFR as well as bias the effect of blood pressure on ESKD risk (i.e. inappropriately block the effect of previous blood pressure measurements on ESKD risk).’ [Chesnaye et al. 2022]\n\n\n30.3.2 Assumptions\nDifferent methods have different causal assumptions - see the page on assumptions for a description of these, and see individual method sections for the assumptions of that method.\nThis example compares assupmtions of some of the conventional methods, and is taken from [Shiba and Kawahara 2021].\n\n\n\nAssumptions\n\n\nThe conventional and G-methods assume these is no unmeasured confounding (i.e. conditional exchangeability). However, this is often not plausible in observational study designs - hence the benefit of the methods that address unmeasured confounding.[Igelström et al. 2022]\n\n\n30.3.3 Causal estimand & conditional/marginal effects\nIt is important when choosing a method to have a clear causal estimand - considerations include: * Target population * Whether estimating conditional or marginal effect * Outcome type * Whether the effect measure is non-collapsible or collapsible * Refer to the page on causal estimands for more details.\nWith regards to conditional (relevant to whole population) and marginal effects (specific to certain population): * Estimate conditional effects: Stratification, regression [Vansteelandt and Keiding 2011] * Estimate marginal effects: G-methods [Igelström et al. 2022]",
    "crumbs": [
      "Causal methods",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "content/causal_methods/summary.html#what-methods-are-typically-used",
    "href": "content/causal_methods/summary.html#what-methods-are-typically-used",
    "title": "30  Overview",
    "section": "30.4 What methods are typically used?",
    "text": "30.4 What methods are typically used?\nWijn et al. 2022 conducted a mapping literature review to determine which confounding adjustment methods were used in longitudinal observational data to estimate a treatment effect. They identified the following studies:\n\n\n\nWijn et al 2022 studies\n\n\nAbbreviations: CA, covariate adjustment; IPW, inverse probability weighting; PS, propensity score; PSM, propensity score matching; TdPSM, time-dependent propensity score matching.\nYou can see that in the context of longitudinal observational data * 66% time-varying treatment, 26% treatment at baseline, 8% time of treatment not clearly defined * For treatment at baseline, majority of papers use propensity score matching with baseline covariates (82%) * For time-varying treatment: * 30% inverse probability weighting * 25% propensity score matching with baseline coavariates * 14% propensity score matching with baseline covariates combined with time-dependent Cox regression * 10% covariate adjustment using propensity score * 8% time-dependent propensity score * 4% parametric G-formula * 2% propensity score stratification * 3% G-estimation * Hence, for time-varying treatment, often inappropriate methods are use - 25% used probability score matching with baseline covariates ‘which can potentially result in a biased treatment effect’ - and only 45% of the papers used g-methods. [Wijn et al. 2022]\nOther reviews include - * 2019 systematic review of studies adjusting for time-dependent confounding, finds inverse probability of treatment weighting estimated marginal structure models to be the most common technique [Clare et al. 2019]",
    "crumbs": [
      "Causal methods",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "content/causal_methods/summary.html#reporting-guidelines",
    "href": "content/causal_methods/summary.html#reporting-guidelines",
    "title": "30  Overview",
    "section": "30.5 Reporting guidelines",
    "text": "30.5 Reporting guidelines\nThere are currently guidelines in development…\nPrediction of Counterfactuals Guideline (PRECOG): ‘While there are guidelines for reporting on observational studies (eg, Strengthening the Reporting of Observational Studies in Epidemiology, Reporting of Studies Conducted Using Observational Routinely Collected Health Data Statement), estimation of causal effects from both observational data and randomised experiments (eg, A Guideline for Reporting Mediation Analyses of Randomised Trials and Observational Studies, Consolidated Standards of Reporting Trials, PATH) and on prediction modelling (eg, Transparent Reporting of a multivariable prediction model for Individual Prognosis or Diagnosis), none is purposely made for deriving and validating models from observational data to predict counterfactuals for individuals on one or more possible interventions, on the basis of given (or inferred) causal structures.’. A 2022 protocol proposes to draw up reporting guidelines for such studies. [Xu et al. 2022]\nTrAnsparent ReportinG of observational studies Emulating a Target trial (TARGET) guideline: ‘Observational studies are increasingly used to inform health decision-making when randomised trials are not feasible, ethical or timely. The target trial approach provides a framework to help minimise common biases in observational studies that aim to estimate the causal effect of interventions. Incomplete reporting of studies using the target trial framework limits the ability for clinicians, researchers, patients and other decision-makers to appraise, synthesise and interpret findings to inform clinical and public health practice and policy. This paper describes the methods that we will use to develop the TrAnsparent ReportinG of observational studies Emulating a Target trial (TARGET) reporting guideline.’ [Hansford et al. 2023]\nRelated: Systematic review demonstrating inconsistencies in reporting of target trials - [Hansford et al. 2023]",
    "crumbs": [
      "Causal methods",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "content/causal_methods/summary.html#resources",
    "href": "content/causal_methods/summary.html#resources",
    "title": "30  Overview",
    "section": "30.6 Resources",
    "text": "30.6 Resources\nLots of different resources were used in this section, but I wanted to highlight a few of the key online textbooks/websites that were great: * Causal Inference: What If - By Miguell A. Hernan and James M. Robins * The Effect: An Introduction to Research Design and Causality by Nick Huntington-Klein * Causal Inference: The Mixtape by Scott Cunningham * STA 640: Causal Inference - Fan Li - Department of Statistical Science, Duke University * Causal Inference for The Brave and True",
    "crumbs": [
      "Causal methods",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "content/causal_methods/a1_multivariable_regression.html",
    "href": "content/causal_methods/a1_multivariable_regression.html",
    "title": "31  Multivariable regression",
    "section": "",
    "text": "31.1 All possible confounders\nAlso referred to as covariate adjustment\nThe conventional multivariable regression approach - where confounders are included as covariates - can be used to estimate causal effects, and can be referred to as “selection on observables”. This requires you meet certain assumptions however - namely, “Do we have data on the minimum number of variables needed to satisfy the backdoor path criterion?”. [source] This is described in this section of this book, but to recap, we want to identify a minimal set of covariates that: 1. Block all backdoor paths - so would want to block the open back door path that is in the image on the left 2. Do not inadvertenly open closed pathways by conditioning on colliders or descendents - as is the case in the image on the right (not conditoning on the collider, so the path remains blocked - but if you did, it would open it)\nAlike the methods above, we assume conditional exchangeability given measured convariates - i.e. only addresses confounding caused by measured covaries and can be biases by unmeasured covariates.[Shiba and Kawahara 2021]\nShiba and Kawahara 2021 explain the benefits, however, of propensity score methods (propensity score matching, inverse probability weighting) over multivariable regression: * Modeling decisions for propensity score methods typically come before looking at data (hence minimises p-hacking and tweaking of spec to align with expectations) * ‘Potential positivity violations tend to become more visible in the propensity score methods as extreme propensity score values can signal covariate pattenrs in which only the exposed or the unexposed are present’ * For rare outcomes, conditioning on lots of covariates in a regression can produce imprecise estimates - whereas propensity score methods convert lots of covariates into a single value * Modelling assumptions (don’t fully understand… it’s around misspecification, knowledge of relationships with exposure or outcome, doubly-robust methods, conditional and marginal effect estimates) * Inverse probability weighting can be expanded to account for time-varying confounding [Shiba and Kawahara 2021]\nConfounders can be included as individual covariaties, or by just including the estimated propensity score as a covariate in the regression model. Propensity score: * Can be attractive as it allows the incorporation of many covariates * Should be used with caution, as ‘bias may increase when the variance in the treated and untreated groups are very different (actually, the untreated group variance is much larger than the treated groups variance).’ [Valojerdi et al. 2018]\nIn the context of an intervention effect (i.e. the treatment paradox), Schuit et al. 2013 recommend inclusion of the intervention in the predictor model as a solution - as we do for multivariable regression. They note that ‘if an intervention is equally effective in all patients, modelling the intervention effect doesn’t require an interaction between predictor and intervention in the model. If the intervention is more effective in, for example, those having the predictor, then an interaction between intervention and predictor is required’.[Schuit et al. 2013]\n‘Many analysts take the strategy of putting in all possible confounders. This can be bad news, because adjusting for colliders and mediators can introduce bias, as we’ll discuss shortly. Instead, we’ll look at minimally sufficient adjustment sets: sets of covariates that, when adjusted for, block all back-door paths, but include no more or no less than necessary. That means there can be many minimally sufficient sets, and if you remove even one variable from a given set, a back-door path will open.’[source]\nSchisterman et al. 2009 ‘define unnecessary adjustment as control for a variable that does not affect bias of the causal relation between exposure and outcome but may affect its precision.’ - so: * Adjusting for a variable completely outside the system of interest (C1) * Adjusting for a variable that causes the exposure only (C2) * Adjusting for a variable whose only causal association with variables of interest is as a descendent of the exposure and not in the causal pathway (C3) * Adjusting for a variable whose only causal association with variables of interest is as a cause of the outcome (C4)\nAdjusting for these varaibles should not impact the total causal effect on the outcome, but may be gain or loss in precision of relationship between exposure of interest, the unnecessary adjustment variables, and the outcome of interest.[Schisterman et al. 2009]\nflowchart LR;\n\n    C1:::white\n    C2:::white\n    C3:::white\n    C4:::white\n    E(Exposure):::white\n    D(Outcome):::white\n\n    C2 --&gt; E;\n    E --&gt; C3;\n    E --&gt; D;\n    C4 --&gt; D;\n\n    classDef white fill:#FFFFFF, stroke:#FFFFFF",
    "crumbs": [
      "Causal methods",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Multivariable regression</span>"
    ]
  },
  {
    "objectID": "content/causal_methods/a1_multivariable_regression.html#data-driven-selection-of-confounders",
    "href": "content/causal_methods/a1_multivariable_regression.html#data-driven-selection-of-confounders",
    "title": "31  Multivariable regression",
    "section": "31.2 Data-driven selection of confounders",
    "text": "31.2 Data-driven selection of confounders\n‘Though generally not advisable, data-driven confounder selection may be employed in small datasets, under the condition that the data has been pre-processed to entail that covariates fed into the statistical selection method are only potential confounders and free of mediators’.[Ramspek et al. 2021 (supplementary)]",
    "crumbs": [
      "Causal methods",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Multivariable regression</span>"
    ]
  },
  {
    "objectID": "content/causal_methods/a2_stratification.html",
    "href": "content/causal_methods/a2_stratification.html",
    "title": "32  Stratification",
    "section": "",
    "text": "32.1 Simple example\nStratification or principal stratification is the simplest method to control confounding.[Tripepi et al. 2010] It is represented by drawing a box on the DAG.[HarvardX PH559x]\nIt was proposed by Frangakis and Rubin 2002.[source] It has gained popularity since the ICH E9 addendum on statistical principles for clinical trials, which listed it as a valid approach to ICEs.[source] The use of stratification to adjust for confounding is so common that some investigators consider the terms ‘stratification’ and ‘adjustment’ synonymous. Whilst it can be used to adjust for confounding - but it can also be used to identify effect modification.[Hernán and Robins 2024]\nPrincipal stratification involves partitioning participants into principal strata - i.e. particular values of a variable. ‘Stratification necessarily results in multiple stratum-specific effect measures (one per stratum defined by the variables L). Each of them quantifies the average causal effect in a nonoverlapping subset of the population but, in general, none of them quantifies the average causal effect in the entire population.’ Instead, they are conditional effect measures. [Hernán and Robins 2024]\n‘Often, one of the principal strata is the focus of inference, but sometimes it is of interest to combine principal effects across several (or all) principal strata while accounting for a confounding effect of a post-randomization variable.’[Lipkovich et al. 2022] Hence, stratification involves either: * Restricting analysis to subset of study population with particular value of confounder.[HarvardX PH559x] This type of stratification is referred to as restriction. When positivity fails for some strata of the population (i.e. impossible to get a certain exposure), restriction is used to limit causal inference to the strata where it does hold. [Hernán and Robins 2024] * Performing analysis in each stratum of confounder.[HarvardX PH559x] For causal inference, stratification is simply applies restriction to several mutually exclusive subsets of the population, with exchangeability within each subset. [Hernán and Robins 2024]\nflowchart LR;\n\n    order(\"Birth order\"):::white;\n    down(\"Down syndrome\"):::white;\n    age(\"Maternal age\"):::black;\n\n    order --&gt;|?| down;\n    age --&gt; order;\n    age --&gt; down;\n\n    classDef white fill:#FFFFFF, stroke:#FFFFFF\n    classDef black fill:#FFFFFF, stroke:#000000\nSee figure: * (a) Association of down syndrome with birth order and age groups seperately * (b) Down syndrome cases stratified by birth order and maternal age\nCan observe that crude association between birth order and Down syndrome was just due to maternal age (as in each age category, birth order did not affect down syndrome frequency, but in each birth order category, age did). [Tripepi et al. 2010]",
    "crumbs": [
      "Causal methods",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Stratification</span>"
    ]
  },
  {
    "objectID": "content/causal_methods/a2_stratification.html#simple-example",
    "href": "content/causal_methods/a2_stratification.html#simple-example",
    "title": "32  Stratification",
    "section": "",
    "text": "Exposure: Birth order\nOutcome: Down syndrome\nPotential confounder: Maternal age\n\n\n\n\n\n\n\nTripepi et al. 2010 Figure 1",
    "crumbs": [
      "Causal methods",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Stratification</span>"
    ]
  },
  {
    "objectID": "content/causal_methods/a2_stratification.html#mantel-haenszel-formula",
    "href": "content/causal_methods/a2_stratification.html#mantel-haenszel-formula",
    "title": "32  Stratification",
    "section": "32.2 Mantel-Haenszel Formula",
    "text": "32.2 Mantel-Haenszel Formula\nThe Mantel-Haenszel formula can be used to provide a pooled odds ratio across different strata. There are alternative methods (e.g. Woolf and inverse variance) but the Mantel-Haenszel method is generally the most robust.[source]\nKey steps: 1. Calculate crude relative risks (RR) or odds ratio (OR) (i.e. without stratifying) 2. Stratify by confounding variable and calculate stratum-specific RR or OR 3. Assess whether effect estimates are roughly homogenous across strata and do not differ from that in the whole group * If they are homogeneous, this means there is no confounding, and you can calculate the overall adjusted RR or OR by the Mantel-Haenszel formula. The pooling estimate provides an average of the stratum-specific RRs or ORs with weights proportional to the number of individuals in each stratum. * If they are heterogeneous and we are interested in effect modification, stratum-specific effect estimates should be reported separately. [Tripepi et al. 2010]",
    "crumbs": [
      "Causal methods",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Stratification</span>"
    ]
  },
  {
    "objectID": "content/causal_methods/a2_stratification.html#stratification-on-the-propensity-score",
    "href": "content/causal_methods/a2_stratification.html#stratification-on-the-propensity-score",
    "title": "32  Stratification",
    "section": "32.3 Stratification on the propensity score",
    "text": "32.3 Stratification on the propensity score\nYou can stratify subjects based on their propensity scores. ‘The literature showed that five strata are adequate to reduce at least 90% of the bias associated with a confounding variable. With a large sample size, we can use between 10 or 20 strata.’ [Valojerdi et al. 2018]",
    "crumbs": [
      "Causal methods",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Stratification</span>"
    ]
  },
  {
    "objectID": "content/causal_methods/a2_stratification.html#limitations-of-stratification",
    "href": "content/causal_methods/a2_stratification.html#limitations-of-stratification",
    "title": "32  Stratification",
    "section": "32.4 Limitations of stratification",
    "text": "32.4 Limitations of stratification\n\nComputes conditional effect measures (not average effect measures)\nRequires computation of effect measures in subsets of population defined by combining all variables required for conditional exchangeability[Hernán and Robins 2024]\n\nThis can be laborious and demands a large sample size when there is more than one confounder.[Tripepi et al. 2010] It is therefore ‘prone to sparse-data problem (it occurs when there are few or no study participants at some combinations of the outcome, exposure, and covariates) and unstable estimates’, and as such, ‘is rarely used in practice’. [Gharibzadeh et al. 2016]\nThis is even if we’re not interested in such effect modification. Solution: Stratification by something of interest (i.e. effect modifier) followed by IP weighting or standardisation (to adjust for confounding) allows you to deal with exchangeability (confounders) and effect modification (modifiers)\n\nNoncollapsibility of certain effect measures like the odds ratio [Hernán and Robins 2024] - i.e. ‘the crude OR from the marginal table cannot be expressed as the weighted average of the stratum-specific ORs even in the absence of confounding’ - as ‘the magnitude of the OR is different when comparing the aggregate analysis to the subgroup analysis’ - but both estimates are still valid. Other effect measures are collapsible e.g. stratified risk ratios [Pang et al. 2013]\nRequires continuous confounders to be constrained to a limited number of categories, which could generate residual confounding [Tripepi et al. 2010]",
    "crumbs": [
      "Causal methods",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Stratification</span>"
    ]
  },
  {
    "objectID": "content/causal_methods/a3_matching.html",
    "href": "content/causal_methods/a3_matching.html",
    "title": "33  Matching",
    "section": "",
    "text": "33.1 Matching methods\nMatching involves selecting a sample where exposed and unexposed groups have the same distribution of confounders.[HarvardX PH559x] We often start with the group with fewer individuals, and then use the other group to find matches. It does not have to be one-to-one (matching pairs) - it can be one-to-many (matching sets). Matching is often based on a combination of confounders. [Hernán and Robins 2024]\nMatching can’t be represented in DAG, because non-faithfulness - the association to a backdoor path is exactly cancelled by the matched subset.[HarvardX PH559x]\nWe make an assumption of conditional exchangeability given L (the confounder), meaning that matching results in ‘(unconditional) exchangeability of the treated and untreated in the matched population’, and so we directly compare their outcomes. Matching ensures positivity since strata with only treated or untreated individuals are excluded. [Hernán and Robins 2024]\nAbove describes individual matching, but you can also use frequency matching. For example, randomly selected individuals but ensuring 70% have L=1 (certain value of confounder), and then repeating for the other population. [Hernán and Robins 2024]\nThere are a few approaches to matching, which include: * Propensity score matching - matched based on propensity scores * This is commonly one-to-one matching based on similar values of the propensity score, which can be done with or without replacement, but with replacement can decrease bias and is helpful where the numbers of controls are limited. [Valojerdi et al. 2018] * Selecting the propensity score ‘close’ to the treated subject is done using either nearest neighbour matching or nearest neighbour matching within a specific caliper distance. * You can choose between greed matching or optimal matching * Outcomes… * ‘If the outcome is continuous (e.g., a depression scale), the effect of treatment can be estimated as the difference between the mean outcome for treated subjects and the mean outcome for untreated subjects in the matched sample’ * ‘If the outcome is dichotomous (self-report of the presence or absence of depression), the effect of treatment can be estimated as the difference between the proportion of subjects experiencing the event in each of the two groups (treated vs. untreated) in the matched sample. With binary outcomes, the effect of treatment can also be described using the relative risk or the NNT.’ * ‘Once the effect of treatment has been estimated in the propensity score matched sample, the variance of the estimated treatment effect and its statistical significance can be estimated.’ [Austin 2011] * Matched difference-in-differences - perform matching then compute difference-in-differences - this controls for unobserved, time-invariant characteristics between the groups * Synthetic control method - weight one group in a manner to it closely resembles the other group Above, we are describing the synthetic control method. [source]",
    "crumbs": [
      "Causal methods",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Matching</span>"
    ]
  },
  {
    "objectID": "content/causal_methods/a3_matching.html#limitations",
    "href": "content/causal_methods/a3_matching.html#limitations",
    "title": "33  Matching",
    "section": "33.2 Limitations",
    "text": "33.2 Limitations\n\nRequires extensive datasets to properly match, with detailed information on baseline characteristics, but this is not always available\nAssumes there are no unobserved characteristics between the matched groups. Possible solution: Matched difference-in-differences. [source]\nComputes conditional effect measures (not average effect measures) - i.e. only for certain subset of population [Hernán and Robins 2024]",
    "crumbs": [
      "Causal methods",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Matching</span>"
    ]
  },
  {
    "objectID": "content/causal_methods/a4_iptw.html",
    "href": "content/causal_methods/a4_iptw.html",
    "title": "34  Inverse probability of treatment weighting (IPTW) with baseline covariates",
    "section": "",
    "text": "34.1 Example\nNames: * Inverse probability weighting (IPW) * Inverse probability of treatment weighting (IPTW) * Propensity score weighting\nIPTW is one of the various propensity score methods. Below describes IPTW to account for confounders at baseline, but it can be used to: * Estimate parameters of a marginal structure model and adjust for confounding measured over time (see page on G-methods). * Account for informative censoring (pateitsn censored when lost to follow-up or reach study end without encountering event) - calculating inverse probability of censoring weights or each time point as the inverse probability of remaining in the study up to the current time point, given the previous exposure, and patient characteristics related to censoring [Chesnaye et al. 2022]\nIPTW involves using the propensity scores to balance the baseline characteristics in the treated and untreated (or ‘exposed and unexposed’) groups. This is done by weighting each individual by the inverse probability of receiving their actual treatment. [Chesnaye et al. 2022]\nThis means that larger weights go to: * Exposed individuals with a lower probability of exposure * Unexposed individuals with a higher probability of exposure\nConceptually, IPTW can be considered mathematically equivalent to standardisation.\nAs IPTW aims to balance patient characteristics in the exposed and unexposed groups, it is considered good practice to assess the standardized differences between groups for all baseline characteristics both before and after weighting. The advantage of checking standardized mean differences is that it allows for comparisons of balance across variables measured in different units. As a rule of thumb, a standardized difference of &lt;10% may be considered a negligible imbalance between groups. If the standardized differences remain too large after weighting, the propensity model should be revisited (e.g. by including interaction terms, transformations, splines). [Chesnaye et al. 2022]\n‘As the weighting creates a pseudopopulation containing ‘replications’ of individuals, the sample size is artificially inflated and correlation is induced within each individual. This lack of independence needs to be accounted for in order to correctly estimate the variance and confidence intervals in the effect estimates, which can be achieved by using either a robust ‘sandwich’ variance estimator or bootstrap-based methods.’ [Chesnaye et al. 2022]\nImage from Chesnaye et al. 2022:",
    "crumbs": [
      "Causal methods",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Inverse probability of treatment weighting (IPTW) with baseline covariates</span>"
    ]
  },
  {
    "objectID": "content/causal_methods/a4_iptw.html#example",
    "href": "content/causal_methods/a4_iptw.html#example",
    "title": "34  Inverse probability of treatment weighting (IPTW) with baseline covariates",
    "section": "",
    "text": "Study on receiving extended-hours haemodialysis (EHD) treatment v.s. conventional HD (CHD)\nWant to balance distribution of diabetes - at baseline, higher % diabetes patients in CHD group than in EHD group\nPatients with diabetes have 25% probability of receiving EHD = propensity score of 0.25\nTo balance distribution, weight-up patients in EHD group by inverse of propensity score, which is 1/0.25=4, so conceptually each EHD diabetes patient represents four patients, creating a “pseudo-population”\nSimilarly, CHD diabetes patients weighted by 1/(1-0.25)=1.33.\nDiabetes now equally distributed across the EHD and CHD groups\nThese weights can then be incorporated into an outcome model to get an estimate of the average treatment effect adjusted for confounders\n\n\n\n\n\n\n\n\n\n\n\nDiabetes + Group\nEstimated probabilityof EHD (givendiabetes status)\nEstimated probabilityof actual treatmentreceived\nInverse\n\n\n\n\nDiabetes + CHD\n0.25\n0.75\n1/0.75 = 1.33\n\n\nNo diabetes + CHD\n0.75\n0.25\n1/0.25 = 4\n\n\nDiabetes + EHD\n0.25\n0.25\n1/0.25 = 4\n\n\nNo diabetes + EHD\n0.75\n0.75\n1/0.75 = 1.33\n\n\n\n\n\n\n\nChesnaye et al. 2022 propensity score diagram",
    "crumbs": [
      "Causal methods",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Inverse probability of treatment weighting (IPTW) with baseline covariates</span>"
    ]
  },
  {
    "objectID": "content/causal_methods/a4_iptw.html#strengths-and-limitations",
    "href": "content/causal_methods/a4_iptw.html#strengths-and-limitations",
    "title": "34  Inverse probability of treatment weighting (IPTW) with baseline covariates",
    "section": "34.2 Strengths and limitations",
    "text": "34.2 Strengths and limitations\nStrengths: * (As for all propensity score-based methods) able to summarise all characteristics in a single covariate * Retains more individuals than propensity score matching * Can be used for categorical or continuous exposures (whilst matching generally compares a single treatment group with a control group) * Estimates hazard ratios with less bias than propensity score stratification or adjustment using the propensity score * Can be used in marginal structural models to correct for time-dependent confounders\nLimitations: * Simulation studies have shown IPTW can be no better than multivariable regression * IPTW cautioned against for sample sizes under 150 due to underestimation of the variance (i.e. standard error, confidence interval and P-values) of effect estimates * ‘Sensitive to misspecifications of the propensity score model, as omission of interaction effects or misspecification of functional forms of included covariates may induce imbalanced groups, biasing the effect estimate’ [Chesnaye et al. 2022] * ‘Propensity values near 0 and 1 yield extreme weights (after taking the inverse)’[source] - i.e. not recommended when propensities are small (close to 0) as weights can be unstable * ATE may not always be the sensible estimand [source]",
    "crumbs": [
      "Causal methods",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Inverse probability of treatment weighting (IPTW) with baseline covariates</span>"
    ]
  },
  {
    "objectID": "content/causal_methods/a4_iptw.html#assumptions",
    "href": "content/causal_methods/a4_iptw.html#assumptions",
    "title": "34  Inverse probability of treatment weighting (IPTW) with baseline covariates",
    "section": "34.3 Assumptions",
    "text": "34.3 Assumptions\n‘Treatment effects obtained using IPTW may be interpreted as causal under the following assumptions: * Exchangeability * No misspecification of the propensity score model * Positivity * Consistency’ [Chesnaye et al. 2022]",
    "crumbs": [
      "Causal methods",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Inverse probability of treatment weighting (IPTW) with baseline covariates</span>"
    ]
  },
  {
    "objectID": "content/causal_methods/a4_iptw.html#iptw-variants",
    "href": "content/causal_methods/a4_iptw.html#iptw-variants",
    "title": "34  Inverse probability of treatment weighting (IPTW) with baseline covariates",
    "section": "34.4 IPTW variants",
    "text": "34.4 IPTW variants\nOne of the limitations described above is that propensity values near 0 and 1 yield extreme weights. This can ‘inflate the variance and confident intervals of the effect estimate. This may occur when the exposure is rare in a small subset of individuals, which subsequently receives very large weights, and thus have a disproportionate influence on the analysis.’ It’s worth considering why these individuals have such a low probability of being treatment but actually receive the treatment, or vice versa - they may be outliers. For example, ‘patients with a 100% probbaility of receiving a particular treatment would not be eligible to be randomised to both treatments’. There are variants to IPTW that attempt to address some of its limitations. [Chesnaye et al. 2022]\n\n34.4.1 Weight stabilisation\n\nReplace ‘numerator (which is 1 in the unstabilized weights) with the crude probability of exposure (i.e. given by the propensity score model without covariates).’\n‘In case of a binary exposure, the numerator is simply the proportion of patients who were exposed. Stablised weights can therefore be calculated as:’\n\nexposed / propensity_score, instead of 1 / propensity_score\nunexposed / (1-propensity score), instead of 1 / (1-propensity score)\n\n‘Stabilized weights should be preferred over unstabilized weights, as they tend to reduce the variance of the effect estimate’ [Chesnaye et al. 2022]\n\nIn other weights, weight stabilisation invovles multiplying the unstabilised weight by the probability of the observed exposure without conditioning on the confounders, and so it the stablised weight is the ‘ratio of the unconditional probability (the numerator) to the conditional probability (the denominator)’.[Xie et al. 2017]\n\n\n34.4.2 Propensity score trimming and truncation\n\nSymmetric trimming - exclude patients with a score outside [a, 1-a], with often a=0.1, so [0.1, 0.9] [source] - typically 1st and 99th percentiles, but lower thresholds can reduce variance [Chesnaye et al. 2022]\nAsymmetric trimming - exclude patients with a score outside the common range formed by the treated and control patients, and below the q quantile of treated and above the 1-q quantile of control\nPropensity score truncation - set patients with a score below a to a, and above 1-a to 1-a [source]\n\n‘Truncating weights change the population of inference and thus this reduction in variance comes at the cost of increasing bias’. [Chesnaye et al. 2022]\n\n\n34.4.3 Augmented inverse propensity weighting (AIPW)\nAugmented inverse propensity weighting (AIPW) involves: 1. Fitting a propensity score model (i.e. estimated probability of treatment assignment conditional on baseline characteristics) 2. Fit two seperate models that estimate the outcome - one under treatment and one under control 3. Weight each outcome by the propensity scores\nThis improves on IPW to combine information about the probability of treatment and predictive information about the outcome variable. It is a doubly robust estimator / has the property of double robustnesss. This means ‘that it is consistent (i.e., it converges in probability to the true value of the parameter) for the ATE if either the propensity score model or the outcome model is correctly specified’.\nAIPW is more flexible as it doesn’t require the same set of covariates to be used for the propensity score model and model estimaing treatment-outcome relationship. [Kurz 2022]\n\n\n34.4.4 Excluding treated individuals\nPajouheshnia et al. 2017 propose to exclude treated individuals after IPTW - i.e. stratifying the sample but focussing only on untreated - so that the resulting validation set resembles the untreated target population. [Pajouheshnia et al. 2017] This is proposed in the context of the treatment paradox, with the intention of finding the direct effect of the exposure on outcome not mediated by treatment.\nIn their simulation study, Pajouheshnia et al. 2017 find that IPW alone did not improve calibration (compared to when we did nothing to account for the treatment paradox), but IPW followed by the exclusion of treated individuals provided correct estimates for calibration. IPW alone or followed by the exclusion of treated individuals improved estimates of the c-index in all scenarios where the assumptions of positivity and no unobserved confounding were met. In scenario 4, where treatment allocation was determined by a strict risk-threshold and thus the assumption of positivity was violated, IPW was ineffective, and resulted in the worst estimates of discrimination across all methods. In addition, the extreme weights calculated in scenario 4 led to very large standard errors. In scenarios 13–15, the presence of an unobserved confounder led to the failure of IPW to provide correct estimates of the c-index. Weight truncation at the 98% percentile increased precision, but was less effective in correcting of the c-index for the effects of treatment.\n‘Although the use of IPW prior to the exclusion of treated individuals is a promising solution in data where treatments are non-randomly allocated, it should not be used when there are severe violations of the underlying assumptions, e.g. in the presence of non-positivity (where some individuals had no chance of receiving treatment), or when there is an unobserved confounder, strongly associated with both the outcome and treatment use. There is thus a need to explore alternative methods to IPW to account for the effects of treatment use when validating a prognostic model in settings with non-random treatment use.’ [Pajouheshnia et al. 2017]",
    "crumbs": [
      "Causal methods",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Inverse probability of treatment weighting (IPTW) with baseline covariates</span>"
    ]
  },
  {
    "objectID": "content/causal_methods/b1_marginal_structural_models.html",
    "href": "content/causal_methods/b1_marginal_structural_models.html",
    "title": "35  Marginal structural models (MSM)",
    "section": "",
    "text": "35.1 Method\nAlternative names (often depending on method used to estimate weights): * Inverse probability-weighted marginal structural models [Naimi et a. 2017] * IPTW-based marginal structural models [Williamson and Ravani 2017] * Inverse probability of treatment weighting with time-varying covariates [Chesnaye et al. 2022] * Marginal structural Cox proportional hazards model (if you were using a Cox model) [Xie et al. 2017]\nIn marginal structural models, each observation is weighted, with weights most commonly estimated using IPTW.[Williamson and Ravani 2017]\n‘Results of marginal structural models have similar interpretation as clinical trials (i.e. a marginal or population-level interpretation). Marginal structural models estimate what would happen if a person always received a certain treatment versus never, which is an idealized situation that does not reflect clinical practice, unless it is interpreted as an ‘intention to continue treatment’ similar to the ‘intention to treat’ interpretation of randomized controlled trials. Other methods that address time-varying confounding affected by previous treatment allow different types of inference based on a conditional, as opposed to marginal interpretation. For example, the sequential Cox approach estimates the effect of starting a treatment versus never, and ignoring previous treatment.’ [Williamson and Ravani 2017]",
    "crumbs": [
      "Causal methods",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Marginal structural models (MSM)</span>"
    ]
  },
  {
    "objectID": "content/causal_methods/b1_marginal_structural_models.html#method",
    "href": "content/causal_methods/b1_marginal_structural_models.html#method",
    "title": "35  Marginal structural models (MSM)",
    "section": "",
    "text": "35.1.1 Step 1. Weight observations using IPTW\n\nInverse probability of treatment weighting (IPTW) can be used to ‘estimate the parameters of a marginal structural model.’\n‘Unlike the procedure followed for baseline confounders, which calculates a single weight to account for baseline characteristics, a separate weight is calculated for each measurement at each time point individually. To achieve this, the weights are calculated at each time point as the inverse probability of being exposed, given the previous exposure status, the previous values of the time-dependent confounder and the baseline confounders. This creates a pseudopopulation in which covariate balance between groups is achieved over time and ensures that the exposure status is no longer affected by previous exposure nor confounders.’\nExtreme weights can be dealt with as described for IPTW with baseline covariates, but for weight stablisation, the numerator would be probability of being exposed given previous exposure status and baseline confounders. ‘Although including baseline confounders in the numerator may help stabilize the weights, they are not necessarily required. If the choice is made to include baseline confounders in the numerator, they should also be included in the outcome model’. [Chesnaye et al. 2022]\nCensoring weights can also be estimated and included.\nCreate the pseudo-population with confounding removed by multiplying each observation by its individual weights. [Williamson and Ravani 2017]\n\n\n\n35.1.2 Step 2. Use weights in model to estimate treatment-outcome association\n\nThe re-weighted sample can then be used to estimate the treatment-outcome relationship. This type of weighted model where time-dependent confounding is controlled for is referred to as a marginal structural model.\nThey are simple to implement. For example, ‘a marginal structural Cox regression model is simply a Cox model using the weights as calculated in the procedure described above’[Chesnaye et al. 2022]\nWeighting each observation makes the exposed and unexposed groups groups exchangeable in terms of confounders, with the distribution of confounders similar in both groups. ‘An ATE can then be calculated by a simple comparison or unadjusted regression model.’ [Igelström et al. 2022]\n‘Validity of marginal structural models is assessed with several sensitivity analyses. The distributions of treatment weights, censoring weights and final weights are usually assessed graphically. If extreme values are identified sensitivity analyses are conducted by comparing results of outcome analyses including and excluding outliers (see limitations section and alternative approaches). Often, however, marginal structural models require fitting several different variations of each of the weight-generating models to achieve optimal weight distributions.’ [Williamson and Ravani 2017]",
    "crumbs": [
      "Causal methods",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Marginal structural models (MSM)</span>"
    ]
  },
  {
    "objectID": "content/causal_methods/b1_marginal_structural_models.html#strengths-and-limitations",
    "href": "content/causal_methods/b1_marginal_structural_models.html#strengths-and-limitations",
    "title": "35  Marginal structural models (MSM)",
    "section": "35.2 Strengths and limitations",
    "text": "35.2 Strengths and limitations\nStrengths: * ‘Don’t suffer from collider stratification bias because weighting, as opposed to conditioning, is used to control for time-varying confounders affected by previous treatment’ [Williamson and Ravani 2017]\nLimitations: * As all causal models, MSMs can only balance on known factors, and the exchangeability assumption is not verifiable * ‘Number of balancing variables may be limited by sample size - unusual (or very common) covariates histories may result in failture to achieve stability of estimated weights’ - hence the importance of sensitivity analyses - often through trimming or inc/excluding observations with extreme values * ‘IPTW-based marginal structural models need to include all covariates in the weight estimation. Interaction effect can be estimated for baseline modifiers but not for time-varying modifiers in standard marginal structural models (although history-adjusted marginal structural models have been formulated)’ [Williamson and Ravani 2017]",
    "crumbs": [
      "Causal methods",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Marginal structural models (MSM)</span>"
    ]
  },
  {
    "objectID": "content/causal_methods/b2_gcomputation.html",
    "href": "content/causal_methods/b2_gcomputation.html",
    "title": "36  G-computation",
    "section": "",
    "text": "36.1 Terminology\nNames for this method: * G-computation * Parametric G-formula * G-standardisation * Standardisation [Vansteelandt and Keiding 2011] * Outcome regression [source]\nThere has been some debate around terminology. As quoted from Vansteelandt and Keiding 2011: &gt; ‘The term standardization is revealing and rather well-known to epidemiologists and therefore, in our opinion, is the terminology of choice. The term G-computation has so far been mostly reserved to refer to standardization of the effects of time-varying exposures; potentially the term “G-standardization” as nomenclature for “standardization with respect to generalized exposure regimens” would have been more enlightening. Despite the essential equivalence of G-computation for point exposures and standardization with the total population as the reference, we believe that the developments from the causal inference literature add to the literature on standardization. They give a precise meaning to standardized effect measures in terms of counterfactuals, provide insight into the delicate differences between conditional and marginal epidemiologic effect measures, and suggest novel standardization techniques that combine precision with robustness against model misspecification and extrapolation.’",
    "crumbs": [
      "Causal methods",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>G-computation</span>"
    ]
  },
  {
    "objectID": "content/causal_methods/b2_gcomputation.html#about-the-method",
    "href": "content/causal_methods/b2_gcomputation.html#about-the-method",
    "title": "36  G-computation",
    "section": "36.2 About the method",
    "text": "36.2 About the method\nG-computation involves using a statistical model (e.g. predict) to predict potential outcomes (counterfactuals - with and without exposure).[Igelström et al. 2022]\n\n\n\n\n\n  flowchart LR;\n\n    X(\"Binary treatment (X)\"):::white;\n    Y(\"Outcome (Y)\"):::white;\n    W(\"Confounder (W)\"):::white;\n\n    X --&gt; Y;\n    W --&gt; X;\n    W --&gt; Y;\n  \n    classDef white fill:#FFFFFF, stroke:#FFFFFF;\n    classDef black fill:#FFFFFF, stroke:#000000;\n    classDef empty width:0px,height:0px;\n    classDef green fill:#DDF2D1, stroke: #FFFFFF;\n\n\n\n\n\n\nKey steps: 1. Fit model on observed data - regression model with Y as outcome and X and W as predictors (perhaps also with polynomials and/or interactions if there are multiple control variables) 2. Make identical copy of observed data, but just replace outcome so all X=1. 3. Create another where all to X=0. 4. Use our fitted model to predict outcomes in the two counterfactual datasets [source] [Batten 2023] [source] 5. Estimate the Average treatment effect (ATE) - this is the mean difference (+ 95% CI) in the predicted outcomes between the two groups.[Batten 2023] It essentially describes the average effect, at a population level, of moving an entire population from untreated to treated.[Chatton et al. 2020]\nFor average treatment effect on the treated (ATT): * This is the average effect of treatment on those subjects who ultimately received the treatement [Chatton et al. 2020] * To calculate ATT, use only the treated units in steps 2 and 3. ‘The control units are still used to fit the model in 1, but only the treated units are used to compute the predicted values.’ [source]\nYou could also compute average treatment effect on the untreated (ATU). [Wang et al. 2017]\nTo get standard errors, you can use bootstrapping or the delta method. [source]",
    "crumbs": [
      "Causal methods",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>G-computation</span>"
    ]
  },
  {
    "objectID": "content/causal_methods/b2_gcomputation.html#how-does-this-method-enable-us-to-deal-with-treatment-dependent-confounding",
    "href": "content/causal_methods/b2_gcomputation.html#how-does-this-method-enable-us-to-deal-with-treatment-dependent-confounding",
    "title": "36  G-computation",
    "section": "36.3 How does this method enable us to deal with treatment-dependent confounding?",
    "text": "36.3 How does this method enable us to deal with treatment-dependent confounding?\nSee the example of treatment-dependent confounding below.\n\n\n\n\n\n  flowchart LR;\n\n    ak(\"A&lt;sub&gt;K&lt;/sub&gt;: Treatment\"):::green;\n    ak1(\"A&lt;sub&gt;K+1&lt;/sub&gt;: Treatment\"):::green;\n    yk1(\"Y&lt;sub&gt;K+1&lt;/sub&gt;: Outcome\"):::green;\n    yk2(\"Y&lt;sub&gt;K+2&lt;/sub&gt;: Outcome\"):::green;\n    lk(\"L&lt;sub&gt;K&lt;/sub&gt;: Confounder\"):::white;\n    lk1(\"L&lt;sub&gt;K+1&lt;/sub&gt;: Confounder\"):::white;\n    u(\"U: Unmeasured&lt;br&gt;confounder\"):::white;\n\n    ak --&gt; yk1;\n    u --&gt; yk1;\n    u --&gt; lk;\n    lk --&gt; ak;\n    lk --&gt; ak1;\n    ak --&gt; ak1;\n    ak --&gt; lk1;\n    u --&gt; lk1;\n    lk1 --&gt; ak1;\n    ak1 --&gt; yk2;\n    u --&gt; yk2;\n    lk --&gt; yk1;\n    lk --&gt; lk1;\n    lk1 --&gt; yk2;\n    yk1 --&gt; yk2;\n\n    classDef white fill:#FFFFFF, stroke:#FFFFFF\n    classDef black fill:#FFFFFF, stroke:#000000\n    classDef green fill:#DDF2D1, stroke: #FFFFFF;\n\n\n\n\n\n\nIf we regressed AK and AK+1 on YK+2 while adjusting for confounders should we adjust for the time-varying LK+1 and YK+1? We can see that: 1. LK+1 is a collider on the non-causal path AK –&gt; LK+1 &lt;– U –&gt; YK+2\n\n\n\n\n\n  flowchart LR;\n\n    ak(\"A&lt;sub&gt;K&lt;/sub&gt;: Treatment\"):::red;\n    ak1(\"A&lt;sub&gt;K+1&lt;/sub&gt;: Treatment\"):::white;\n    yk1(\"Y&lt;sub&gt;K+1&lt;/sub&gt;: Outcome\"):::white;\n    yk2(\"Y&lt;sub&gt;K+2&lt;/sub&gt;: Outcome\"):::red;\n    lk(\"L&lt;sub&gt;K&lt;/sub&gt;: Confounder\"):::white;\n    lk1(\"L&lt;sub&gt;K+1&lt;/sub&gt;: Confounder\"):::red;\n    u(\"U: Unmeasured&lt;br&gt;confounder\"):::red;\n\n    ak --&gt; yk1;\n    u --&gt; yk1;\n    u --&gt; lk;\n    lk --&gt; ak;\n    lk --&gt; ak1;\n    ak --&gt; ak1;\n    ak --&gt; lk1;\n    u --&gt; lk1;\n    lk1 --&gt; ak1;\n    ak1 --&gt; yk2;\n    u --&gt; yk2;\n    lk --&gt; yk1;\n    lk --&gt; lk1;\n    lk1 --&gt; yk2;\n    yk1 --&gt; yk2;\n\n    classDef white fill:#FFFFFF, stroke:#FFFFFF\n    classDef black fill:#FFFFFF, stroke:#000000\n    classDef red fill:#FFCCCB, stroke:red\n    classDef green fill:#DDF2D1, stroke: #FFFFFF\n    linkStyle 6,7,10 stroke:red;\n\n\n\n\n\n\n\nLK+1 is a mediator on the causal path AK –&gt; LK+1 –&gt; AK+1 –&gt; YK+2\n\n\n\n\n\n\n  flowchart LR;\n\n    ak(\"A&lt;sub&gt;K&lt;/sub&gt;: Treatment\"):::red;\n    ak1(\"A&lt;sub&gt;K+1&lt;/sub&gt;: Treatment\"):::red;\n    yk1(\"Y&lt;sub&gt;K+1&lt;/sub&gt;: Outcome\"):::white;\n    yk2(\"Y&lt;sub&gt;K+2&lt;/sub&gt;: Outcome\"):::red;\n    lk(\"L&lt;sub&gt;K&lt;/sub&gt;: Confounder\"):::white;\n    lk1(\"L&lt;sub&gt;K+1&lt;/sub&gt;: Confounder\"):::red;\n    u(\"U: Unmeasured&lt;br&gt;confounder\"):::white;\n\n    ak --&gt; yk1;\n    u --&gt; yk1;\n    u --&gt; lk;\n    lk --&gt; ak;\n    lk --&gt; ak1;\n    ak --&gt; ak1;\n    ak --&gt; lk1;\n    u --&gt; lk1;\n    lk1 --&gt; ak1;\n    ak1 --&gt; yk2;\n    u --&gt; yk2;\n    lk --&gt; yk1;\n    lk --&gt; lk1;\n    lk1 --&gt; yk2;\n    yk1 --&gt; yk2;\n\n    classDef white fill:#FFFFFF, stroke:#FFFFFF\n    classDef black fill:#FFFFFF, stroke:#000000\n    classDef red fill:#FFCCCB, stroke:red\n    classDef green fill:#DDF2D1, stroke: #FFFFFF\n    linkStyle 6,8,9 stroke:red;\n\n\n\n\n\n\n\nLK+1 is a confounder on the non-causal path AK+1 &lt;– LK+1 –&gt; YK+2\n\n\n\n\n\n\n  flowchart LR;\n\n    ak(\"A&lt;sub&gt;K&lt;/sub&gt;: Treatment\"):::white;\n    ak1(\"A&lt;sub&gt;K+1&lt;/sub&gt;: Treatment\"):::red;\n    yk1(\"Y&lt;sub&gt;K+1&lt;/sub&gt;: Outcome\"):::white;\n    yk2(\"Y&lt;sub&gt;K+2&lt;/sub&gt;: Outcome\"):::red;\n    lk(\"L&lt;sub&gt;K&lt;/sub&gt;: Confounder\"):::white;\n    lk1(\"L&lt;sub&gt;K+1&lt;/sub&gt;: Confounder\"):::red;\n    u(\"U: Unmeasured&lt;br&gt;confounder\"):::white;\n\n    ak --&gt; yk1;\n    u --&gt; yk1;\n    u --&gt; lk;\n    lk --&gt; ak;\n    lk --&gt; ak1;\n    ak --&gt; ak1;\n    ak --&gt; lk1;\n    u --&gt; lk1;\n    lk1 --&gt; ak1;\n    ak1 --&gt; yk2;\n    u --&gt; yk2;\n    lk --&gt; yk1;\n    lk --&gt; lk1;\n    lk1 --&gt; yk2;\n    yk1 --&gt; yk2;\n\n    classDef white fill:#FFFFFF, stroke:#FFFFFF\n    classDef black fill:#FFFFFF, stroke:#000000\n    classDef red fill:#FFCCCB, stroke:red\n    classDef green fill:#DDF2D1, stroke: #FFFFFF\n    linkStyle 8,9,13 stroke:red;\n\n\n\n\n\n\nIf we adjusted for for LK+1 we would therefore: * Eliminate confounding bias * Introduce collider (stratification) bias * Introduce over-adjustment bias in the effect of AK (from controlling for mediator)\nThe same argument applies when adjusting for the intermediate outcome YK+1. In other words, its impossible as we need to simultaneously adjust for and not adjust for LK+1 and YK+1. ‘The g-formula resolves this problem by decoupling adjusting, and not adjusting, for treatment-dependent confounders’ like LK+1.\n\nG-formula step 1 involves fitting a model with outcome, treatment and confounders. This adjusts for LK+1 and YK+1, to avoid confounding bias.\nG-formula step 2 involves predicting outcomes where all patients are set to no treatment, or all to treatment, and comparing the outcomes to get the average treatment effect. This marginalises/averages over the counterfactual distribution of LK+1 and YK+1, so they are not adjusted for, to avoid collider and over-adjustment biases\n\nHence, it simultaneously does and not adjust for LK+1 and YK+1. [Loh et al. 2023]",
    "crumbs": [
      "Causal methods",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>G-computation</span>"
    ]
  },
  {
    "objectID": "content/causal_methods/b2_gcomputation.html#assumptions",
    "href": "content/causal_methods/b2_gcomputation.html#assumptions",
    "title": "36  G-computation",
    "section": "36.4 Assumptions",
    "text": "36.4 Assumptions\n\nSequential ignorability assumption i.e. no unmeasured confounding - ‘when there is no causal effect, the treatment and outcome are conditionally independent given a set of pre-treatment covariates’ [Loh et al. 2023]",
    "crumbs": [
      "Causal methods",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>G-computation</span>"
    ]
  },
  {
    "objectID": "content/causal_methods/b2_gcomputation.html#g-computation-v.s.-iptw",
    "href": "content/causal_methods/b2_gcomputation.html#g-computation-v.s.-iptw",
    "title": "36  G-computation",
    "section": "36.5 G-computation v.s. IPTW",
    "text": "36.5 G-computation v.s. IPTW\n‘Standardization models the outcome, whereas inverse probability weighting models the treatment’. If we were to do IPW and standardisation ‘without using any models (i.e. non-parametrically) then we would expect both methods to give the exact same result’. [Batten 2023]\nHowever, we expect them to differ if we use models to estimate them since some degree of misspecification is inescapable in models, ‘but misspecification in the treatment model (IP weighting) and outcome model (standardisation) will not generally result in the same magnitude and direction of bias in the effect estimate’.\n‘Both IP weighting and standardization are estimators of the g-formula, a general method for causal inference first described in 1986… We say that standardization is a plug-in g-formula estimator because it simply replaces the conditional mean outcome in the g-formula by its estimates. When those estimates come from parametric models, we refer to the method as the parametric g-formula. Because here we were only interested in the average causal effect, we estimated parametrically the conditional mean outcome.’\n‘Often there is no need to choose between IP weighting and the parametric g-formula. When both methods can be used to estimate a causal effect, just use both methods. Also, whenever possible, use doubly robust methods that combine models for treatment and for outcome in the same estimator’. [Hernán and Robins 2024]",
    "crumbs": [
      "Causal methods",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>G-computation</span>"
    ]
  },
  {
    "objectID": "content/causal_methods/b3_gestimation.html",
    "href": "content/causal_methods/b3_gestimation.html",
    "title": "37  G-estimation of structural nested models",
    "section": "",
    "text": "37.1 G-computation + IPTW\n‘G-estimation refers to the method of estimating the parameter(s) (see below) and does not specify the model. Most implementations of g-estimation are for structural nested models.’ G-estimation of structural nested models (SNM) is a method first proposed by Robins (1986). It’s applications include: * Adjusting for time-varying confounded affected by prior exposure/treatment (treatment-confounder feedback) * Mediation analysis (when exposure affects a confounder of the mediator-outcome relationship) [Picciotto and Neophytou 2016]\nDescription of method: ‘G-estimation of structural nested models (SNM) predicts the counterfactual outcome at each time point given no exposure from that point onwards, conditional on prior values of the exposure and confounders.’ [Igelström et al. 2022] In other words, ‘these models control for time-varying confounding affected by previous treatment (exposure) by modeling the outcome at each time as a function of the treatment and covariate history up to that time’. [Williamson and Ravani 2017] The models are “nested” as we conceptualise the longitudinal data as a nested series of trials. In this approach: 1. Analyse last trial first. We adjust for past exposures and covariates. 2. Once the relationship between that exposure and the outcome is understood, the method analytically “removes” the effect of that final exposure from the outcome. This process is repeated at each time point m, for m taking every value down to the beginning of follow-up, after “intervening” to remove exposures occurring at all later time points. 3. At the end, the counterfactual outcome has been estimated for each individual under a regimen of being always unexposed/untreated, as a function of observed variables and the unknown parameter’ [Picciotto and Neophytou 2016]\nEstimation of the parameters using G-estimation: ‘In applications, the process entails a grid search or optimization algorithm in which hypothetical values for the parameter are used to calculate candidate values for counterfactual outcomes for each individual under no exposure/treatment. Unlike, for example, a maximum likelihood estimation procedure, g-estimation directly leverages the assumption of conditional exchangeability by determining which set of candidate counterfactual outcomes is statistically independent of observed exposures, conditional on previously measured covariates and treatments/exposures. This can be done by including the candidate counterfactual outcome along with earlier covariates and exposures in a model predicting observed exposure and checking whether the coefficient for the candidate counterfactual outcome is zero. The parameter value that was used to calculate that set of candidate counterfactual outcomes is the g-estimate’ [Picciotto and Neophytou 2016]\nDiagram from Picciotto and Neophytou 2016:\nAssumptions: * Consistency - true if treatment is well-defined or various versions of treatment have equivalent effects * Conditional exchangeability (“no unmeasured confounders”) * Correct specification of models * No interference (“stable unit treatment value assumption”) * Does not require the positive assumption [Picciotto and Neophytou 2016]\nClasses of structural nested models: * Structural Nested Mean Models - studies ‘the change (difference or ratio) in the mean of an outcome that is a continuous variable (e.g., a biomarker) as a function of time-varying treatment or exposure, conditional on observed history of covariates’ * Structural Nested Cumulative Failure Time Models - ‘outcome under study is the risk of a failure event at each time point, not just at the end of follow-up’ * Structural Nested Accelerated Failure Time Models - ‘Rather than the risk of outcome at each time point, this class of models considers length of survival time as the outcome; time to event can be accelerated or delayed by exposure. Furthermore, instead of the change in counterfactual outcome due to a change in exposure, structural nested accelerated failure time models (SNAFTMs) model the counterfactual outcome itself: (median) counterfactual survival time if unexposed is modeled as a function of observed variables’ [Picciotto and Neophytou 2016]\nVariations: * Optimal Dynamic Treatment Regimen * Double Robustness [Picciotto and Neophytou 2016]\n‘Some authors have proposed combinations of G-computation and propensity scores to improve the estimation of the marginal causal effect. These methods are known as doubly robust estimators (DRE) because they require the specification of both the outcome (for GC) and treatment allocation (for PS) mechanisms to minimise the impact of model misspecification’. [Chatton et al. 2020]",
    "crumbs": [
      "Causal methods",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>G-estimation of structural nested models</span>"
    ]
  },
  {
    "objectID": "content/causal_methods/c1_instrumental_variable.html",
    "href": "content/causal_methods/c1_instrumental_variable.html",
    "title": "38  Instrumental variables",
    "section": "",
    "text": "38.1 What is an instrumental variable?\nAn instrumental variable (IV) is a variable that satisfies the following assumptions: 1. It causes variation in the exposure/treatment (i.e. relevance assumption - it is correlated with X, an endogenous explanatory variable) - also known as the strong first stage assumption 2. It is unrelated to the outcome (and so any affect on outcome is exclusively via the instruments effect on the exposure) - also known as the exclusion restriction 3. As it is unrelated to the outcome, is is therefore unrelated to unmeasured confounders (unmeasured differences in characteristics that affect outcomes) (i.e. exogeneity assumption - it is an exogneous variable) [McClellan et al. 1994][Igelström et al. 2022] [source] [source]\nThis is illustrated below:\nflowchart LR;\n\n    iv(\"Instrumental variable\"):::white;\n    e(\"Exposure/treatment\"):::white;\n    o(\"Outcome\"):::white;\n    u(\"Unmeasured confounders\"):::white;\n\n    iv --&gt; e;\n    e --&gt; o;\n    u -.-&gt; e;\n    u -.-&gt; o;\n  \n    classDef white fill:#FFFFFF, stroke:#FFFFFF;\n    classDef black fill:#FFFFFF, stroke:#000000;\n    classDef empty width:0px,height:0px;\n    classDef green fill:#DDF2D1, stroke: #FFFFFF;\nDue to these characteristics, instrumental variables enable us to mimic randomisation to treatment. [McClellan et al. 1994] In fact, randomisation to treatment in an RCT is an example instrumental variable (it meets the above assumptions).\nflowchart LR;\n\n    iv(\"RCT randomisation to treatment\"):::white;\n    e(\"Exposure/treatment\"):::white;\n    o(\"Outcome\"):::white;\n    u(\"Unmeasured confounders\"):::white;\n\n    iv --&gt; e;\n    e --&gt; o;\n    u -.-&gt; e;\n    u -.-&gt; o;\n  \n    classDef white fill:#FFFFFF, stroke:#FFFFFF;\n    classDef black fill:#FFFFFF, stroke:#000000;\n    classDef empty width:0px,height:0px;\n    classDef green fill:#DDF2D1, stroke: #FFFFFF;\nYou have likely identified an instrumental variable ‘if people are confused when you tell them about the instrument’s relationship to the outcome’. * No-one is confused if you say family size will reduce labour supply of women * They will be confused if you use gender composition of first two children as instrumental variable, and find ‘that mothers whose first two children were the same gender were employed outside the home less than those whose two children had a balanced sex ratio’, as they don’t expect gender composition to incentive work outside home - but it is related as if both the same gender, they’re more likely to try again for child of another gender [Causal Inference: The Mixtape - Scott Cunningham]",
    "crumbs": [
      "Causal methods",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Instrumental variables</span>"
    ]
  },
  {
    "objectID": "content/causal_methods/c1_instrumental_variable.html#two-stage-least-squares-2sls",
    "href": "content/causal_methods/c1_instrumental_variable.html#two-stage-least-squares-2sls",
    "title": "38  Instrumental variables",
    "section": "38.2 Two-stage least squares (2SLS)",
    "text": "38.2 Two-stage least squares (2SLS)\nYou can use the two-stage least squares (2SLS) method to estimate the causal effect, using instrumental variables for individual-level data.\n\nRegress exposure (\\({x}\\)) on instrumental variable (G) to produce \\(\\hat{x}\\) (estimate of exposure independent of confounders)\nRegress outcome (Y) on \\(\\hat{x}\\). Here, \\(\\hat{x}\\) is replacing the actual value of the problematic predictor, \\({x}\\). [BSc Medical Sciences]\n\nWe include the measured confounders in both stages. * Included in first stage to reflect exogenous movement in treatment * Included in second stage to avoid omitted variable bias\nThe ‘groups being compared differ only in likelihoods of treatment, as opposed to a division into pure treatment and control groups’. Hence, this ‘method estimates an incremental or “marginal” effect of treatment only over the range of variation in treatment across the IV groups’. [McClellan et al. 1994]\n‘IV analysis estimates a local average treatment effect (LATE) among ’compliers’ - individudals whose exposure status is affected by the instrument. This group cannot be precisely identified, and the LATE may therefore sometimes be of limited practical or policy relevance’. [Igelström et al. 2022]\nThere is a variant called ‘Three-Stage Least Squares (3SLS). This method is an extension of the 2SLS method and is used when there are more than two endogenous variables in the model. The 3SLS method is based on the idea that the endogenous variables are correlated with each other, and therefore, the coefficients of these variables need to be estimated simultaneously.’ [source]",
    "crumbs": [
      "Causal methods",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Instrumental variables</span>"
    ]
  },
  {
    "objectID": "content/causal_methods/c1_instrumental_variable.html#assumptions-of-instrumental-variable-analysis",
    "href": "content/causal_methods/c1_instrumental_variable.html#assumptions-of-instrumental-variable-analysis",
    "title": "38  Instrumental variables",
    "section": "38.3 Assumptions of instrumental variable analysis",
    "text": "38.3 Assumptions of instrumental variable analysis\n\nInstrumental variable assumptions(as above)\nHomogeneity assumption - the association between the instrumental variable and the exposure is homogenous (same for everyone in the population), or the effect of the exposure on the outcome is homogenous [BSc Medical Sciences]",
    "crumbs": [
      "Causal methods",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Instrumental variables</span>"
    ]
  },
  {
    "objectID": "content/causal_methods/c1_instrumental_variable.html#limitations",
    "href": "content/causal_methods/c1_instrumental_variable.html#limitations",
    "title": "38  Instrumental variables",
    "section": "38.4 Limitations",
    "text": "38.4 Limitations\n‘A concern with any IV strategy is that the instrumental variable is correlated with unobserved determinants of the outcome of interest.’ Can address by estimating models for held-back risk factors pre-determined at delivery date, looking for correlation with the IV. [Card et al. 2018]\nA weak instrument will produce an estimate that is biased and imprecise (and we may then have false confidence in it being unbiased). To help address this… * Check strong first stage assumption - i.e. want statistically significant relationship when regress X on Z - suggests F-statistic of at least 11 in first stage. * Can’t test exclusion restriction assumption - just have to think through any possible ways that the IV could affect the outcome that aren’t via the treatment. [source]",
    "crumbs": [
      "Causal methods",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Instrumental variables</span>"
    ]
  },
  {
    "objectID": "content/causal_methods/c1_instrumental_variable.html#non-parametric-instrumental-variable-methods",
    "href": "content/causal_methods/c1_instrumental_variable.html#non-parametric-instrumental-variable-methods",
    "title": "38  Instrumental variables",
    "section": "38.5 Non-parametric instrumental variable methods",
    "text": "38.5 Non-parametric instrumental variable methods\n‘Most IV applications make use of a two-stage least squares procedure [2SLS; e.g., Angrist et al., 1996] that requires assumptions of linearity and homogeneity (e.g., all airline customers must have the same price sensitivity). Nonparametric IV methods from the econometrics literature relax these assumptions [e.g., Newey and Powell, 2003]. However, these methods work by modeling the outcome as an unknown linear combination of a pre-specified set of basis functions of the treatment and other covariates (e.g. Hermite polynomials, wavelets or splines) and then modeling the conditional expectation of each of these basis functions in terms of the instruments (i.e. the number of equations is quadratic in the number of basis functions). This requires a strong prior understanding of the data generating process by the researcher, and the complexity of both specification and estimation explodes when there are more than a handful of inputs.’ [Hartford et al. 2017]\nHartford et al. 2017 propose the Deep IV framework, which uses ML to perform IV analysis… * First stage: fit conditional distribution for treatment given instruments and covariates with a deep neural net (stochastic gradient descent (SGD)) [Hartford et al. 2017] * Second stage: ‘target a loss function involving integration over the conditional treatment distribution from the first stage’ [Hartford et al. 2017]… ‘incorporate the fitted conditional distribution to minimize a loss function using a second deep neural net, and use an out-of-sample causal validation procedure to tune the deep network hyper-parameters’ [source]\nThey provide the package on GitHub for this analysis using Python (broken - only supports Keras 2.0.6) (although there has been non-integrated pull request to make it work with Keras 2.3.1).\nXu et al. 2021 provide another example of IV with deep learning, again sharing code on GitHub. This has open peer review, and so helpfully has some insightful comments around assumptions and theoretical justifications.",
    "crumbs": [
      "Causal methods",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Instrumental variables</span>"
    ]
  },
  {
    "objectID": "content/causal_methods/c1_instrumental_variable.html#examples",
    "href": "content/causal_methods/c1_instrumental_variable.html#examples",
    "title": "38  Instrumental variables",
    "section": "38.6 Examples",
    "text": "38.6 Examples\n\n38.6.1 Example 1: Mendelian randomisation\nDisease association with non-genetic risk factors are often confounded - for example:\n\n\n\n\n\n  flowchart LR;\n\n    e(\"Drinking alcohol\"):::green;\n    o(\"Lung cancer\"):::green;\n    c(\"Smoking\"):::white;\n\n    e --&gt; o;\n    c --&gt; e; c --&gt; o;\n  \n    classDef white fill:#FFFFFF, stroke:#FFFFFF;\n    classDef black fill:#FFFFFF, stroke:#000000;\n    classDef empty width:0px,height:0px;\n    classDef green fill:#DDF2D1, stroke: #FFFFFF;\n\n\n\n\n\n\nGenotype-phenotype associations are much less likely to be confounded. They are aetiological associations (causing or contributing to disease/condition development).\nIf we know of a gene closely linked to the phenotype without direct effect on the disease, it can often be reasonably assumed that the gene is not itself associated with any confounding factors - a phenomenon called Mendelian randomization. Genetic variants (G) should influence the exposure (X) but should not be directly associated with confounders (U) or the outcome (Y).\nWe can then identify the causal relationship between X and Y - if it doesn’t cause Y, then G should be independent of Y. This is the same logic as we use for an RCT (where G would be randomisation to treatment).\n\n\n\n\n\n  flowchart TD;\n\n    con:::outline;\n    subgraph con[\"If X doesn't cause Y...\"]\n      G:::white;\n      X:::white;\n      Y:::white;\n      U:::white;\n    end\n\n    G --&gt; X;\n    U --&gt; X;\n    U --&gt; Y;\n\n    uncon:::outline;\n    subgraph uncon[\"If X causes Y...\"]\n      G2(\"G\"):::white;\n      X2(\"X\"):::white;\n      Y2(\"Y\"):::white;\n      U2(\"U\"):::white;\n    end\n\n    G2 --&gt; X2;\n    U2 --&gt; X2;\n    U2 --&gt; Y2;\n    X2 --&gt; Y2;\n\n    classDef white fill:#FFFFFF, stroke:#FFFFFF;\n    classDef black fill:#FFFFFF, stroke:#000000;\n    classDef outline fill:#FFFFFF;\n\n\n\n\n\n\n[BSc Medical Sciences]\n\n\n38.6.2 Example 2: Effect of intensive treatment on mortality in patients with acute myocardial infarction\nExample from McClellan et al. 1994: Want to determine the effect of more intensive treatments (e.g. catheterisation and revascualisation) on mortality in elderly patients with acute myocardial infarction. They use distance from hospital as an ‘instrumental variable to account for unobserved case-mix variation (selection bias) in observational Medicare claims data’ [McClellan et al. 1994]\nWhy is distance an instrumental variable? * Intensive treatment only performed at certain hospitals * Comparing treatments and outcomes between patients treated at different hospitals risks selection bias, ‘because physician and patient decisions that influence treatment choice may ALSO influence choice of hospital to go to - e.g. AMI patients who appear to be better candidates for catheterisation may be disproportionately admitted to catheterisation hospitals’ [McClellan et al. 1994] * Distance from hospital (specifically, differential distance to hospital providing intensive treatment): * Affects probability of receiving intensive treatment * Does not affect patient characteristics * Hence, distance from hospital can be used as an instrumental variable [Igelström et al. 2022] - it meets the IV assumptions: 1. Causes variation in whether receive intensive treatment 2. Unrelated to mortality 3. Unrelated to unmeasured confounders (like the physician and patient decisions)\n\n\n\n\n\n  flowchart LR;\n\n    treat(\"&lt;b&gt;Intensive treatment&lt;/b&gt;&lt;br&gt;(e.g. catheterisation)\"):::green;\n    mortality(\"&lt;b&gt;Mortality&lt;/b&gt;\"):::green;\n    hosp(\"&lt;b&gt;Hospital&lt;/b&gt; attended&lt;br&gt;(i.e. whether it&lt;br&gt;offers the treatment)\"):::white;\n    dist(\"&lt;b&gt;Differential distance&lt;/b&gt; to&lt;br&gt; hospital providing&lt;br&gt;intenstive treatment\"):::white;\n    decisions(\"Physician and patient&lt;br&gt;&lt;b&gt;decisions&lt;/b&gt;\"):::white;\n\n    dist --&gt; hosp;\n    decisions --&gt; hosp;\n    hosp --&gt; treat;\n    treat --&gt; mortality;\n  \n    classDef white fill:#FFFFFF, stroke:#FFFFFF;\n    classDef black fill:#FFFFFF, stroke:#000000;\n    classDef empty width:0px,height:0px;\n    classDef green fill:#DDF2D1, stroke: #FFFFFF;\n\n\n\n\n\n\nFirst, checked for presence of selection bias (i.e. whether hospital type affects treatment intensity - and therefore whether simple comparisons of treatments and outcomes across hospital types are valid). Then, as was present, used IV methods to estimate effect of intensive treatment on mortality. Differential distance approximately randomises patients to different likelihoods of receiving intensive treatments, uncorrelated with health status. * First, compared two groups of apx. equal size - patients near to catheterisation hospital (differential distance &lt;= 2.5 miles) and patients far from them (&gt; 2.5 miles). This evidenced assumption that distribution of health status in AMI patients is independent of differential distance, and illustrates IV method, estimating the ‘average effect of invasive treatment for all patients who are marginal from the standpoint of the near-far IVs - those who undergo catheterisation in the relatively near group and not in the relatively far group - if the two groups are balanced and if catheterisation is the only treatment that differs between the groups’ * Then used more general IV estimation netchniques with wide range of differential distance groups, and account for small remaining observable differences between differential-distance groups [McClellan et al. 1994]\n\n\n38.6.3 Example 3: caesarean section\nThis example is taken from It’s about time: Cesarean sections and neonatal health [Costa-Ramón et al. 2018].\nThe study aimed to estimate the causal relationship between caesarean section and newborn health: Apgar-1 and Apgar-5; Reanimation (assisted ventilation); ICU admission; Neonatal death; Umbilical cord pH.\nSeveral known measured confounders (e.g. age, gestational length, obstetric risk), but also omitted variable bias due to other unmeausred confounders.\n\n\n\n\n\n  flowchart LR;\n\n    e(\"Caesarean section\"):::green;\n    o(\"Neonatal health&lt;br&gt;(e.g. Apgar)\"):::green;\n    c(\"Measured and&lt;br&gt;unmeasured confounders\"):::white;\n\n    e --&gt; o;\n    c --&gt; e; c --&gt; o;\n  \n    classDef white fill:#FFFFFF, stroke:#FFFFFF;\n    classDef black fill:#FFFFFF, stroke:#000000;\n    classDef empty width:0px,height:0px;\n    classDef green fill:#DDF2D1, stroke: #FFFFFF;\n\n\n\n\n\n\nTime of birth can be used as an instrumental variable as: * It is associated with treatment (unplanned C-sections more likely in early hours of night) * It is unrelated to outcome and confounders (mothers giving birth at different times of day are observationally similar - suggesting excess number of C-sections observed are due to non-medical reasons)\n\n\n\n\n\n  flowchart LR;\n\n    e(\"Caesarean section\"):::green;\n    o(\"Neonatal health&lt;br&gt;(e.g. Apgar)\"):::green;\n    c(\"Measured and&lt;br&gt;unmeasured confounders\"):::white;\n    i(\"Time of day\"):::white;\n\n    i --&gt; e;\n    e --&gt; o;\n    c --&gt; e; c --&gt; o;\n  \n    classDef white fill:#FFFFFF, stroke:#FFFFFF;\n    classDef black fill:#FFFFFF, stroke:#000000;\n    classDef empty width:0px,height:0px;\n    classDef green fill:#DDF2D1, stroke: #FFFFFF;\n\n\n\n\n\n\nShowing example of results for Apgar-5 scores.\nFrom standard OLS estimation, unplanned CS coefficient for predicting Apgar-5 (with standard error in parentheses, clustered at hospital-shift level): * -0.219 (0.038) (regression controlling only for weekday and hospital fixed effects) * -0.219 (0.037) (maternal controls added) * -0.142 (0.043) (pregnancy controls added)\nInterpretation: Delivering C-section associated with decline in Apgar-5. However, likely biased as C-section v.s. vaginal birth cohorts not comparable.\nHence, performed 2SLS. In first stage, early night coefficients for predicting unplanned C-section were: * 0.073 (0.011) * 0.073 (0.011) * 0.063 (0.011)\nIntepretation: Births in early night are 6% more likely to be by caesarean.\nIn second stage, unplanned CS coefficients for predicting Apgar-5: * -0.965 (0.404) * -0.987 (0.408) * -0.936 (0.464)\nInterpretation: Increased probability of Apgar-5, significant at 5% significance level.",
    "crumbs": [
      "Causal methods",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Instrumental variables</span>"
    ]
  },
  {
    "objectID": "content/causal_methods/c1_instrumental_variable.html#python-example",
    "href": "content/causal_methods/c1_instrumental_variable.html#python-example",
    "title": "38  Instrumental variables",
    "section": "38.7 Python example",
    "text": "38.7 Python example\nThis example is taken from Causal Inference for the Brave and True (MIT license).\nInterested in causal effect of education on wage. Confounders like ability (which affect education and wage).\n\n\n\n\n\n  flowchart LR;\n\n    e(\"Education&lt;br&gt;(years of schooling)\"):::green;\n    o(\"Wage\"):::green;\n    c(\"Year of birth&lt;br&gt;State of birth&lt;br&gt;Ability\"):::white;\n\n    e --&gt; o;\n    c --&gt; e; c --&gt; o;\n  \n    classDef white fill:#FFFFFF, stroke:#FFFFFF;\n    classDef black fill:#FFFFFF, stroke:#000000;\n    classDef empty width:0px,height:0px;\n    classDef green fill:#DDF2D1, stroke: #FFFFFF;\n\n\n\n\n\n\nQuarter of birth (i.e. dividing year into Q1, Q2, Q3, Q4) impacts education (kids born earlier in school year start education at older age than kids born later in school year), and does not impact wage (other than through its impact on education).\n\n\n\n\n\n  flowchart LR;\n\n    e(\"Education&lt;br&gt;(years of schooling)\"):::green;\n    o(\"Wage\"):::green;\n    c(\"Year of birth&lt;br&gt;State of birth&lt;br&gt;Ability\"):::white;\n    i(\"Quarter of birth\"):::white;\n\n    i --&gt; e;\n    e --&gt; o;\n    c --&gt; e; c --&gt; o;\n  \n    classDef white fill:#FFFFFF, stroke:#FFFFFF;\n    classDef black fill:#FFFFFF, stroke:#000000;\n    classDef empty width:0px,height:0px;\n    classDef green fill:#DDF2D1, stroke: #FFFFFF;\n\n\n\n\n\n\n\n38.7.1 Set-up (import packages and data)\n\n# Import packages\nfrom dataclasses import dataclass\nfrom linearmodels.iv import IV2SLS\nimport matplotlib.pyplot as plt\nimport os\nimport pandas as pd\nimport statsmodels.formula.api as smf\n\n# Define file paths\n@dataclass(frozen=True)\nclass Paths:\n    '''Singleton object for storing paths to data and database.'''\n\n    data = '../data'\n    ak91 = 'ak91.csv'\n\n\npaths = Paths()\n\n\ndata = pd.read_csv(os.path.join(paths.data, paths.ak91))\ndata.head()\n\n\n\n\n\n\n\n\nlog_wage\nyears_of_schooling\nyear_of_birth\nquarter_of_birth\nstate_of_birth\n\n\n\n\n0\n5.790019\n12.0\n30.0\n1.0\n45.0\n\n\n1\n5.952494\n11.0\n30.0\n1.0\n45.0\n\n\n2\n5.315949\n12.0\n30.0\n1.0\n45.0\n\n\n3\n5.595926\n12.0\n30.0\n1.0\n45.0\n\n\n4\n6.068915\n12.0\n30.0\n1.0\n37.0\n\n\n\n\n\n\n\n\n# Convert quarter of birth from categorcal to dummy variable\nfactor_data = data.assign(\n    **{f'q{int(q)}': (data['quarter_of_birth'] == q).astype(int)\n       for q in data['quarter_of_birth'].unique()})\n\nfactor_data.head()\n\n\n\n\n\n\n\n\nlog_wage\nyears_of_schooling\nyear_of_birth\nquarter_of_birth\nstate_of_birth\nq1\nq2\nq3\nq4\n\n\n\n\n0\n5.790019\n12.0\n30.0\n1.0\n45.0\n1\n0\n0\n0\n\n\n1\n5.952494\n11.0\n30.0\n1.0\n45.0\n1\n0\n0\n0\n\n\n2\n5.315949\n12.0\n30.0\n1.0\n45.0\n1\n0\n0\n0\n\n\n3\n5.595926\n12.0\n30.0\n1.0\n45.0\n1\n0\n0\n0\n\n\n4\n6.068915\n12.0\n30.0\n1.0\n37.0\n1\n0\n0\n0\n\n\n\n\n\n\n\n\n\n38.7.2 Traditional OLS estimate\n\ndef parse(model, exog=\"years_of_schooling\"):\n    param = model.params[exog]\n    se = model.std_errors[exog]\n    p_val = model.pvalues[exog]\n    print(f\"Parameter: {param}\")\n    print(f\"SE: {se}\")\n    print(f\"95 CI: {(-1.96*se,1.96*se) + param}\")\n    print(f\"P-value: {p_val}\")\n\n\nformula = \"log_wage ~ years_of_schooling + C(state_of_birth) + C(year_of_birth) + C(quarter_of_birth)\"\nols = IV2SLS.from_formula(formula, data=data).fit()\nparse(ols)\n\nParameter: 0.06732572817658422\nSE: 0.00038839984390486796\n95 CI: [0.06656446 0.06808699]\nP-value: 0.0\n\n\n\n\n38.7.3 Quarter of birth\nShow that quarter of birth is an instrumental variable, which requires it to:\n\nCause variation in exposure - individuals born in the last quarter of the year have slightly more time in education than those born in the beginning of the year\n\nWe check this using a graph and regression. For simplicity, we’re just using Q4 (yes/no) as our instrument.\nIn the regression, we regress the instrumental variable (q4) and confounders (year + state of birth) on the exposure (years of schooling). This will statistically demonstrate that Q4 is an instrumental variable - will show us that quarter of birth affects years of schooling, whilst also control for year and state of birth. Here, on average, Q4 have 0.1 more years of education than those born in other quarters of year.\n\ngroup_data = (data\n              .groupby([\"year_of_birth\", \"quarter_of_birth\"])\n              [[\"log_wage\", \"years_of_schooling\"]]\n              .mean()\n              .reset_index()\n              .assign(time_of_birth = lambda d: d[\"year_of_birth\"] + (d[\"quarter_of_birth\"])/4))\n\nplt.figure(figsize=(15,6))\nplt.plot(group_data[\"time_of_birth\"], group_data[\"years_of_schooling\"], zorder=-1)\nfor q in range(1, 5):\n    x = group_data.query(f\"quarter_of_birth=={q}\")[\"time_of_birth\"]\n    y = group_data.query(f\"quarter_of_birth=={q}\")[\"years_of_schooling\"]\n    plt.scatter(x, y, marker=\"s\", s=200, c=f\"C{q}\")\n    plt.scatter(x, y, marker=f\"${q}$\", s=100, c=f\"white\")\n\nplt.title(\"Years of Education by Quarter of Birth (first stage)\")\nplt.xlabel(\"Year of Birth\")\nplt.ylabel(\"Years of Schooling\");\n\n\n\n\n\n\n\n\n\n# Run regression\nfirst_stage = smf.ols(\n    'years_of_schooling ~ C(year_of_birth) + C(state_of_birth) + q4',\n    data=factor_data).fit()\n\n# Display results\nprint(\"q4 parameter estimate:, \", first_stage.params[\"q4\"])\nprint(\"q4 p-value:, \", first_stage.pvalues[\"q4\"])\n\nq4 parameter estimate:,  0.10085809272786678\nq4 p-value:,  5.464829416613623e-15\n\n\n\nBe unrelated to the outcome (and therefore to unmeasured confounders) - influence should all be due to effect on treatment. Here, we see seasonal pattern in outcome (earnings). We run a regression of the instrumental variable (Q4) and confounders (year + state of bith) on the outcome (wages). We see a significant result. Those born in Q4 have, on average 0.8% higher wages. P-value not as close to 0 as before, but pretty significant.\n\nNote: It’s not possible to verify this condition - we can only argue in favour of it - i.e. no reason for pattern to affect income beyond through education.\n\nplt.figure(figsize=(15,6))\nplt.plot(group_data[\"time_of_birth\"], group_data[\"log_wage\"], zorder=-1)\nfor q in range(1, 5):\n    x = group_data.query(f\"quarter_of_birth=={q}\")[\"time_of_birth\"]\n    y = group_data.query(f\"quarter_of_birth=={q}\")[\"log_wage\"]\n    plt.scatter(x, y, marker=\"s\", s=200, c=f\"C{q}\")\n    plt.scatter(x, y, marker=f\"${q}$\", s=100, c=f\"white\")\n\nplt.title(\"Average Weekly Wage by Quarter of Birth (reduced form)\")\nplt.xlabel(\"Year of Birth\")\nplt.ylabel(\"Log Weekly Earnings\");\n\n\n\n\n\n\n\n\n\nreduced_form = smf.ols(\n    \"log_wage ~ C(year_of_birth) + C(state_of_birth) + q4\",\n    data=factor_data).fit()\n\nprint(\"q4 parameter estimate:, \", reduced_form.params[\"q4\"])\nprint(\"q4 p-value:, \", reduced_form.pvalues[\"q4\"])\n\nq4 parameter estimate:,  0.008603484260139599\nq4 p-value:,  0.001494912718366745\n\n\n\n\n38.7.4 Estimating the average causal effect\n\n38.7.4.1 Option 1. Dividing reduced form by 1st stage.\nWe can use the two regressions above - namely, the first stage:\nfirst_stage = smf.ols(\n    'years_of_schooling ~ C(year_of_birth) + C(state_of_birth) + q4',\n    data=factor_data).fit()\nAnd the reduced form:\nreduced_form = smf.ols(\n    \"log_wage ~ C(year_of_birth) + C(state_of_birth) + q4\",\n    data=factor_data).fit()\nTo get an unbiased IV estimate of the average causal effect, we can scale the effect of the reduced form coefficient by the first stage coefficient:\n$ _{IV} = $\nBelow, we see 0.08… which means we expect each additional year in school to increase wages by 8%.\n\nreduced_form.params[\"q4\"] / first_stage.params[\"q4\"]\n\n0.08530286492084817\n\n\n\n\n38.7.4.2 Option 2. 2 stages least squares (2SLS)\nIn 2SLS, we do the first stage as before -\nfirst_stage = smf.ols(\n    'years_of_schooling ~ C(year_of_birth) + C(state_of_birth) + q4',\n    data=factor_data).fit()\nBut then, run second stage where replace treatment variable with fitted values of the first stage. These are essentially “the treatment purged from omitted variable bias”. Fitted values are the predicted values of the outcome variable from the model, when you input the values of the predictors into the model.\nWe see the same results as with option 1.\n\nsls_data = factor_data.assign(\n    years_of_schooling_fitted=first_stage.fittedvalues)\n\niv_by_hand = smf.ols(\n    \"log_wage ~ C(year_of_birth) + C(state_of_birth) + years_of_schooling_fitted\",\n    data=sls_data).fit()\n\niv_by_hand.params[\"years_of_schooling_fitted\"]\n\n0.08530286492089094\n\n\n\n\n38.7.4.3 Option 3. Automated 2SLS\nThis is recommended, as else standard errors from second stage are a bit off.\n\nformula = 'log_wage ~ 1 + C(year_of_birth) + C(state_of_birth) + [years_of_schooling ~ q4]'\niv2sls = IV2SLS.from_formula(formula, factor_data).fit()\nparse(iv2sls)\n\nParameter: 0.08530286494062492\nSE: 0.025540812815019267\n95 CI: [0.03524287 0.13536286]\nP-value: 0.0008381914642161536\n\n\nYou can also run with multiple instruments - here, all the quarters of birth…\n\nformula = 'log_wage ~ 1 + C(year_of_birth) + C(state_of_birth) + [years_of_schooling ~ q1+q2+q3]'\niv_many_zs = IV2SLS.from_formula(formula, factor_data).fit()\nparse(iv_many_zs)\n\nParameter: 0.1076937048813452\nSE: 0.01955714901081754\n95 CI: [0.06936169 0.14602572]\nP-value: 3.657974700921329e-08\n\n\n\n\n38.7.4.4 Summary\nComparing traditional OLS with 2SLS: * Education estimate lower with OLS than 2SLS, suggesting omitted variable bias may be less strong than thought * 2SLS has much wider confidence intervals than OLS\nSome things to consider: * If an instrumental variable has a small correlation with the treatment, then it is a weak instrument, and we won’t be able to learn much from the instrument. * 2SLS is biased towards OLS (i.e. if OLS has positive/negative bias, then so will 2SLS) * Bias will increase with the number of instruments added\nCommon mistakes: * Doing IV by hand * Using alternatives to OLS for the first stage. ‘The consistency of IV relies on a property that only OLS can give, which is the orthogonality of the residuals, so anything different than OLS on the 1st stage will yield something biased.’",
    "crumbs": [
      "Causal methods",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Instrumental variables</span>"
    ]
  },
  {
    "objectID": "content/causal_methods/c2_regression_discontinuity.html",
    "href": "content/causal_methods/c2_regression_discontinuity.html",
    "title": "39  Regression discontinuity (RD)",
    "section": "",
    "text": "39.1 How does RDD work?\nRegression Discontinuity Design (RDD) was first used by Thistlethwaite and Campbell (1960). It’s appeal is that it can convinvingly eliminate selection bias.\nIt can be used when the exposure status (wholly or partly) is determined by some continuous variable exceeding some arbitrary threshold. [Igelström et al. 2022] It works best when the cut-off is known, precise and free of manipulation - and is most effect when the exposure ‘has a “hair trigger” that is not tightly related to the outcome being studied’. Examples: * Arrest for DWI when &gt; 0.08 blood-alcohol content * Probably of receiving healthcare insurance at 65 * Probability of receiving medical attention jumping when birthweight falls below 1,500 grams * Probability of attending summer school when grades fall below some minimum level [Causal Inference: The Mixtape - Scott Cunningham]\n‘If the relationship between the forcing variable and the outcome is otherwise continuous, any discontinuity or jump in the relationship can be attributed to the exposure.’ RD estimates a local average treatment effect (LATE) ‘among the individuals who fall just above or just below the threshold.’ For this reason, we require large datasets.\n‘As with IV analysis, bias can occur if the forcing variable is connected to the outcome through a back-door path or any other pathway besides the exposure.’ [Igelström et al. 2022]\nWhy does this work? See below…\nTo understand RDD, we first focus on continuity, which is a core assumption that is illustrated below.\nIn the first graph observe that: * X (often called the “running variable” or “assignment variable” or “forcing variable”) is a confounder (causes D and Y) * As treatment assignment is based on cut-off, we can never observe units in both treatment and control with same value of X (does not satisfy overlap condition, can’t meet backdoor criterion)\nThe second graph illustrates that: * We can identify causal effects for subjects with values of X close to the cut-off c0. This is possible because the cut-off is the sole point where treatment and control subjects overlap. * There are lots of assumptions in this graph * One of the assumptions is continuity * This means X has no direct effect on Y * i.e. At c0, X no longer has a direct effect on Y * i.e. The cut-off c0 cannot be triggering a competing intervention at the same time it triggers treatment D * i.e. Expected potential outcomes are continuous at the cut-off (which would necessarily rule out competing interventions occurring at the same time) * The null hypothesis is continuity (i.e. things changes gradually), and any discontinuity implies some cause (“nature does not make jump” - ‘if you see a turtle on a fencepost, you know he didn’t get there by himself’)\n[Causal Inference: The Mixtape - Scott Cunningham]\nflowchart TD;\n\n    graphb:::outline;\n    subgraph graphb[\" \"]\n      X2(\"&lt;b&gt;X near c&lt;sub&gt;0&lt;/sub&gt;&lt;/b&gt;\"):::white;\n      D2(\"&lt;b&gt;D&lt;/b&gt;\"):::white;\n      U2(\"&lt;b&gt;U&lt;/b&gt;\"):::white;\n      Y2(\"&lt;b&gt;Y&lt;/b&gt;\"):::white;\n    end\n\n    X2 --&gt; D2;\n    D2 --&gt; Y2;\n    U2 --&gt; Y2;\n\n    grapha:::outline;\n    subgraph grapha[\" \"]\n      X1(\"&lt;b&gt;X&lt;/b&gt; (continuous variable)\"):::white;\n      D1(\"&lt;b&gt;D&lt;/b&gt; (treatment)&lt;br&gt;&lt;i&gt;receive if X &gt; cut-off c&lt;sub&gt;0&lt;/sub&gt;&lt;/i&gt;\"):::white;\n      U1(\"&lt;b&gt;U&lt;/b&gt;\"):::white;\n      Y1(\"&lt;b&gt;Y&lt;/b&gt; (outcome)\"):::white;\n    end\n\n    X1 --&gt; D1;\n    D1 --&gt; Y1;\n    X1 --&gt; Y1;\n    U1 --&gt; Y1;\n    X1 &lt;-.-&gt; U1;\n\n    classDef white fill:#FFFFFF, stroke:#FFFFFF;\n    classDef black fill:#FFFFFF, stroke:#000000;\n    classDef outline fill:#FFFFFF;\nAn example of RDD is Hoekstra (2009) * Aim: Estimate causal effect of college on earnings * Problem: Selection bias. State flagship universities are more selective than public universities in the same state. State flagship schools have individuals with higher observed and unobserved ability, and due to their ability, expected them to earn more regardless of their university.\nSee first image from Hoekstra (2009), which is about admission to flagship college based on SAT points: * Horizontal axis (SAT points above/below admission cut-off) is centred around 0 (cut-off). Cut-off was binding but not deterministic - some students below cut-off still get in - they likely had qualifications compensating for low SAT scores. Re-centred SAT scores are the running variable * This is not individual data - it is binned, with each dot the conditional mean (enrollment rate) * Lines are least squares fitted values of running variable, and regression could include higher-order terms, so line is above to more flexibly track central tendencies of data, with lines fit seperately for left and right * There is a discontinuous jump at cut-off * Looking at a large sample of students just either side of the cut-off, we expect them to be pretty similar to one another in terms of observable and unobservable characteristics.\n[Causal Inference: The Mixtape - Scott Cunningham]\nIn this figure, notice: * Estimated discontinuity 0.095 means that those just above cut-off earn 9.5% higher wages on average than those just below. With a variety of bins, estimates range from 7.4 to 11.1%. * He has found that ‘where workers experienced a jump in the probability of enrolling at the state flagship university, there is, ten to fifteen years later, a separate jump in logged earnings of around 10%. Those individuals who just barely made it in to the state flagship university made around 10% more in long-term earnings than those individuals who just barely missed the cutoff.’ * ‘Insofar as the two groups of applicants right around the cutoff have comparable future earnings in a world where neither attended the state flagship university, then there is no selection bias confounding his comparison’.\n[Causal Inference: The Mixtape - Scott Cunningham]",
    "crumbs": [
      "Causal methods",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Regression discontinuity (RD)</span>"
    ]
  },
  {
    "objectID": "content/causal_methods/c2_regression_discontinuity.html#how-does-rdd-work",
    "href": "content/causal_methods/c2_regression_discontinuity.html#how-does-rdd-work",
    "title": "39  Regression discontinuity (RD)",
    "section": "",
    "text": "Hoekstra RDD\n\n\n\n\n\n\n\nHoekstra RDD",
    "crumbs": [
      "Causal methods",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Regression discontinuity (RD)</span>"
    ]
  },
  {
    "objectID": "content/causal_methods/c2_regression_discontinuity.html#sharp-v.s.-fuzzy-rdd",
    "href": "content/causal_methods/c2_regression_discontinuity.html#sharp-v.s.-fuzzy-rdd",
    "title": "39  Regression discontinuity (RD)",
    "section": "39.2 Sharp v.s. fuzzy RDD",
    "text": "39.2 Sharp v.s. fuzzy RDD\nSharp RDD - when probability of treatment goes from 0 to 1 at cut-off - and so running variable is deterministic of X (dashed line)\nFuzzy RDD - when probability of treatment discontinuously increases at cut-off (as in example above) (solid line)\n\n\n\nFuzzy v.s. sharp RDD",
    "crumbs": [
      "Causal methods",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Regression discontinuity (RD)</span>"
    ]
  },
  {
    "objectID": "content/causal_methods/c2_regression_discontinuity.html#applying-to-our-work",
    "href": "content/causal_methods/c2_regression_discontinuity.html#applying-to-our-work",
    "title": "39  Regression discontinuity (RD)",
    "section": "39.3 Applying to our work",
    "text": "39.3 Applying to our work\nThis might be less relevant for caesarean (no single one cut-off for a single treatment).\nHowever, it would be relevant to stroke, which has much clearer cut-offs. Although maybe focus per hospital would you need to? If you think hospital has impact on threshold?",
    "crumbs": [
      "Causal methods",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Regression discontinuity (RD)</span>"
    ]
  },
  {
    "objectID": "content/causal_methods/c3_interrupted_time.html",
    "href": "content/causal_methods/c3_interrupted_time.html",
    "title": "40  Interrupted time series (ITS)",
    "section": "",
    "text": "40.1 Things to consider\nThe interrupted time series (ITS) design compares ‘the trend over time in a population-level outcome before and after an exposure is introduced.’\n‘Assuming that the trend would have been unchanged if the intervention was not introduced, a change in trend at the point of introduction (in terms of level and/or slope) can be attributed to the exposure.’ [Igelström et al. 2022]\nImage from Tam D Tran-The 3 Feb 2022 Towards Data Science blogpost:\n‘ITS can be regarded as a special case of IV or RD, with time being the instrument or forcing variable. ITS addresses time-invariant confounding but can be biased if other events that influence the outcome happen at the same time as the exposure’. [Igelström et al. 2022]\nIt is vitally important to carefully design an ITS study. Considerations include…\nNumber of time-points before and after the intervention * Usually equally spaced intervals, recommendations from 3 - 50 time points per segment, depends on methods used for analysis (e.g. OLS can have fewer than ARIMA) * General consensus: ‘longer time series tend to have more power than shorter time series’\nSample size per time point: * Larger sample –&gt; more stable estimates –&gt; less variability and outliers\nFrequency of time points * ‘Trade-off between number of time points and sample size per time point, depending on the choice of time interval’ * ‘When possible, choose frequency that have clinical or seasonal meaning so that a true underlying trend can be established. Also consider whether there may be a delay or waning intervention effect, especially when the impact occurs gradually, so you can choose frequency accordingly.’\nLocation of intervention * Intervention can be be early (e.g. 1/3 time points before), midway (most commonly), or late (e.g. 2/3 time points before) - as long as sufficient time points per segment + sample size\nExpected effect size * Two effect types - slope change (gradual change in gradient of trend) and level change (instant change in level) - and can be a combination of both\n[Tam D Tran-The 2022]\nImage from Tam D Tran-The 3 Feb 2022 Towards Data Science blogpost:",
    "crumbs": [
      "Causal methods",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Interrupted time series (ITS)</span>"
    ]
  },
  {
    "objectID": "content/causal_methods/c4_diff_in_diff.html",
    "href": "content/causal_methods/c4_diff_in_diff.html",
    "title": "41  Difference in differences (DiD)",
    "section": "",
    "text": "41.1 Example: John Snow and cholera\nThe difference-in-differences (DiD) design predates the randomised experiment by roughly 85 years. It was first used in 1855 by John Snow in his analysis of the cause of cholera. However, it is used relatively infrequently nowadays. [Canglia and Murray 2020]\nIn its simplest form, the Did method compares the change over time in a continuous population-level outcome between: * An exposed group * An unexposed group\n(Compares change over time in the exposed group, to change over time in the unexposed group, as just comparing the exposed group before and after has issues).\nIt ‘attempts to control for changing time trends, by using a comparison group to represent the counterfactual outcome trend in the exposed.’ ‘DiD also addresses time-invariant confounding but requires assuming that there would have been no difference in trend between the groups in the absence of the intervention (the “parallel trends” assumption).’ [Igelström et al. 2022]\nImage from Figarri Keisha 25 March 2022 Medium blog post:\nIn 1855, the cause of cholera was unknown. John Snow suspected it was waterborne. He compared: * Southwark and Vauxhall Company * Lambeth Water company\nBoth used water from River Thames. Neighbourhoods served by them both had very high mortality during the 1849 outbreak. In 1852, Lambeth Water Company moved its water intake to a site upstream of the sewage outflow.\nQuestion: Did the rate of cholera death decrease when water intake moved upstream, compared to if it had remained downstream? He couldn’t just compare before and after for Lambeth due to the change in cholera cases over time - so instead, answered by comparing the change in deaths, to the changes in deaths for Southwark and Vauxhall Company\nAnswer by estimating the average causal effect in the treated, or average treatment effect in the treated, or ATT.\nSnow observed from 1849 to 1854 that: * Southwark: increased by 118 deaths per 100,000 * Lambeth: decreased by 653 deaths per 100,000\n[Canglia and Murray 2020]",
    "crumbs": [
      "Causal methods",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Difference in differences (DiD)</span>"
    ]
  },
  {
    "objectID": "content/causal_methods/c4_diff_in_diff.html#assumptions",
    "href": "content/causal_methods/c4_diff_in_diff.html#assumptions",
    "title": "41  Difference in differences (DiD)",
    "section": "41.2 Assumptions",
    "text": "41.2 Assumptions\nConsistency - ‘the observed exposure of interest is equal to the counterfactual exposure of interest’ * e.g. ‘Moving the water source from opposite Hungerford Market to Thames Ditton maps to a sufficiently well-defined intervention’\nExchangeability and parallel trends - ‘any unmeasured determinants of the outcome are either time-invariant or group-invariant’ * e.g. ‘Any unmeasured determinants of the cholera death rate either do not vary with time or do not vary by district.’\nExchangeability and strict exogeneity - ‘implementation of the change in exposure was not related to the baseline value of the outcome variable’ * e.g. ‘The decisions to move or not move the Lambeth Company and the Southwalk & Vauxhall Company water intake sources were independent of the cholera mortality rates in neighborhoods supplied by those companies.’\nPositivity - ‘all individuals or subgroups of individuals are eligible to receive all levels of exposure’ * e.g. ‘The decision to move or not move the water intake source was available to both the Lambeth and Southwalk & Vauxhall Companies.’\n[Canglia and Murray 2020]",
    "crumbs": [
      "Causal methods",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Difference in differences (DiD)</span>"
    ]
  },
  {
    "objectID": "content/causal_methods/other.html",
    "href": "content/causal_methods/other.html",
    "title": "42  Other methods",
    "section": "",
    "text": "42.1 More on: Synthetic control\nThere are lots of causal inference methods that I have yet to explore! This page lists some of the methods that I haven’t covered, and goes into detail on a few.\nSynthetic control was popularised in Abadie, Diamond, and Hainmueller (2010). It’s similar to difference-in-differences, although less popular, and requires access to lots of pre-treatment data (‘otherwise the matching quality, or at least your ability to check match quality, will get iffy’)\nSteps: 1. Get ‘treated group and “donor set” of potential control groups’ 2. Matching algorithm assigns weights to each of the potential controls based on pre-treatment data. ‘These weights are designed such that the time trend of the outcome for the treated group should be almost exactly the same as the time trend of the outcome for the weighted average of the control group (the “synthetic control” group).’ 3. Compare outcomes after treatment\nExplanation and image from [The Effect: An Introduction to Research Design and Causality - Nick Huntington-Klein]",
    "crumbs": [
      "Causal methods",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>Other methods</span>"
    ]
  },
  {
    "objectID": "content/causal_methods/other.html#more-on-synthetic-control",
    "href": "content/causal_methods/other.html#more-on-synthetic-control",
    "title": "42  Other methods",
    "section": "",
    "text": "Synthetical control effect\n\n\n\n42.1.1 Comparison to difference-in-differences\nIt begins similar to difference-in-differences, but you ‘use data from the pre-treatment period to adjust for differences between the treatment and control groups, and then see how they differ after treatment goes into effect. The post-treatment difference, adjusting for pre-treatment differences, is your effect’.\nSo it differs from difference-in-differences as: * Pre-treatment different adjustment done with matching (rather than regression like DID), and the matching aims to eliminate prior differences (unlike DiD, where trying to account for propensity of treatment) * ‘Relies on long period of pre-treatment data’ * ‘After matching, the treated and control groups should have basically no pre-treatment differences. This is often accomplished by including the outcome variable as a matching variable’ * ‘Statistical significance is generally not determined by figuring out the sampling distribution of our estimation method beforehand, but rather by “randomization inference,” a method of using placebo tests to estimate a null distribution we can compare our real estimate to’\n[The Effect: An Introduction to Research Design and Causality - Nick Huntington-Klein]",
    "crumbs": [
      "Causal methods",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>Other methods</span>"
    ]
  }
]