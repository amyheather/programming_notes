

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Prediction v.s. causal inference &#8212; Amy Notes  documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=5b4479735964841361fd" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=5b4479735964841361fd" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd" />
  <script src="../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=5b4479735964841361fd"></script>

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/sphinx_highlight.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script type="module" src="https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs"></script>
    <script type="module" src="https://cdn.jsdelivr.net/npm/@mermaid-js/layout-elk@0.1.4/dist/mermaid-layout-elk.esm.min.mjs"></script>
    <script type="module">import mermaid from "https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs";import elkLayouts from "https://cdn.jsdelivr.net/npm/@mermaid-js/layout-elk@0.1.4/dist/mermaid-layout-elk.esm.min.mjs";mermaid.registerLayoutLoaders(elkLayouts);mermaid.initialize({startOnLoad:false});</script>
    <script src="https://cdn.jsdelivr.net/npm/d3@7.9.0/dist/d3.min.js"></script>
    <script type="module">
import mermaid from "https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs";
window.addEventListener("load", () => mermaid.run());
</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'causal_concepts/1_predict_vs_causal';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="The three “languages” of causal inference" href="2_intro_to_causality.html" />
    <link rel="prev" title="Discrete event simulation (DES)" href="../simulation/discrete_event.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
<div class="bd-header__inner bd-page-width">
  <label class="sidebar-toggle primary-toggle" for="__primary">
    <span class="fa-solid fa-bars"></span>
  </label>
  
  
  <div class="col-lg-3 navbar-header-items__start">
    
      <div class="navbar-item">

  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
  
    <p class="title logo__title">Programming notes</p>
  
</a></div>
    
  </div>
  
  <div class="col-lg-9 navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav class="navbar-nav">
  <p class="sidebar-header-items__title"
     role="heading"
     aria-level="1"
     aria-label="Site Navigation">
    Site Navigation
  </p>
  <ul class="bd-navbar-elements navbar-nav">
    
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../books/making_books.html">
                        Books
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../python/general.html">
                        Python
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../r/general.html">
                        R
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../programming_notes/git.html">
                        Git
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../simulation/simulation.html">
                        Simulation
                      </a>
                    </li>
                
            <li class="nav-item dropdown">
                <button class="btn dropdown-toggle nav-item" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-controls="pst-nav-more-links">
                    More
                </button>
                <ul id="pst-nav-more-links" class="dropdown-menu">
                    
                    <li class="nav-item current active">
                      <a class="nav-link dropdown-item nav-internal" href="#">
                        Causality
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link dropdown-item nav-internal" href="../programming_notes/latex.html">
                        Other
                      </a>
                    </li>
                
                </ul>
            </li>
            
  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script>
        </div>
      
      
        <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links navbar-nav"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/amyheather/programming_notes/" title="GitHub" class="nav-link" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><span><i class="fab fa-github-square fa-lg" aria-hidden="true"></i></span>
            <span class="sr-only">GitHub</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script>
    </div>
  

  
    <label class="sidebar-toggle secondary-toggle" for="__secondary" tabindex="0">
      <span class="fa-solid fa-outdent"></span>
    </label>
  
</div>

    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          <div class="navbar-item">
<nav class="navbar-nav">
  <p class="sidebar-header-items__title"
     role="heading"
     aria-level="1"
     aria-label="Site Navigation">
    Site Navigation
  </p>
  <ul class="bd-navbar-elements navbar-nav">
    
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../books/making_books.html">
                        Books
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../python/general.html">
                        Python
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../r/general.html">
                        R
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../programming_notes/git.html">
                        Git
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../simulation/simulation.html">
                        Simulation
                      </a>
                    </li>
                
            <li class="nav-item dropdown">
                <button class="btn dropdown-toggle nav-item" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-controls="pst-nav-more-links-2">
                    More
                </button>
                <ul id="pst-nav-more-links-2" class="dropdown-menu">
                    
                    <li class="nav-item current active">
                      <a class="nav-link dropdown-item nav-internal" href="#">
                        Causality
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link dropdown-item nav-internal" href="../programming_notes/latex.html">
                        Other
                      </a>
                    </li>
                
                </ul>
            </li>
            
  </ul>
</nav></div>
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links navbar-nav"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/amyheather/programming_notes/" title="GitHub" class="nav-link" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><span><i class="fab fa-github-square fa-lg" aria-hidden="true"></i></span>
            <span class="sr-only">GitHub</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
<nav class="bd-docs-nav bd-links"
     aria-label="Section Navigation">
  <p class="bd-links__title" role="heading" aria-level="1">Section Navigation</p>
  <div class="bd-toc-item navbar-nav"><p aria-level="2" class="caption" role="heading"><span class="caption-text">Introduction and causal concepts</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Prediction v.s. causal inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="2_intro_to_causality.html">The three “languages” of causal inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="3_dags.html">Directed acyclic graphs</a></li>
<li class="toctree-l1"><a class="reference internal" href="4_assumptions.html">Assumptions</a></li>
<li class="toctree-l1"><a class="reference internal" href="5_estimands.html">Causal estimands</a></li>
<li class="toctree-l1"><a class="reference internal" href="6_propensity_scores.html">Propensity scores</a></li>
<li class="toctree-l1"><a class="reference internal" href="7_target_trial.html">Target trial emulation</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Methods</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../causal_methods/summary.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../causal_methods/a1_multivariable_regression.html">Multivariable regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../causal_methods/a2_stratification.html">Stratification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../causal_methods/a3_matching.html">Matching</a></li>
<li class="toctree-l1"><a class="reference internal" href="../causal_methods/a4_iptw.html">Inverse probability of treatment weighting (IPTW) with baseline covariates</a></li>
<li class="toctree-l1"><a class="reference internal" href="../causal_methods/b1_marginal_structural_models.html">Marginal structural models (MSM)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../causal_methods/b2_gcomputation.html">G-computation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../causal_methods/b3_gestimation.html">G-estimation of structural nested models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../causal_methods/c1_instrumental_variable.html">Instrumental variables</a></li>
<li class="toctree-l1"><a class="reference internal" href="../causal_methods/c2_regression_discontinuity.html">Regression discontinuity (RD)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../causal_methods/c3_interrupted_time.html">Interrupted time series (ITS)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../causal_methods/c4_diff_in_diff.html">Difference in differences (DiD)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../causal_methods/other.html">Other methods</a></li>
</ul>
</div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Breadcrumb">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    <li class="breadcrumb-item active" aria-current="page">Prediction...</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <div class="section" id="prediction-v-s-causal-inference">
<h1>Prediction v.s. causal inference<a class="headerlink" href="#prediction-v-s-causal-inference" title="Permalink to this heading">#</a></h1>
<div class="toctree-wrapper compound">
</div>
<div class="toctree-wrapper compound">
</div>
<div class="info admonition">
<p class="admonition-title">Executive summary</p>
<p>Predictive research aims to predict an outcome with the <strong>best accuracy</strong>. Explainability (e.g. SHAP) is about understanding why a model makes certain predictions. When making predictions, whether the direction of relationships (e.g. from SHAP values) is true/causal doesn’t matter, as the goal is just to make the best predictions.</p>
<p>Etiological research aims to <strong>uncover causal effects</strong>. It involves finding an unbiased estimate of the effect of X on Y, by controlling for confounding factors that could bias the estimate. In causal inference, the true direction of relationships (and the counterfactual scenarios) are important. It typically starts with drawing a causal diagram.</p>
</div>
<div class="section" id="how-do-you-know-whether-you-are-interested-in-prediction-or-causation">
<h2>How do you know whether you are interested in prediction or causation?<a class="headerlink" href="#how-do-you-know-whether-you-are-interested-in-prediction-or-causation" title="Permalink to this heading">#</a></h2>
<p>Scientific research can be categorised into descriptive, predictive and etiological research. Descriptive research aims to summarise the characteristics of a group (or person).<a class="reference external" href="https://doi.org/10.1016/j.dcn.2020.100867">[Hamaker et al. 2020]</a> However, this page focusses just on <strong>predictive and etiological research</strong>.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p><strong>Predictive research</strong></p></th>
<th class="head"><p><strong>Etiological research</strong> (or “explanatory” research)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Aim</p></td>
<td><p>Aims to predict an outcome with the <strong>best accuracy</strong>.<a class="reference external" href="https://link.springer.com/article/10.1007/s10654-021-00794-w">[Ramspek et al. 2021]</a></p></td>
<td><p>Aims to uncover <strong>causal effects</strong> - i.e. <strong>causal inference</strong>.</p></td>
</tr>
<tr class="row-odd"><td><p>Relationships</p></td>
<td><p>It doesn’t matter whether predictors are causal or not - <strong>just focussed on best prediction</strong>.</p></td>
<td><p>Concerned with the <strong>true causal relationships</strong> between variables.</p></td>
</tr>
<tr class="row-even"><td><p>Directionality</p></td>
<td><p>We’re interested in <strong>associations</strong> (i.e. relationships aren’t directional).<a class="reference external" href="https://stats.stackexchange.com/questions/56909/what-is-the-relation-between-causal-inference-and-prediction">[source]</a></p></td>
<td><p>It is important that relationships are <strong>directional</strong>, as these directions are required to support interventional reasoning.<a class="reference external" href="https://stats.stackexchange.com/questions/56909/what-is-the-relation-between-causal-inference-and-prediction">[source]</a></p></td>
</tr>
</tbody>
</table>
<div class="section" id="illustrative-example">
<h3>Illustrative example<a class="headerlink" href="#illustrative-example" title="Permalink to this heading">#</a></h3>
<p>A team have built an XGBoost model to predict whether customers will renew their subscription. They use SHAP values to understand how the model made its predictions. They notice a suprising finding: <strong>users who report more bugs are more likely to renew. Is this a problem? It depends on their goal</strong></p>
<ul class="simple">
<li><p>If their goal is to <strong>predict customer retention to estimate future revenue</strong>, then this relationship is helpful for <strong>prediction</strong>, and it doesn’t matter about the direction, as long as our predictions are good.</p></li>
<li><p>However, if their goal is to <strong>inform actions to help retain customers</strong>, then it is important to understand the true relationships between features and the outcomes, and the counterfactual scenarios if features were modified. In this case, the team are interested in <strong>causation</strong>. In order for the team to understand the causal relationships, they would need to use causal inference methods (causal diagrams, appropriate techniques to account for confounding).</p></li>
</ul>
<p><strong>Why did this finding occur?</strong> If the team are interested in causation, they could draw a causal diagram (simplified version below). In it, they notice that some features are influenced by unmeasured confounding. WIth the example above, <strong>users who report more bugs are people who use the product more so encounter more bugs, but need the product more so are more likely to report</strong>. Because they can’t directly measure product need, the correlation they end up capturing in the predict model between bugs reported and renewal combines a small negative direct effect of bugs faced and a large positive confounding effect from product need. <a class="reference external" href="https://shap.readthedocs.io/en/latest/example_notebooks/overviews/Be%20careful%20when%20interpreting%20predictive%20models%20in%20search%20of%20causal%20insights.html">[source]</a></p>
<pre  class="mermaid">
          flowchart LR;

    need(&quot;&lt;b&gt;Unmeasured&lt;/b&gt;&lt;br&gt;Product need&quot;):::white;
    month(&quot;Monthly usage&quot;):::white;
    face(&quot;&lt;b&gt;Unmeasured&lt;/b&gt;&lt;br&gt;Bugs faced&quot;):::white;
    report(&quot;Bugs reported&quot;):::white;
    ren(&quot;Did renew&quot;):::important;


    month --&gt; face; face --&gt; report; face --&gt; ren;
    need --&gt; report;
    need --&gt; month;
    need --&gt; ren;
    month --&gt; ren;

    classDef white fill:#FFFFFF, stroke:#FFFFFF
    classDef black fill:#FFFFFF, stroke:#000000
    classDef important fill:#DDF2D1, stroke: #FFFFFF;
    </pre></div>
<div class="section" id="explainability-v-s-causality">
<h3>Explainability v.s. causality<a class="headerlink" href="#explainability-v-s-causality" title="Permalink to this heading">#</a></h3>
<p>Explainability refers to being able to <strong>understand why a model makes certain predictions</strong>. The aim of explainable AI is to make ML models more transparent. It provides insights on:</p>
<ul class="simple">
<li><p>How a model makes predictions</p></li>
<li><p>What features are most important</p></li>
<li><p>How sensitive a model is to changes in the input <a class="reference external" href="https://doi.org/10.1038/s42256-020-0197-y">[Prosperi et al. 2020]</a></p></li>
</ul>
<p>The contribution of individual covariates are often mistakenly interpreted causally, but the methods used were focused on combining covariates to optimise predictive accuracy, and not to predict the outcome distribution under hypothetical interventions. <a class="reference external" href="https://doi.org/10.1186/s41512-021-00092-9">[Lin et al. 2021]</a></p>
<p>However, it cannot be used to infer causal relationships, since the findings may be biased by stratification or unmeasured confounders, or mediated by other factors in the causal pathway.<a class="reference external" href="https://doi.org/10.1038/s42256-020-0197-y">[Prosperi et al. 2020]</a> If you wish to make claims about causality, you will need to build a casual model. Causal ML aims to <strong>infer causal relationships</strong> from observational data by estimating the effect of a specific variable on the outcome, while <strong>appropriately controlling</strong> for other confounding factors that could bias the estimate.<a class="reference external" href="https://medium.com/&#64;dahnert.sebastian/understand-the-difference-and-intersection-between-causal-ml-and-explainable-ai-65583132e704">[source]</a></p>
</div>
</div>
<div class="section" id="doing-predictive-and-etiological-research">
<h2>Doing predictive AND etiological research<a class="headerlink" href="#doing-predictive-and-etiological-research" title="Permalink to this heading">#</a></h2>
<p>Many problems will require a <strong>combination of prediction and causation</strong>.</p>
<ul class="simple">
<li><p>“Pure forecasting task” - e.g. just want to predict whether or not it will rain, and don’t care why/what caused the rain</p></li>
<li><p>“Pure causation task” - e.g. performing a rain dance presumed to save dying crops, only if it actually causes rain</p></li>
<li><p>Combination of the two - e.g. if planning assignment of fire inspectors across a city, should (a) predict will establishment will be in violation of fire codes, and (b) estimate causal effect on establishment’s behaviour of receiving an inspection or not</p></li>
</ul>
<p>Beck et al. 2018 also argue that <strong>prediction remains relevant even if you’re only interested in understanding causal effects</strong>. Explanations that invoke causal mechanisms always make predictions - specifically, predictions about what will happen under an intervention. ‘Whether they do so explicitly or not, that is, causal claims necessarily make predictions; thus it is both fair and arguably useful to hold them accountable for the accuracy of the predictions they make.’ They therefore argue that the <strong>predictive performance of models and of explanations</strong> is important to include (e.g. R<sup>2</sup>, MAE, RMSE, AUC, accuracy, recall, F1).<a class="reference external" href="https://doi.org/10.31219/osf.io/u6vz5">[Beck et al. 2018]</a></p>
</div>
<div class="section" id="when-can-prediction-models-answer-causal-questions">
<h2>When can prediction models answer causal questions?<a class="headerlink" href="#when-can-prediction-models-answer-causal-questions" title="Permalink to this heading">#</a></h2>
<p>As this example is from a simulation study where know true causal effects, we can plot the SHAP values from the prediction models v.s. the known true causal effects.</p>
<p><img alt="Causal effects" src="../_images/shap_bugs_causal_vs_shap.png" /></p>
<p>We can also add clustering to see the redundancy structure of the data as a dendrogram - ‘when features merge together at the bottom (left) of the dendrogram it means that that the information those features contain about the outcome (renewal) is very redundant and the model could have used either feature. When features merge together at the top (right) of the dendrogram it means the information they contain about the outcome is independent from each other.’</p>
<p><img alt="Redundancy" src="../_images/shap_bugs_redundancy.png" /></p>
<p><strong>When can predictive models answer causal questions?</strong> When the feature is independent of (a) other features in the model, and (b) unobserved confounders. Hence, it is not subject to bias from either unmeasured confounders or feature redundancy. Example: Economy</p>
<ul class="simple">
<li><p>Independent from other features in dendogram (no observed confounding)</p></li>
<li><p>No unobserved confounding in causal digram</p></li>
</ul>
<p><strong>When can they not be used? (1) When you have observed confounding.</strong> Example: Ad Spend (no direct causal effect on retention, but correlated with Last upgrade and Monthly usage which do drive retention). ‘Our predictive model identifies Ad Spend as the one of the best single predictors of retention because it captures so many of the true causal drivers through correlations. XGBoost imposes regularization, which is a fancy way of saying that it tries to choose the simplest possible model that still predicts well. If it could predict equally well using one feature rather than three, it will tend to do that to avoid overfitting.’</p>
<p>However, there are methods to deal with observed confounding, such as double/debiased machine learning model. This involves:</p>
<ol class="arabic simple">
<li><p>Train model to predict feature (Ad spend) using set of confounders (features not caused by Ad spend)</p></li>
<li><p>Train model to predict outcome (Did Renew) using that set of confounders</p></li>
<li><p>Train model to predict residual variation of outcome (the variation left after subtracting our prediction) using the residual variation of the causal feature of interest</p></li>
</ol>
<p>‘The intuition is that if Ad Spend causes renewal, then the part of Ad Spend that can’t be predicted by other confounding features should be correlated with the part of renewal that can’t be predicted by other confounding features.’ There are packages like econML’s LinearDML for this.</p>
<p><strong>When can they not be used? (2) When you have non-confounding redundancy.</strong> ‘This occurs when the feature we want causal effects for causally drives, or is driven by, another feature included in the model, but that other feature is not a confounder of our feature of interest.’</p>
<p>Example: Sales Calls directly impact retention, but also have an indirect effect on retention through Interactions. We can see this in the SHAP scatter plots above, which show how XGBoost underestimates the true causal effect of Sales Calls because most of that effect got put onto the Interactions feature.</p>
<p>‘<strong>Non-confounding redundancy can be fixed in principle by removing the redundant variables from the model</strong> (see below). For example, if we removed Interactions from the model then we will capture the full effect of making a sales call on renewal probability. This removal is also important for double ML, since double ML will fail to capture indirect causal effects if you control for downstream features caused by the feature of interest. In this case double ML will only measure the “direct” effect that does not pass through the other feature. Double ML is however robust to controlling for upstream non-confounding redundancy (where the redundant feature causes the feature of interest), though this will reduce your statistical power to detect true effects. Unfortunately, we often don’t know the true causal graph so it can be hard to know when another feature is redundant with our feature of interest because of observed confounding vs. non-confounding redundancy. If it is because of confounding then we should control for that feature using a method like double ML, whereas if it is a downstream consequence then we should drop the feature from our model if we want full causal effects rather than only direct effects. Controlling for a feature we shouldn’t tends to hide or split up causal effects, while failing to control for a feature we should have controlled for tends to infer causal effects that do not exist. This generally makes controlling for a feature the safer option when you are uncertain.’</p>
<p><strong>When can they not be used? (3) When you have unobserved confounding.</strong> ‘The Discount and Bugs Reported features both suffer from unobserved confounding because not all important variables (e.g., Product Need and Bugs Faced) are measured in the data. Even though both features are relatively independent of all the other features in the model, there are important drivers that are unmeasured. In this case, both predictive models and causal models that require confounders to be observed, like double ML, will fail. This is why double ML estimates a large negative causal effect for the Discount feature even when controlling for all other observed features’</p>
<p>‘Specialized causal tools based on the principals of instrumental variables, differences-in-differences, or regression discontinuities can sometimes exploit partial randomization even in cases where a full experiment is impossible. For example, instrumental variable techniques can be used to identify causal effects in cases where we cannot randomly assign a treatment, but we can randomly nudge some customers towards treatment, like sending an email encouraging them to explore a new product feature. Difference-in-difference approaches can be helpful when the introduction of new treatments is staggered across groups. Finally, regression discontinuity approaches are a good option when patterns of treatment exhibit sharp cut-offs (for example qualification for treatment based on a specific, measurable trait like revenue over $5,000 per month).’</p>
<p><a class="reference external" href="https://shap.readthedocs.io/en/latest/example_notebooks/overviews/Be%20careful%20when%20interpreting%20predictive%20models%20in%20search%20of%20causal%20insights.html#">[source]</a></p>
</div>
<div class="section" id="confusion-and-controversy">
<h2>Confusion and controversy<a class="headerlink" href="#confusion-and-controversy" title="Permalink to this heading">#</a></h2>
<div class="section" id="reasons-for-confusion">
<h3>Reasons for confusion<a class="headerlink" href="#reasons-for-confusion" title="Permalink to this heading">#</a></h3>
<p>Causal inference can be confusing and controversial. Reasons for this are:</p>
<ul class="simple">
<li><p>Causally <strong>unrelated</strong> variables can be <strong>highly correlated</strong></p></li>
<li><p>Results may be reported in a way that is careful to avoid referring to any causal relationships, but it will often still <strong>naturally be read and interpreted as causal</strong></p></li>
<li><p>Even if there is a causal relationship, sometimes the <strong>direction is unclear</strong> - would need to carefully examine the temporal relationships between the variables<a class="reference external" href="https://www.coursera.org/learn/crash-course-in-causality/lecture/x4UMR/confusion-over-causality">[source]</a></p></li>
</ul>
</div>
<div class="section" id="confusion-in-the-literature">
<h3>Confusion in the literature<a class="headerlink" href="#confusion-in-the-literature" title="Permalink to this heading">#</a></h3>
<p>In practice, prediction and causation are <strong>commonly conflated</strong>. A review of observational studies found that 26% (46 / 180) observational cohort studies conflated between etiology and prediction -</p>
<ul class="simple">
<li><p>In causal studies, this was mainly due to selection of covariates based on their ability to predict <strong>without taking causal structure into account</strong>.</p></li>
<li><p>In prediction studies, this was mainly due to <strong>causal interpretation</strong> of covariates included in a prediction model.</p></li>
</ul>
</div>
</div>
<div class="section" id="concepts-and-principles">
<h2>Concepts and principles<a class="headerlink" href="#concepts-and-principles" title="Permalink to this heading">#</a></h2>
<div class="section" id="ladder-of-causality">
<h3>Ladder of causality<a class="headerlink" href="#ladder-of-causality" title="Permalink to this heading">#</a></h3>
<p>Judea Pearl proposed the ‘<strong>Ladder of Causality</strong>’ to categorise different levels of causal thinking, with increasing levels of difficulty.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Level</p></th>
<th class="head"><p>Typical activity</p></th>
<th class="head"><p>Typical questions</p></th>
<th class="head"><p>Examples</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Association</strong></p></td>
<td><p>Seeing</p></td>
<td><p>What is?<br>How would seeing X change my belief in Y?</p></td>
<td><p>What does a symptom tell me about a disease?<br>What does a survey tell us about the election results?</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Intervention</strong></p></td>
<td><p>Doing</p></td>
<td><p>What if?<br>What if I do X?</p></td>
<td><p>What if I take aspirin, will my headachbe be cured?<br>What if we ban cigarettes?</p></td>
</tr>
<tr class="row-even"><td><p><strong>Counterfactuals</strong></p></td>
<td><p>Imagining, retrospection</p></td>
<td><p>Why?<br>Was it X that caused Y?<br> What if I had acted differently?</p></td>
<td><p>Was it aspirin that stopped my headache?<br>Would Kennedy be alive had Oswald not shot him?<br>What if I had not been smoking the past two years?</p></td>
</tr>
</tbody>
</table>
<p><a class="reference external" href="https://cacm.acm.org/magazines/2019/3/234929-the-seven-tools-of-causal-inference-with-reflections-on-machine-learning/fulltext?mobile=false">[source]</a></p>
<p>Difference between interventions and counterfactuals in this hierarchy:</p>
<ul class="simple">
<li><p>With interventions, you ask what will happen on average if you perform an action.</p></li>
<li><p>With counterfactuals, you ask what would have happened if you had performed a different action.</p></li>
<li><p>These two queries are mathematically distinct as they require different levels of information to be answered (counterfactuals need more information to be answered)’.<a class="reference external" href="https://stats.stackexchange.com/questions/379799/difference-between-rungs-two-and-three-in-the-ladder-of-causation">[source]</a></p></li>
</ul>
</div>
<div class="section" id="types-of-inference">
<h3>Types of inference<a class="headerlink" href="#types-of-inference" title="Permalink to this heading">#</a></h3>
<p>From C.S.Peirce in late 1800s:</p>
<ul class="simple">
<li><p>‘<strong>Deduction</strong> - necessary inference following logic’ <a class="reference external" href="https://www.statslab.cam.ac.uk/~qz280/teaching/causal-2023/notes-2021.pdf">[Zhao 2022]</a></p>
<ul>
<li><p>e.g. If dentist appointment at 10 and it’s 30 minute drive, deduce you need to leave at 9.30 <a class="reference external" href="https://www.merriam-webster.com/grammar/deduction-vs-induction-vs-abduction">[source]</a></p></li>
</ul>
</li>
<li><p>‘<strong>Induction</strong> - probable or non-necessary inference (purely) based on statistical data</p>
<ul>
<li><p>e.g. Correlation between cigarette smoking and lung cancer’ <a class="reference external" href="https://www.statslab.cam.ac.uk/~qz280/teaching/causal-2023/notes-2021.pdf">[Zhao 2022]</a></p></li>
<li><p>Four of your six coworker order the same sandwich so you induce that the sandwich is probably good <a class="reference external" href="https://www.merriam-webster.com/grammar/deduction-vs-induction-vs-abduction">[source]</a></p></li>
</ul>
</li>
<li><p>‘<strong>Abduction</strong> - inference with implicit or explicit appeal to explanatory considerations</p>
<ul>
<li><p>e.g. Investigation of a crime scene</p></li>
<li><p>Cigarette smoking causes lung cancer’ <a class="reference external" href="https://www.statslab.cam.ac.uk/~qz280/teaching/causal-2023/notes-2021.pdf">[Zhao 2022]</a></p></li>
</ul>
</li>
</ul>
</div>
</div>
</div>


                </article>
              
              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../simulation/discrete_event.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Discrete event simulation (DES)</p>
      </div>
    </a>
    <a class="right-next"
       href="2_intro_to_causality.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">The three “languages” of causal inference</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">

  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-do-you-know-whether-you-are-interested-in-prediction-or-causation">How do you know whether you are interested in prediction or causation?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#illustrative-example">Illustrative example</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#explainability-v-s-causality">Explainability v.s. causality</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#doing-predictive-and-etiological-research">Doing predictive AND etiological research</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#when-can-prediction-models-answer-causal-questions">When can prediction models answer causal questions?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#confusion-and-controversy">Confusion and controversy</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reasons-for-confusion">Reasons for confusion</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#confusion-in-the-literature">Confusion in the literature</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#concepts-and-principles">Concepts and principles</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ladder-of-causality">Ladder of causality</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#types-of-inference">Types of inference</a></li>
</ul>
</li>
</ul>
  </nav></div>

  <div class="sidebar-secondary-item">

  <div class="tocsection sourcelink">
    <a href="../_sources/causal_concepts/1_predict_vs_causal.md.txt">
      <i class="fa-solid fa-file-lines"></i> Show Source
    </a>
  </div>
</div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd"></script>

  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      © Copyright 2024, Amy Heather.
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">

  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 5.3.0.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.14.4.
</p></div>
      
    </div>
  
</div>

  </footer>
  </body>
</html>